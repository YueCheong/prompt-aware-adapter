| distributed init (rank 0, world 4): env://
| distributed init (rank 2, world 4): env://
| distributed init (rank 1, world 4): env://
| distributed init (rank 3, world 4): env://
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/coco_captions/coco_karpathy_train.json
vis_path:/home/users/nus/idmwyk/scratch/temp/dataset/coco/images/train2014
loading dataset refcoco into memory...
creating index...
index created.
DONE (t=2.49s)
loading dataset refcoco+ into memory...
creating index...
index created.
DONE (t=2.46s)
loading dataset refcocog into memory...
creating index...
index created.
DONE (t=2.27s)
loading dataset refcoco into memory...
creating index...
index created.
DONE (t=2.99s)
loading dataset refcoco+ into memory...
creating index...
index created.
DONE (t=2.49s)
loading dataset refcocog into memory...
creating index...
index created.
DONE (t=2.28s)
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/gqa/train_balanced_questions.json
vis_path:/home/users/nus/idmwyk/scratch/temp/dataset/gqa/images/train2014
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/vqav2/vqa_train.json
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/vqav2/vqa_val.json
vis_path:/home/users/nus/idmwyk/scratch/temp/dataset/coco/images/train2014
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/okvqa/okvqa_train.json
vis_path:/home/users/nus/idmwyk/scratch/temp/dataset/coco/images/train2014
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/aokvqa/aokvqa_v1p0_train.json
vis_path:/home/users/nus/idmwyk/scratch/temp/dataset/coco/images/train2014
len(exist_annotation):11106
trainable params: 27262976 || all params: 8057524224 || trainable%: 0.33835425426081844
Position interpolate from 16x16 to 32x32
Load Minigpt-4-LLM Checkpoint: /home/users/nus/idmwyk/scratch/temp/output/saved-ckpt/stage1/20240907183/checkpoint_49.pth
model arch:
 MiniGPTv4(
  (llama_model): PeftModelForCausalLM(
    (base_model): LoraModel(
      (model): LlamaForCausalLM(
        (model): LlamaModel(
          (embed_tokens): Embedding(128256, 4096)
          (layers): ModuleList(
            (0-31): 32 x LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(
                  in_features=4096, out_features=4096, bias=False
                  (lora_dropout): Dropout(p=0.05, inplace=False)
                  (lora_A): Linear(in_features=4096, out_features=64, bias=False)
                  (lora_B): Linear(in_features=64, out_features=4096, bias=False)
                )
                (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
                (v_proj): Linear(
                  in_features=4096, out_features=1024, bias=False
                  (lora_dropout): Dropout(p=0.05, inplace=False)
                  (lora_A): Linear(in_features=4096, out_features=64, bias=False)
                  (lora_B): Linear(in_features=64, out_features=1024, bias=False)
                )
                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
                (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
                (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
                (act_fn): SiLU()
              )
              (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
              (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
            )
          )
          (norm): LlamaRMSNorm((4096,), eps=1e-05)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (lm_head): CastOutputToFloat(
          (0): Linear(in_features=4096, out_features=128256, bias=False)
        )
      )
    )
  )
  (visual_encoder): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0-38): 39 x Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (ln_vision): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)
  (local_attn): LocalAttention(
    (txt): Linear(in_features=4096, out_features=1408, bias=True)
    (img): Linear(in_features=1408, out_features=1408, bias=True)
  )
  (llama_proj): Linear(in_features=5632, out_features=4096, bias=True)
)
batch sizes [[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]]
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.28.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.28.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.28.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.28.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.29.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.29.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.29.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.29.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.30.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.30.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.30.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.30.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.31.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.31.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.31.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.31.self_attn.v_proj.lora_B.weight
module.local_attn.txt.weight
module.local_attn.txt.bias
module.local_attn.img.weight
module.local_attn.img.bias
module.llama_proj.weight
module.llama_proj.bias
Train: data epoch: [0]  [   0/1000]  eta: 8:29:26  lr: 0.000001  loss: 0.4621  time: 30.5667  data: 0.0000  max mem: 28963
Train: data epoch: [0]  [  50/1000]  eta: 0:58:06  lr: 0.000001  loss: 1.2383  time: 1.8585  data: 0.0000  max mem: 30425
Train: data epoch: [0]  [ 100/1000]  eta: 0:34:03  lr: 0.000002  loss: 1.5385  time: 0.4502  data: 0.0000  max mem: 30508
Train: data epoch: [0]  [ 150/1000]  eta: 0:24:05  lr: 0.000002  loss: 1.6710  time: 0.4622  data: 0.0000  max mem: 30604
Train: data epoch: [0]  [ 200/1000]  eta: 0:18:39  lr: 0.000003  loss: 0.7477  time: 0.4469  data: 0.0000  max mem: 30604
Train: data epoch: [0]  [ 250/1000]  eta: 0:15:07  lr: 0.000003  loss: 1.2925  time: 0.4476  data: 0.0000  max mem: 30645
Train: data epoch: [0]  [ 300/1000]  eta: 0:12:45  lr: 0.000004  loss: 2.7825  time: 0.4549  data: 0.0000  max mem: 30932
Train: data epoch: [0]  [ 350/1000]  eta: 0:10:51  lr: 0.000004  loss: 0.7238  time: 0.4494  data: 0.0000  max mem: 30975
Train: data epoch: [0]  [ 400/1000]  eta: 0:09:20  lr: 0.000005  loss: 1.4253  time: 0.4700  data: 0.0000  max mem: 30975
Train: data epoch: [0]  [ 450/1000]  eta: 0:08:03  lr: 0.000005  loss: 0.1980  time: 0.4466  data: 0.0000  max mem: 30975
Train: data epoch: [0]  [ 500/1000]  eta: 0:06:58  lr: 0.000005  loss: 0.2184  time: 0.4486  data: 0.0000  max mem: 30975
Train: data epoch: [0]  [ 550/1000]  eta: 0:06:01  lr: 0.000006  loss: 0.4143  time: 0.4488  data: 0.0000  max mem: 30975
Train: data epoch: [0]  [ 600/1000]  eta: 0:05:09  lr: 0.000006  loss: 2.9505  time: 0.4482  data: 0.0000  max mem: 30975
Train: data epoch: [0]  [ 650/1000]  eta: 0:04:22  lr: 0.000007  loss: 1.2825  time: 0.4463  data: 0.0000  max mem: 30975
Train: data epoch: [0]  [ 700/1000]  eta: 0:03:39  lr: 0.000007  loss: 0.2221  time: 0.4475  data: 0.0000  max mem: 30975
Train: data epoch: [0]  [ 750/1000]  eta: 0:03:01  lr: 0.000008  loss: 1.1730  time: 0.9879  data: 0.0000  max mem: 30975
Train: data epoch: [0]  [ 800/1000]  eta: 0:02:21  lr: 0.000008  loss: 1.3345  time: 0.4508  data: 0.0000  max mem: 30975
Train: data epoch: [0]  [ 850/1000]  eta: 0:01:44  lr: 0.000009  loss: 1.7565  time: 0.4473  data: 0.0000  max mem: 31379
Train: data epoch: [0]  [ 900/1000]  eta: 0:01:08  lr: 0.000009  loss: 1.1806  time: 0.4499  data: 0.0000  max mem: 31379
Train: data epoch: [0]  [ 950/1000]  eta: 0:00:33  lr: 0.000010  loss: 0.5799  time: 0.4487  data: 0.0000  max mem: 31379
Train: data epoch: [0]  [ 999/1000]  eta: 0:00:00  lr: 0.000010  loss: 2.0639  time: 0.5795  data: 0.0000  max mem: 31379
Train: data epoch: [0] Total time: 0:11:00 (0.6606 s / it)
Train: data epoch: [1]  [   0/1000]  eta: 0:07:28  lr: 0.000010  loss: 2.5850  time: 0.4481  data: 0.0000  max mem: 31379
Train: data epoch: [1]  [  50/1000]  eta: 0:07:04  lr: 0.000010  loss: 0.6646  time: 0.4478  data: 0.0000  max mem: 31379
Train: data epoch: [1]  [ 100/1000]  eta: 0:06:43  lr: 0.000010  loss: 1.3081  time: 0.4471  data: 0.0000  max mem: 31379
Train: data epoch: [1]  [ 150/1000]  eta: 0:06:20  lr: 0.000010  loss: 1.3312  time: 0.4472  data: 0.0000  max mem: 31379
Train: data epoch: [1]  [ 200/1000]  eta: 0:06:34  lr: 0.000010  loss: 1.3222  time: 0.4465  data: 0.0000  max mem: 31379
Train: data epoch: [1]  [ 250/1000]  eta: 0:06:16  lr: 0.000010  loss: 1.3125  time: 0.6807  data: 0.0000  max mem: 31379
Train: data epoch: [1]  [ 300/1000]  eta: 0:05:45  lr: 0.000010  loss: 0.4540  time: 0.4495  data: 0.0000  max mem: 31379
Train: data epoch: [1]  [ 350/1000]  eta: 0:05:16  lr: 0.000010  loss: 1.4238  time: 0.4618  data: 0.0000  max mem: 31379
Train: data epoch: [1]  [ 400/1000]  eta: 0:04:49  lr: 0.000010  loss: 1.3663  time: 0.4492  data: 0.0000  max mem: 31379
Train: data epoch: [1]  [ 450/1000]  eta: 0:04:24  lr: 0.000010  loss: 2.7488  time: 0.4902  data: 0.0000  max mem: 31379
Train: data epoch: [1]  [ 500/1000]  eta: 0:03:58  lr: 0.000010  loss: 0.7445  time: 0.4474  data: 0.0000  max mem: 31379
Train: data epoch: [1]  [ 550/1000]  eta: 0:03:33  lr: 0.000010  loss: 0.3109  time: 0.4496  data: 0.0000  max mem: 31379
Train: data epoch: [1]  [ 600/1000]  eta: 0:03:13  lr: 0.000010  loss: 0.9417  time: 0.4493  data: 0.0000  max mem: 31379
Train: data epoch: [1]  [ 650/1000]  eta: 0:02:49  lr: 0.000010  loss: 3.1286  time: 0.4488  data: 0.0000  max mem: 31379
Train: data epoch: [1]  [ 700/1000]  eta: 0:02:30  lr: 0.000010  loss: 2.3663  time: 1.2032  data: 0.0000  max mem: 31379
Train: data epoch: [1]  [ 750/1000]  eta: 0:02:04  lr: 0.000010  loss: 1.3349  time: 0.4492  data: 0.0000  max mem: 31379
Train: data epoch: [1]  [ 800/1000]  eta: 0:01:39  lr: 0.000010  loss: 1.8694  time: 0.4473  data: 0.0000  max mem: 31379
Train: data epoch: [1]  [ 850/1000]  eta: 0:01:14  lr: 0.000010  loss: 1.3422  time: 0.4481  data: 0.0000  max mem: 31379
Train: data epoch: [1]  [ 900/1000]  eta: 0:00:49  lr: 0.000010  loss: 1.3702  time: 0.4479  data: 0.0000  max mem: 31379
Train: data epoch: [1]  [ 950/1000]  eta: 0:00:24  lr: 0.000010  loss: 1.2938  time: 0.4485  data: 0.0000  max mem: 31379
Train: data epoch: [1]  [ 999/1000]  eta: 0:00:00  lr: 0.000010  loss: 1.2905  time: 0.4860  data: 0.0000  max mem: 31379
Train: data epoch: [1] Total time: 0:08:07 (0.4879 s / it)
Train: data epoch: [2]  [   0/1000]  eta: 1:59:53  lr: 0.000010  loss: 1.2350  time: 7.1939  data: 0.0007  max mem: 31379
Train: data epoch: [2]  [  50/1000]  eta: 0:09:13  lr: 0.000010  loss: 1.5105  time: 0.4485  data: 0.0000  max mem: 31379
Train: data epoch: [2]  [ 100/1000]  eta: 0:07:44  lr: 0.000010  loss: 2.7900  time: 0.4474  data: 0.0000  max mem: 31379
Train: data epoch: [2]  [ 150/1000]  eta: 0:07:49  lr: 0.000010  loss: 1.2063  time: 0.4485  data: 0.0000  max mem: 31379
Train: data epoch: [2]  [ 200/1000]  eta: 0:07:02  lr: 0.000010  loss: 0.6685  time: 0.4585  data: 0.0000  max mem: 31379
Train: data epoch: [2]  [ 250/1000]  eta: 0:06:24  lr: 0.000010  loss: 1.4102  time: 0.4498  data: 0.0000  max mem: 31379
Train: data epoch: [2]  [ 300/1000]  eta: 0:05:51  lr: 0.000010  loss: 0.1561  time: 0.4470  data: 0.0000  max mem: 31379
Train: data epoch: [2]  [ 350/1000]  eta: 0:05:30  lr: 0.000010  loss: 1.3137  time: 0.7099  data: 0.0000  max mem: 31379
Train: data epoch: [2]  [ 400/1000]  eta: 0:05:00  lr: 0.000010  loss: 1.8752  time: 0.4493  data: 0.0000  max mem: 31379
Train: data epoch: [2]  [ 450/1000]  eta: 0:04:45  lr: 0.000010  loss: 1.3951  time: 0.4596  data: 0.0000  max mem: 31379
Train: data epoch: [2]  [ 500/1000]  eta: 0:04:16  lr: 0.000010  loss: 0.2056  time: 0.4496  data: 0.0000  max mem: 31379
Train: data epoch: [2]  [ 550/1000]  eta: 0:03:47  lr: 0.000010  loss: 2.8662  time: 0.4487  data: 0.0000  max mem: 31379
Train: data epoch: [2]  [ 600/1000]  eta: 0:03:28  lr: 0.000010  loss: 1.3421  time: 1.0200  data: 0.0000  max mem: 31379
Train: data epoch: [2]  [ 650/1000]  eta: 0:03:00  lr: 0.000010  loss: 2.7731  time: 0.4488  data: 0.0000  max mem: 31379
Train: data epoch: [2]  [ 700/1000]  eta: 0:02:33  lr: 0.000010  loss: 2.5347  time: 0.4473  data: 0.0000  max mem: 31379
Train: data epoch: [2]  [ 750/1000]  eta: 0:02:09  lr: 0.000010  loss: 1.3683  time: 0.8819  data: 0.0000  max mem: 31379
Train: data epoch: [2]  [ 800/1000]  eta: 0:01:43  lr: 0.000010  loss: 1.3569  time: 0.4609  data: 0.0000  max mem: 31379
Train: data epoch: [2]  [ 850/1000]  eta: 0:01:16  lr: 0.000010  loss: 2.0458  time: 0.4479  data: 0.0000  max mem: 31379
Train: data epoch: [2]  [ 900/1000]  eta: 0:00:51  lr: 0.000010  loss: 1.2982  time: 0.9026  data: 0.0000  max mem: 31379
Train: data epoch: [2]  [ 950/1000]  eta: 0:00:26  lr: 0.000010  loss: 1.0996  time: 0.5457  data: 0.0000  max mem: 31523
Train: data epoch: [2]  [ 999/1000]  eta: 0:00:00  lr: 0.000010  loss: 1.2258  time: 0.5189  data: 0.0000  max mem: 31523
Train: data epoch: [2] Total time: 0:08:48 (0.5287 s / it)
Train: data epoch: [3]  [   0/1000]  eta: 0:09:09  lr: 0.000010  loss: 1.0397  time: 0.5491  data: 0.0000  max mem: 31523
Train: data epoch: [3]  [  50/1000]  eta: 0:10:14  lr: 0.000010  loss: 1.3328  time: 0.4474  data: 0.0000  max mem: 31523
Train: data epoch: [3]  [ 100/1000]  eta: 0:08:17  lr: 0.000010  loss: 1.3149  time: 0.4581  data: 0.0000  max mem: 31523
Train: data epoch: [3]  [ 150/1000]  eta: 0:08:13  lr: 0.000010  loss: 0.6814  time: 0.4521  data: 0.0000  max mem: 31523
Train: data epoch: [3]  [ 200/1000]  eta: 0:08:14  lr: 0.000010  loss: 0.4373  time: 0.4528  data: 0.0000  max mem: 31523
Train: data epoch: [3]  [ 250/1000]  eta: 0:07:53  lr: 0.000010  loss: 0.1139  time: 0.4488  data: 0.0000  max mem: 31523
Train: data epoch: [3]  [ 300/1000]  eta: 0:07:27  lr: 0.000010  loss: 1.1745  time: 0.4562  data: 0.0000  max mem: 31523
Train: data epoch: [3]  [ 350/1000]  eta: 0:06:56  lr: 0.000010  loss: 1.1945  time: 0.4490  data: 0.0000  max mem: 31523
Train: data epoch: [3]  [ 400/1000]  eta: 0:06:44  lr: 0.000010  loss: 1.2548  time: 0.9648  data: 0.0000  max mem: 31523
Train: data epoch: [3]  [ 450/1000]  eta: 0:06:32  lr: 0.000010  loss: 0.1963  time: 1.4027  data: 0.0000  max mem: 31523
Train: data epoch: [3]  [ 500/1000]  eta: 0:06:11  lr: 0.000010  loss: 0.7635  time: 1.4297  data: 0.0000  max mem: 31523
Train: data epoch: [3]  [ 550/1000]  eta: 0:06:01  lr: 0.000010  loss: 2.0094  time: 1.5218  data: 0.0000  max mem: 31523
Train: data epoch: [3]  [ 600/1000]  eta: 0:05:44  lr: 0.000010  loss: 0.7773  time: 1.7330  data: 0.0000  max mem: 31523
Train: data epoch: [3]  [ 650/1000]  eta: 0:05:04  lr: 0.000010  loss: 1.4129  time: 1.0314  data: 0.0000  max mem: 31523
Train: data epoch: [3]  [ 700/1000]  eta: 0:04:25  lr: 0.000010  loss: 2.7236  time: 0.8364  data: 0.0000  max mem: 31523
Train: data epoch: [3]  [ 750/1000]  eta: 0:03:40  lr: 0.000010  loss: 0.2761  time: 0.7655  data: 0.0000  max mem: 31523
Train: data epoch: [3]  [ 800/1000]  eta: 0:02:55  lr: 0.000010  loss: 2.6036  time: 0.7759  data: 0.0000  max mem: 31523
Train: data epoch: [3]  [ 850/1000]  eta: 0:02:12  lr: 0.000010  loss: 1.2989  time: 0.9600  data: 0.0000  max mem: 31523
Train: data epoch: [3]  [ 900/1000]  eta: 0:01:26  lr: 0.000010  loss: 1.3029  time: 0.4518  data: 0.0000  max mem: 31523
Train: data epoch: [3]  [ 950/1000]  eta: 0:00:42  lr: 0.000010  loss: 1.3387  time: 0.4480  data: 0.0000  max mem: 31523
Train: data epoch: [3]  [ 999/1000]  eta: 0:00:00  lr: 0.000010  loss: 1.4520  time: 0.5169  data: 0.0000  max mem: 31523
Train: data epoch: [3] Total time: 0:13:50 (0.8303 s / it)
Train: data epoch: [4]  [   0/1000]  eta: 0:07:27  lr: 0.000010  loss: 2.8576  time: 0.4471  data: 0.0000  max mem: 31523
Train: data epoch: [4]  [  50/1000]  eta: 0:07:06  lr: 0.000010  loss: 1.2422  time: 0.4484  data: 0.0000  max mem: 31523
Train: data epoch: [4]  [ 100/1000]  eta: 0:06:43  lr: 0.000010  loss: 1.2094  time: 0.4482  data: 0.0000  max mem: 31523
Train: data epoch: [4]  [ 150/1000]  eta: 0:06:21  lr: 0.000010  loss: 1.0084  time: 0.4488  data: 0.0000  max mem: 31523
Train: data epoch: [4]  [ 200/1000]  eta: 0:05:58  lr: 0.000010  loss: 1.3265  time: 0.4487  data: 0.0000  max mem: 31523
Train: data epoch: [4]  [ 250/1000]  eta: 0:05:36  lr: 0.000010  loss: 2.8312  time: 0.4489  data: 0.0000  max mem: 31523
Train: data epoch: [4]  [ 300/1000]  eta: 0:05:14  lr: 0.000010  loss: 0.1283  time: 0.4487  data: 0.0000  max mem: 31523
Train: data epoch: [4]  [ 350/1000]  eta: 0:04:51  lr: 0.000010  loss: 1.8036  time: 0.4481  data: 0.0000  max mem: 31523
Train: data epoch: [4]  [ 400/1000]  eta: 0:04:29  lr: 0.000010  loss: 1.2739  time: 0.4479  data: 0.0000  max mem: 31523
Train: data epoch: [4]  [ 450/1000]  eta: 0:04:07  lr: 0.000010  loss: 0.3415  time: 0.4513  data: 0.0000  max mem: 31523
Train: data epoch: [4]  [ 500/1000]  eta: 0:03:44  lr: 0.000010  loss: 1.1750  time: 0.4486  data: 0.0000  max mem: 31523
Train: data epoch: [4]  [ 550/1000]  eta: 0:03:30  lr: 0.000010  loss: 1.2081  time: 0.9623  data: 0.0000  max mem: 31523
Train: data epoch: [4]  [ 600/1000]  eta: 0:03:07  lr: 0.000010  loss: 1.5261  time: 0.4513  data: 0.0000  max mem: 31523
Train: data epoch: [4]  [ 650/1000]  eta: 0:02:43  lr: 0.000010  loss: 1.2429  time: 0.4488  data: 0.0000  max mem: 31523
Train: data epoch: [4]  [ 700/1000]  eta: 0:02:19  lr: 0.000010  loss: 2.3214  time: 0.4525  data: 0.0000  max mem: 31523
Train: data epoch: [4]  [ 750/1000]  eta: 0:01:56  lr: 0.000010  loss: 1.4920  time: 0.4500  data: 0.0000  max mem: 31523
Train: data epoch: [4]  [ 800/1000]  eta: 0:01:32  lr: 0.000010  loss: 2.0883  time: 0.4469  data: 0.0000  max mem: 31523
Train: data epoch: [4]  [ 850/1000]  eta: 0:01:09  lr: 0.000010  loss: 1.2690  time: 0.4488  data: 0.0000  max mem: 31523
Train: data epoch: [4]  [ 900/1000]  eta: 0:00:46  lr: 0.000010  loss: 1.2686  time: 0.4569  data: 0.0000  max mem: 31523
Train: data epoch: [4]  [ 950/1000]  eta: 0:00:23  lr: 0.000010  loss: 1.9962  time: 0.4495  data: 0.0000  max mem: 31523
Train: data epoch: [4]  [ 999/1000]  eta: 0:00:00  lr: 0.000010  loss: 1.2497  time: 0.5035  data: 0.0000  max mem: 31523
Train: data epoch: [4] Total time: 0:07:42 (0.4628 s / it)
Train: data epoch: [5]  [   0/1000]  eta: 0:07:24  lr: 0.000010  loss: 2.5288  time: 0.4442  data: 0.0000  max mem: 31523
Train: data epoch: [5]  [  50/1000]  eta: 0:07:13  lr: 0.000010  loss: 0.2748  time: 0.4683  data: 0.0000  max mem: 31523
Train: data epoch: [5]  [ 100/1000]  eta: 0:06:52  lr: 0.000010  loss: 0.3766  time: 0.4483  data: 0.0000  max mem: 31523
Train: data epoch: [5]  [ 150/1000]  eta: 0:07:00  lr: 0.000010  loss: 0.4469  time: 0.6286  data: 0.0000  max mem: 31523
Train: data epoch: [5]  [ 200/1000]  eta: 0:06:50  lr: 0.000010  loss: 0.5252  time: 0.7473  data: 0.0000  max mem: 31523
Train: data epoch: [5]  [ 250/1000]  eta: 0:06:15  lr: 0.000010  loss: 1.3675  time: 0.4479  data: 0.0000  max mem: 31523
Train: data epoch: [5]  [ 300/1000]  eta: 0:05:45  lr: 0.000010  loss: 2.0313  time: 0.4645  data: 0.0000  max mem: 31566
Train: data epoch: [5]  [ 350/1000]  eta: 0:05:16  lr: 0.000010  loss: 1.3592  time: 0.4501  data: 0.0000  max mem: 31566
Train: data epoch: [5]  [ 400/1000]  eta: 0:04:49  lr: 0.000010  loss: 0.2580  time: 0.4523  data: 0.0000  max mem: 31566
Train: data epoch: [5]  [ 450/1000]  eta: 0:04:23  lr: 0.000010  loss: 1.1464  time: 0.4502  data: 0.0000  max mem: 31566
Train: data epoch: [5]  [ 500/1000]  eta: 0:03:58  lr: 0.000010  loss: 2.1960  time: 0.4595  data: 0.0000  max mem: 31566
Train: data epoch: [5]  [ 550/1000]  eta: 0:03:33  lr: 0.000010  loss: 1.5664  time: 0.4511  data: 0.0000  max mem: 31566
Train: data epoch: [5]  [ 600/1000]  eta: 0:03:08  lr: 0.000010  loss: 1.8712  time: 0.4597  data: 0.0000  max mem: 31566
Train: data epoch: [5]  [ 650/1000]  eta: 0:02:45  lr: 0.000010  loss: 2.3705  time: 0.4705  data: 0.0000  max mem: 31566
Train: data epoch: [5]  [ 700/1000]  eta: 0:02:21  lr: 0.000010  loss: 1.1666  time: 0.4495  data: 0.0000  max mem: 31566
Train: data epoch: [5]  [ 750/1000]  eta: 0:01:58  lr: 0.000010  loss: 3.1675  time: 0.5646  data: 0.0000  max mem: 31566
Train: data epoch: [5]  [ 800/1000]  eta: 0:01:34  lr: 0.000010  loss: 0.3285  time: 0.4492  data: 0.0000  max mem: 31658
Train: data epoch: [5]  [ 850/1000]  eta: 0:01:11  lr: 0.000010  loss: 1.1225  time: 0.4485  data: 0.0000  max mem: 31658
Train: data epoch: [5]  [ 900/1000]  eta: 0:00:47  lr: 0.000010  loss: 1.9267  time: 0.4487  data: 0.0000  max mem: 31658
Train: data epoch: [5]  [ 950/1000]  eta: 0:00:23  lr: 0.000010  loss: 1.3594  time: 0.4501  data: 0.0000  max mem: 31658
Train: data epoch: [5]  [ 999/1000]  eta: 0:00:00  lr: 0.000010  loss: 1.0913  time: 0.5183  data: 0.0000  max mem: 31658
Train: data epoch: [5] Total time: 0:07:55 (0.4759 s / it)
Train: data epoch: [6]  [   0/1000]  eta: 0:07:24  lr: 0.000010  loss: 1.2397  time: 0.4445  data: 0.0000  max mem: 31658
Train: data epoch: [6]  [  50/1000]  eta: 0:07:09  lr: 0.000010  loss: 1.3491  time: 0.4477  data: 0.0000  max mem: 31658
Train: data epoch: [6]  [ 100/1000]  eta: 0:06:52  lr: 0.000010  loss: 1.4113  time: 0.4728  data: 0.0000  max mem: 31658
Train: data epoch: [6]  [ 150/1000]  eta: 0:06:26  lr: 0.000010  loss: 1.0336  time: 0.4476  data: 0.0000  max mem: 31658
Train: data epoch: [6]  [ 200/1000]  eta: 0:06:06  lr: 0.000010  loss: 1.2993  time: 0.4600  data: 0.0000  max mem: 31658
Train: data epoch: [6]  [ 250/1000]  eta: 0:05:51  lr: 0.000010  loss: 2.2121  time: 0.4490  data: 0.0000  max mem: 31658
Train: data epoch: [6]  [ 300/1000]  eta: 0:05:36  lr: 0.000010  loss: 0.2978  time: 0.6669  data: 0.0000  max mem: 31658
Train: data epoch: [6]  [ 350/1000]  eta: 0:05:09  lr: 0.000010  loss: 1.2006  time: 0.4488  data: 0.0000  max mem: 31658
Train: data epoch: [6]  [ 400/1000]  eta: 0:04:46  lr: 0.000010  loss: 1.2441  time: 0.5245  data: 0.0000  max mem: 31658
Train: data epoch: [6]  [ 450/1000]  eta: 0:04:23  lr: 0.000010  loss: 0.3789  time: 0.4523  data: 0.0000  max mem: 31658
Train: data epoch: [6]  [ 500/1000]  eta: 0:03:58  lr: 0.000010  loss: 2.1129  time: 0.4631  data: 0.0000  max mem: 31658
Train: data epoch: [6]  [ 550/1000]  eta: 0:03:35  lr: 0.000010  loss: 0.2115  time: 0.4499  data: 0.0000  max mem: 31658
Train: data epoch: [6]  [ 600/1000]  eta: 0:03:15  lr: 0.000010  loss: 1.5601  time: 0.4723  data: 0.0000  max mem: 32036
Train: data epoch: [6]  [ 650/1000]  eta: 0:02:50  lr: 0.000010  loss: 1.3439  time: 0.4496  data: 0.0000  max mem: 32036
Train: data epoch: [6]  [ 700/1000]  eta: 0:02:25  lr: 0.000010  loss: 1.3225  time: 0.4480  data: 0.0000  max mem: 32036
Train: data epoch: [6]  [ 750/1000]  eta: 0:02:01  lr: 0.000010  loss: 1.1097  time: 0.6776  data: 0.0000  max mem: 32036
Train: data epoch: [6]  [ 800/1000]  eta: 0:01:36  lr: 0.000010  loss: 1.3952  time: 0.4467  data: 0.0000  max mem: 32036
Train: data epoch: [6]  [ 850/1000]  eta: 0:01:18  lr: 0.000010  loss: 0.7292  time: 2.1849  data: 0.0000  max mem: 32036
Train: data epoch: [6]  [ 900/1000]  eta: 0:00:54  lr: 0.000010  loss: 0.3086  time: 0.6268  data: 0.0000  max mem: 32036
Train: data epoch: [6]  [ 950/1000]  eta: 0:00:27  lr: 0.000010  loss: 0.4616  time: 0.4476  data: 0.0000  max mem: 32036
Train: data epoch: [6]  [ 999/1000]  eta: 0:00:00  lr: 0.000010  loss: 1.2165  time: 0.7885  data: 0.0000  max mem: 32036
Train: data epoch: [6] Total time: 0:09:07 (0.5475 s / it)
Train: data epoch: [7]  [   0/1000]  eta: 3:00:27  lr: 0.000010  loss: 1.2650  time: 10.8278  data: 0.0000  max mem: 32036
Train: data epoch: [7]  [  50/1000]  eta: 0:10:50  lr: 0.000010  loss: 1.8922  time: 0.4482  data: 0.0000  max mem: 32036
Train: data epoch: [7]  [ 100/1000]  eta: 0:08:31  lr: 0.000010  loss: 2.1755  time: 0.4523  data: 0.0000  max mem: 32036
Train: data epoch: [7]  [ 150/1000]  eta: 0:09:02  lr: 0.000010  loss: 1.2790  time: 0.4484  data: 0.0000  max mem: 32036
Train: data epoch: [7]  [ 200/1000]  eta: 0:07:55  lr: 0.000010  loss: 0.2412  time: 0.4865  data: 0.0000  max mem: 32036
Train: data epoch: [7]  [ 250/1000]  eta: 0:07:51  lr: 0.000010  loss: 0.1876  time: 1.2399  data: 0.0000  max mem: 32036
Train: data epoch: [7]  [ 300/1000]  eta: 0:06:59  lr: 0.000010  loss: 1.3635  time: 0.4519  data: 0.0000  max mem: 32036
Train: data epoch: [7]  [ 350/1000]  eta: 0:06:15  lr: 0.000010  loss: 2.2178  time: 0.4480  data: 0.0000  max mem: 32036
Train: data epoch: [7]  [ 400/1000]  eta: 0:06:54  lr: 0.000010  loss: 1.3044  time: 1.6846  data: 0.0000  max mem: 32036
Train: data epoch: [7]  [ 450/1000]  eta: 0:07:08  lr: 0.000010  loss: 1.7425  time: 1.6760  data: 0.0000  max mem: 32036
Train: data epoch: [7]  [ 500/1000]  eta: 0:07:06  lr: 0.000010  loss: 0.6604  time: 0.4688  data: 0.0000  max mem: 32036
Train: data epoch: [7]  [ 550/1000]  eta: 0:07:13  lr: 0.000010  loss: 1.2524  time: 0.4684  data: 0.0000  max mem: 32036
Train: data epoch: [7]  [ 600/1000]  eta: 0:06:08  lr: 0.000009  loss: 1.6861  time: 0.4509  data: 0.0000  max mem: 32036
Train: data epoch: [7]  [ 650/1000]  eta: 0:05:09  lr: 0.000009  loss: 0.9787  time: 0.4487  data: 0.0000  max mem: 32036
Train: data epoch: [7]  [ 700/1000]  eta: 0:04:32  lr: 0.000009  loss: 1.1784  time: 0.4470  data: 0.0000  max mem: 32036
Train: data epoch: [7]  [ 750/1000]  eta: 0:03:53  lr: 0.000009  loss: 0.1041  time: 1.7587  data: 0.0000  max mem: 32036
Train: data epoch: [7]  [ 800/1000]  eta: 0:03:00  lr: 0.000009  loss: 0.2128  time: 0.4556  data: 0.0000  max mem: 32036
Train: data epoch: [7]  [ 850/1000]  eta: 0:02:15  lr: 0.000009  loss: 1.3696  time: 0.4481  data: 0.0000  max mem: 32036
Train: data epoch: [7]  [ 900/1000]  eta: 0:01:30  lr: 0.000009  loss: 1.2532  time: 1.6542  data: 0.0000  max mem: 32036
Train: data epoch: [7]  [ 950/1000]  eta: 0:00:43  lr: 0.000009  loss: 1.6504  time: 0.4477  data: 0.0000  max mem: 32036
Train: data epoch: [7]  [ 999/1000]  eta: 0:00:00  lr: 0.000009  loss: 0.4292  time: 1.7021  data: 0.0000  max mem: 32036
Train: data epoch: [7] Total time: 0:14:59 (0.9000 s / it)
Train: data epoch: [8]  [   0/1000]  eta: 0:08:15  lr: 0.000009  loss: 1.2587  time: 0.4951  data: 0.0000  max mem: 32036
Train: data epoch: [8]  [  50/1000]  eta: 0:07:06  lr: 0.000009  loss: 0.7965  time: 0.4486  data: 0.0000  max mem: 32036
Train: data epoch: [8]  [ 100/1000]  eta: 0:21:38  lr: 0.000009  loss: 0.2333  time: 0.4540  data: 0.0000  max mem: 32036
Train: data epoch: [8]  [ 150/1000]  eta: 0:16:41  lr: 0.000009  loss: 1.7738  time: 0.4619  data: 0.0000  max mem: 32036
Train: data epoch: [8]  [ 200/1000]  eta: 0:15:15  lr: 0.000009  loss: 0.3452  time: 0.4537  data: 0.0000  max mem: 32036
Train: data epoch: [8]  [ 250/1000]  eta: 0:13:12  lr: 0.000009  loss: 1.1287  time: 0.5537  data: 0.0000  max mem: 32036
Train: data epoch: [8]  [ 300/1000]  eta: 0:12:49  lr: 0.000009  loss: 2.2205  time: 1.1950  data: 0.0000  max mem: 32036
Train: data epoch: [8]  [ 350/1000]  eta: 0:12:06  lr: 0.000009  loss: 0.5021  time: 1.1838  data: 0.0000  max mem: 32036
Train: data epoch: [8]  [ 400/1000]  eta: 0:11:42  lr: 0.000009  loss: 1.2480  time: 0.6626  data: 0.0000  max mem: 32036
Train: data epoch: [8]  [ 450/1000]  eta: 0:11:18  lr: 0.000009  loss: 0.7610  time: 1.2771  data: 0.0000  max mem: 32036
Train: data epoch: [8]  [ 500/1000]  eta: 0:11:09  lr: 0.000009  loss: 1.2714  time: 2.6693  data: 0.0000  max mem: 32036
Train: data epoch: [8]  [ 550/1000]  eta: 0:10:10  lr: 0.000009  loss: 1.2709  time: 2.1143  data: 0.0000  max mem: 32036
Train: data epoch: [8]  [ 600/1000]  eta: 0:09:17  lr: 0.000009  loss: 2.4049  time: 1.6654  data: 0.0000  max mem: 32036
Train: data epoch: [8]  [ 650/1000]  eta: 0:08:49  lr: 0.000009  loss: 0.8742  time: 3.7716  data: 0.0000  max mem: 32036
Train: data epoch: [8]  [ 700/1000]  eta: 0:07:29  lr: 0.000009  loss: 1.2378  time: 0.4627  data: 0.0000  max mem: 32036
Train: data epoch: [8]  [ 750/1000]  eta: 0:05:56  lr: 0.000009  loss: 0.3912  time: 0.4497  data: 0.0000  max mem: 32036
Train: data epoch: [8]  [ 800/1000]  eta: 0:04:36  lr: 0.000009  loss: 0.9780  time: 0.4557  data: 0.0000  max mem: 32036
Train: data epoch: [8]  [ 850/1000]  eta: 0:03:19  lr: 0.000009  loss: 1.3213  time: 0.4515  data: 0.0000  max mem: 32036
Train: data epoch: [8]  [ 900/1000]  eta: 0:02:07  lr: 0.000009  loss: 1.9950  time: 0.4491  data: 0.0000  max mem: 32036
Train: data epoch: [8]  [ 950/1000]  eta: 0:01:02  lr: 0.000009  loss: 2.1492  time: 0.4582  data: 0.0000  max mem: 32036
Train: data epoch: [8]  [ 999/1000]  eta: 0:00:01  lr: 0.000009  loss: 1.3582  time: 0.5117  data: 0.0000  max mem: 32036
Train: data epoch: [8] Total time: 0:20:17 (1.2177 s / it)
Train: data epoch: [9]  [   0/1000]  eta: 0:09:01  lr: 0.000009  loss: 2.5376  time: 0.5415  data: 0.0000  max mem: 32036
Train: data epoch: [9]  [  50/1000]  eta: 0:17:04  lr: 0.000009  loss: 1.4285  time: 0.4468  data: 0.0000  max mem: 32036
Train: data epoch: [9]  [ 100/1000]  eta: 0:15:04  lr: 0.000009  loss: 0.3023  time: 1.6569  data: 0.0000  max mem: 32036
Train: data epoch: [9]  [ 150/1000]  eta: 0:11:37  lr: 0.000009  loss: 1.8750  time: 0.4484  data: 0.0000  max mem: 32036
Train: data epoch: [9]  [ 200/1000]  eta: 0:09:43  lr: 0.000009  loss: 1.9696  time: 0.4486  data: 0.0000  max mem: 32036
Train: data epoch: [9]  [ 250/1000]  eta: 0:10:04  lr: 0.000009  loss: 1.2727  time: 0.4493  data: 0.0000  max mem: 32036
Train: data epoch: [9]  [ 300/1000]  eta: 0:09:35  lr: 0.000009  loss: 1.3219  time: 1.5559  data: 0.0000  max mem: 32036
Train: data epoch: [9]  [ 350/1000]  eta: 0:08:19  lr: 0.000009  loss: 1.1079  time: 0.4466  data: 0.0000  max mem: 32036
Train: data epoch: [9]  [ 400/1000]  eta: 0:07:17  lr: 0.000009  loss: 1.8271  time: 0.4481  data: 0.0000  max mem: 32036
Train: data epoch: [9]  [ 450/1000]  eta: 0:07:12  lr: 0.000009  loss: 0.1277  time: 0.4467  data: 0.0000  max mem: 32036
Train: data epoch: [9]  [ 500/1000]  eta: 0:06:48  lr: 0.000009  loss: 0.3952  time: 2.0326  data: 0.0000  max mem: 32036
Train: data epoch: [9]  [ 550/1000]  eta: 0:05:52  lr: 0.000009  loss: 0.3974  time: 0.4473  data: 0.0000  max mem: 32036
Train: data epoch: [9]  [ 600/1000]  eta: 0:05:25  lr: 0.000009  loss: 2.3752  time: 0.4532  data: 0.0000  max mem: 32036
Train: data epoch: [9]  [ 650/1000]  eta: 0:04:35  lr: 0.000009  loss: 0.4618  time: 0.4493  data: 0.0000  max mem: 32036
Train: data epoch: [9]  [ 700/1000]  eta: 0:04:00  lr: 0.000009  loss: 1.3335  time: 0.4523  data: 0.0000  max mem: 32036
Train: data epoch: [9]  [ 750/1000]  eta: 0:03:14  lr: 0.000009  loss: 1.5258  time: 0.4500  data: 0.0000  max mem: 32036
Train: data epoch: [9]  [ 800/1000]  eta: 0:02:36  lr: 0.000009  loss: 0.4199  time: 0.4544  data: 0.0000  max mem: 32036
Train: data epoch: [9]  [ 850/1000]  eta: 0:01:54  lr: 0.000009  loss: 2.1468  time: 0.4487  data: 0.0000  max mem: 32036
Train: data epoch: [9]  [ 900/1000]  eta: 0:01:17  lr: 0.000009  loss: 1.4860  time: 0.4507  data: 0.0000  max mem: 32036
Train: data epoch: [9]  [ 950/1000]  eta: 0:00:37  lr: 0.000009  loss: 1.8271  time: 0.4585  data: 0.0000  max mem: 32036
Train: data epoch: [9]  [ 999/1000]  eta: 0:00:00  lr: 0.000009  loss: 2.3989  time: 1.7597  data: 0.0000  max mem: 32036
Train: data epoch: [9] Total time: 0:12:54 (0.7746 s / it)
Train: data epoch: [10]  [   0/1000]  eta: 0:09:29  lr: 0.000009  loss: 0.7883  time: 0.5691  data: 0.0000  max mem: 32036
Train: data epoch: [10]  [  50/1000]  eta: 0:07:09  lr: 0.000009  loss: 1.4504  time: 0.4493  data: 0.0000  max mem: 32036
Train: data epoch: [10]  [ 100/1000]  eta: 0:10:26  lr: 0.000009  loss: 2.4021  time: 0.4501  data: 0.0000  max mem: 32036
Train: data epoch: [10]  [ 150/1000]  eta: 0:08:42  lr: 0.000009  loss: 1.7116  time: 0.4480  data: 0.0000  max mem: 32036
Train: data epoch: [10]  [ 200/1000]  eta: 0:09:57  lr: 0.000009  loss: 1.1258  time: 2.1930  data: 0.0000  max mem: 32036
Train: data epoch: [10]  [ 250/1000]  eta: 0:08:35  lr: 0.000009  loss: 1.9188  time: 0.4476  data: 0.0000  max mem: 32036
Train: data epoch: [10]  [ 300/1000]  eta: 0:09:12  lr: 0.000009  loss: 1.4871  time: 0.4540  data: 0.0000  max mem: 32036
Train: data epoch: [10]  [ 350/1000]  eta: 0:09:38  lr: 0.000009  loss: 1.1234  time: 3.0537  data: 0.0000  max mem: 32036
Train: data epoch: [10]  [ 400/1000]  eta: 0:10:50  lr: 0.000009  loss: 0.5044  time: 5.4382  data: 0.0000  max mem: 32036
Train: data epoch: [10]  [ 450/1000]  eta: 0:09:17  lr: 0.000009  loss: 1.1684  time: 0.4478  data: 0.0000  max mem: 32036
Train: data epoch: [10]  [ 500/1000]  eta: 0:10:41  lr: 0.000009  loss: 0.6356  time: 8.6291  data: 0.0000  max mem: 32036
Train: data epoch: [10]  [ 550/1000]  eta: 0:10:52  lr: 0.000009  loss: 2.2319  time: 1.5164  data: 0.0000  max mem: 32036
Train: data epoch: [10]  [ 600/1000]  eta: 0:09:06  lr: 0.000009  loss: 1.2118  time: 0.4555  data: 0.0000  max mem: 32036
Train: data epoch: [10]  [ 650/1000]  eta: 0:07:59  lr: 0.000009  loss: 1.2462  time: 2.8484  data: 0.0000  max mem: 32036
Train: data epoch: [10]  [ 700/1000]  eta: 0:06:31  lr: 0.000009  loss: 2.1253  time: 0.4736  data: 0.0000  max mem: 32036
Train: data epoch: [10]  [ 750/1000]  eta: 0:05:23  lr: 0.000009  loss: 1.4057  time: 0.4472  data: 0.0000  max mem: 32036
Train: data epoch: [10]  [ 800/1000]  eta: 0:04:20  lr: 0.000009  loss: 0.3365  time: 2.9276  data: 0.0000  max mem: 32036
Train: data epoch: [10]  [ 850/1000]  eta: 0:03:18  lr: 0.000009  loss: 0.3591  time: 3.3692  data: 0.0000  max mem: 32036
Train: data epoch: [10]  [ 900/1000]  eta: 0:02:07  lr: 0.000009  loss: 1.3800  time: 0.4548  data: 0.0000  max mem: 32036
Train: data epoch: [10]  [ 950/1000]  eta: 0:01:06  lr: 0.000009  loss: 1.1992  time: 2.5637  data: 0.0000  max mem: 32036
Train: data epoch: [10]  [ 999/1000]  eta: 0:00:01  lr: 0.000009  loss: 3.0240  time: 3.3963  data: 0.0000  max mem: 32036
Train: data epoch: [10] Total time: 0:22:37 (1.3576 s / it)
Train: data epoch: [11]  [   0/1000]  eta: 0:09:27  lr: 0.000009  loss: 0.3375  time: 0.5672  data: 0.0000  max mem: 32036
Train: data epoch: [11]  [  50/1000]  eta: 0:20:36  lr: 0.000009  loss: 0.5177  time: 0.4472  data: 0.0000  max mem: 32036
Train: data epoch: [11]  [ 100/1000]  eta: 0:22:11  lr: 0.000009  loss: 1.3765  time: 2.2898  data: 0.0000  max mem: 32036
Train: data epoch: [11]  [ 150/1000]  eta: 0:23:09  lr: 0.000009  loss: 1.2675  time: 1.3366  data: 0.0000  max mem: 32036
Train: data epoch: [11]  [ 200/1000]  eta: 0:24:14  lr: 0.000009  loss: 1.2922  time: 2.1981  data: 0.0000  max mem: 32036
Train: data epoch: [11]  [ 250/1000]  eta: 0:23:13  lr: 0.000009  loss: 0.1655  time: 2.9713  data: 0.0000  max mem: 32036
Train: data epoch: [11]  [ 300/1000]  eta: 0:25:44  lr: 0.000009  loss: 0.1981  time: 5.4347  data: 0.0000  max mem: 32036
Train: data epoch: [11]  [ 350/1000]  eta: 0:30:10  lr: 0.000009  loss: 1.2693  time: 7.4848  data: 0.0000  max mem: 32036
Train: data epoch: [11]  [ 400/1000]  eta: 0:39:20  lr: 0.000009  loss: 1.5645  time: 12.0427  data: 0.0000  max mem: 32036
Train: data epoch: [11]  [ 450/1000]  eta: 0:41:13  lr: 0.000009  loss: 1.3179  time: 7.2160  data: 0.0000  max mem: 32036
Train: data epoch: [11]  [ 500/1000]  eta: 0:34:49  lr: 0.000009  loss: 1.2513  time: 2.5829  data: 0.0000  max mem: 32036
Train: data epoch: [11]  [ 550/1000]  eta: 0:28:47  lr: 0.000009  loss: 2.6156  time: 0.4476  data: 0.0000  max mem: 32036
Train: data epoch: [11]  [ 600/1000]  eta: 0:24:30  lr: 0.000009  loss: 1.3377  time: 0.4748  data: 0.0000  max mem: 32036
Train: data epoch: [11]  [ 650/1000]  eta: 0:20:14  lr: 0.000009  loss: 1.2706  time: 1.7511  data: 0.0000  max mem: 32036
Train: data epoch: [11]  [ 700/1000]  eta: 0:16:16  lr: 0.000009  loss: 1.0236  time: 0.4697  data: 0.0000  max mem: 32036
Train: data epoch: [11]  [ 750/1000]  eta: 0:12:55  lr: 0.000009  loss: 3.3782  time: 0.4554  data: 0.0000  max mem: 32036
Train: data epoch: [11]  [ 800/1000]  eta: 0:09:47  lr: 0.000009  loss: 0.1994  time: 0.4645  data: 0.0000  max mem: 32036
Train: data epoch: [11]  [ 850/1000]  eta: 0:07:08  lr: 0.000009  loss: 1.3153  time: 3.3290  data: 0.0000  max mem: 32036
Train: data epoch: [11]  [ 900/1000]  eta: 0:04:35  lr: 0.000009  loss: 1.1328  time: 0.4632  data: 0.0000  max mem: 32036
Train: data epoch: [11]  [ 950/1000]  eta: 0:02:11  lr: 0.000009  loss: 0.1581  time: 0.4486  data: 0.0000  max mem: 32036
Train: data epoch: [11]  [ 999/1000]  eta: 0:00:02  lr: 0.000009  loss: 0.2330  time: 0.5315  data: 0.0000  max mem: 32036
Train: data epoch: [11] Total time: 0:42:49 (2.5698 s / it)
Train: data epoch: [12]  [   0/1000]  eta: 0:09:16  lr: 0.000009  loss: 0.5005  time: 0.5562  data: 0.0000  max mem: 32036
Train: data epoch: [12]  [  50/1000]  eta: 0:07:10  lr: 0.000009  loss: 0.5648  time: 0.4583  data: 0.0000  max mem: 32036
Train: data epoch: [12]  [ 100/1000]  eta: 0:06:45  lr: 0.000009  loss: 1.2282  time: 0.4462  data: 0.0000  max mem: 32036
Train: data epoch: [12]  [ 150/1000]  eta: 0:06:22  lr: 0.000009  loss: 1.3376  time: 0.4495  data: 0.0000  max mem: 32036
Train: data epoch: [12]  [ 200/1000]  eta: 0:05:59  lr: 0.000009  loss: 2.1463  time: 0.4477  data: 0.0000  max mem: 32036
Train: data epoch: [12]  [ 250/1000]  eta: 0:06:31  lr: 0.000009  loss: 1.2791  time: 0.4804  data: 0.0000  max mem: 32036
Train: data epoch: [12]  [ 300/1000]  eta: 0:05:57  lr: 0.000009  loss: 1.2495  time: 0.4515  data: 0.0000  max mem: 32036
Train: data epoch: [12]  [ 350/1000]  eta: 0:05:26  lr: 0.000009  loss: 0.5907  time: 0.4487  data: 0.0000  max mem: 32036
Train: data epoch: [12]  [ 400/1000]  eta: 0:04:57  lr: 0.000009  loss: 1.8237  time: 0.4493  data: 0.0000  max mem: 32036
Train: data epoch: [12]  [ 450/1000]  eta: 0:04:46  lr: 0.000009  loss: 0.8351  time: 0.9434  data: 0.0000  max mem: 32036
Train: data epoch: [12]  [ 500/1000]  eta: 0:04:19  lr: 0.000009  loss: 1.9317  time: 0.4654  data: 0.0000  max mem: 32036
Train: data epoch: [12]  [ 550/1000]  eta: 0:04:12  lr: 0.000009  loss: 1.2931  time: 0.4487  data: 0.0000  max mem: 32036
Train: data epoch: [12]  [ 600/1000]  eta: 0:03:40  lr: 0.000009  loss: 0.1965  time: 0.4633  data: 0.0000  max mem: 32036
Train: data epoch: [12]  [ 650/1000]  eta: 0:03:10  lr: 0.000009  loss: 0.2608  time: 0.4482  data: 0.0000  max mem: 32036
Train: data epoch: [12]  [ 700/1000]  eta: 0:02:47  lr: 0.000009  loss: 1.5542  time: 0.5008  data: 0.0000  max mem: 32036
Train: data epoch: [12]  [ 750/1000]  eta: 0:02:17  lr: 0.000009  loss: 1.7013  time: 0.4470  data: 0.0000  max mem: 32036
Train: data epoch: [12]  [ 800/1000]  eta: 0:01:49  lr: 0.000009  loss: 0.9426  time: 0.4681  data: 0.0000  max mem: 32036
Train: data epoch: [12]  [ 850/1000]  eta: 0:01:25  lr: 0.000009  loss: 1.3215  time: 1.2905  data: 0.0000  max mem: 32036
Train: data epoch: [12]  [ 900/1000]  eta: 0:00:56  lr: 0.000009  loss: 0.5691  time: 0.4510  data: 0.0000  max mem: 32036
Train: data epoch: [12]  [ 950/1000]  eta: 0:00:29  lr: 0.000009  loss: 2.1833  time: 2.1521  data: 0.0000  max mem: 32036
Train: data epoch: [12]  [ 999/1000]  eta: 0:00:00  lr: 0.000009  loss: 0.2461  time: 0.5166  data: 0.0000  max mem: 32036
Train: data epoch: [12] Total time: 0:09:47 (0.5873 s / it)
Train: data epoch: [13]  [   0/1000]  eta: 0:09:13  lr: 0.000009  loss: 0.6182  time: 0.5537  data: 0.0000  max mem: 32036
Train: data epoch: [13]  [  50/1000]  eta: 0:24:21  lr: 0.000009  loss: 1.3258  time: 0.4565  data: 0.0000  max mem: 32036
Train: data epoch: [13]  [ 100/1000]  eta: 0:19:40  lr: 0.000009  loss: 1.2956  time: 0.4604  data: 0.0000  max mem: 32036
Train: data epoch: [13]  [ 150/1000]  eta: 0:19:16  lr: 0.000009  loss: 1.1654  time: 2.9790  data: 0.0000  max mem: 32036
Train: data epoch: [13]  [ 200/1000]  eta: 0:18:08  lr: 0.000009  loss: 1.1792  time: 2.7224  data: 0.0000  max mem: 32036
Train: data epoch: [13]  [ 250/1000]  eta: 0:14:43  lr: 0.000009  loss: 1.5919  time: 0.4461  data: 0.0000  max mem: 32036
Train: data epoch: [13]  [ 300/1000]  eta: 0:13:36  lr: 0.000009  loss: 1.3005  time: 0.4524  data: 0.0000  max mem: 32036
Train: data epoch: [13]  [ 350/1000]  eta: 0:12:17  lr: 0.000009  loss: 1.4052  time: 1.6877  data: 0.0000  max mem: 32036
Train: data epoch: [13]  [ 400/1000]  eta: 0:10:29  lr: 0.000008  loss: 0.2418  time: 0.4544  data: 0.0000  max mem: 32036
Train: data epoch: [13]  [ 450/1000]  eta: 0:10:31  lr: 0.000008  loss: 1.3341  time: 0.4466  data: 0.0000  max mem: 32036
Train: data epoch: [13]  [ 500/1000]  eta: 0:09:27  lr: 0.000008  loss: 1.2428  time: 0.4508  data: 0.0000  max mem: 32036
Train: data epoch: [13]  [ 550/1000]  eta: 0:08:41  lr: 0.000008  loss: 2.4623  time: 0.4472  data: 0.0000  max mem: 32036
Train: data epoch: [13]  [ 600/1000]  eta: 0:08:05  lr: 0.000008  loss: 2.5437  time: 2.0421  data: 0.0000  max mem: 32036
Train: data epoch: [13]  [ 650/1000]  eta: 0:07:00  lr: 0.000008  loss: 1.4975  time: 0.4468  data: 0.0000  max mem: 32036
Train: data epoch: [13]  [ 700/1000]  eta: 0:06:41  lr: 0.000008  loss: 1.6664  time: 2.9396  data: 0.0000  max mem: 32036
Train: data epoch: [13]  [ 750/1000]  eta: 0:05:34  lr: 0.000008  loss: 0.2430  time: 2.7191  data: 0.0000  max mem: 32036
Train: data epoch: [13]  [ 800/1000]  eta: 0:04:47  lr: 0.000008  loss: 1.2181  time: 4.0425  data: 0.0000  max mem: 32036
Train: data epoch: [13]  [ 850/1000]  eta: 0:03:51  lr: 0.000008  loss: 0.1935  time: 4.3486  data: 0.0000  max mem: 32036
Train: data epoch: [13]  [ 900/1000]  eta: 0:02:44  lr: 0.000008  loss: 0.8260  time: 5.4103  data: 0.0000  max mem: 32036
Train: data epoch: [13]  [ 950/1000]  eta: 0:01:32  lr: 0.000008  loss: 1.1331  time: 4.3715  data: 0.0000  max mem: 32036
Train: data epoch: [13]  [ 999/1000]  eta: 0:00:02  lr: 0.000008  loss: 0.7144  time: 6.6967  data: 0.0000  max mem: 32036
Train: data epoch: [13] Total time: 0:33:49 (2.0297 s / it)
Train: data epoch: [14]  [   0/1000]  eta: 1:24:28  lr: 0.000008  loss: 1.2456  time: 5.0685  data: 0.0000  max mem: 32036
Train: data epoch: [14]  [  50/1000]  eta: 1:33:54  lr: 0.000008  loss: 1.8644  time: 6.3923  data: 0.0000  max mem: 32036
Train: data epoch: [14]  [ 100/1000]  eta: 2:11:02  lr: 0.000008  loss: 1.9722  time: 14.0010  data: 0.0000  max mem: 32036
| distributed init (rank 1, world 4): env://
| distributed init (rank 0, world 4): env://
| distributed init (rank 2, world 4): env://
| distributed init (rank 3, world 4): env://
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/coco_captions/coco_karpathy_train.json
vis_path:/home/users/nus/idmwyk/scratch/temp/dataset/coco/images/train2014
loading dataset refcoco into memory...
creating index...
index created.
DONE (t=2.67s)
loading dataset refcoco+ into memory...
creating index...
index created.
DONE (t=2.98s)
loading dataset refcocog into memory...
creating index...
index created.
DONE (t=2.99s)
loading dataset refcoco into memory...
creating index...
index created.
DONE (t=3.05s)
loading dataset refcoco+ into memory...
creating index...
index created.
DONE (t=2.57s)
loading dataset refcocog into memory...
creating index...
index created.
DONE (t=2.28s)
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/gqa/train_balanced_questions.json
vis_path:/home/users/nus/idmwyk/scratch/temp/dataset/gqa/images/train2014
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/vqav2/vqa_train.json
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/vqav2/vqa_val.json
vis_path:/home/users/nus/idmwyk/scratch/temp/dataset/coco/images/train2014
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/okvqa/okvqa_train.json
vis_path:/home/users/nus/idmwyk/scratch/temp/dataset/coco/images/train2014
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/aokvqa/aokvqa_v1p0_train.json
vis_path:/home/users/nus/idmwyk/scratch/temp/dataset/coco/images/train2014
len(exist_annotation):11106
trainable params: 27262976 || all params: 8057524224 || trainable%: 0.33835425426081844
Position interpolate from 16x16 to 32x32
Load Minigpt-4-LLM Checkpoint: /home/users/nus/idmwyk/scratch/temp/output/saved-ckpt/minigptv4/stage1/20240907183/checkpoint_49.pth
model arch:
 MiniGPTv4(
  (llama_model): PeftModelForCausalLM(
    (base_model): LoraModel(
      (model): LlamaForCausalLM(
        (model): LlamaModel(
          (embed_tokens): Embedding(128256, 4096)
          (layers): ModuleList(
            (0-31): 32 x LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(
                  in_features=4096, out_features=4096, bias=False
                  (lora_dropout): Dropout(p=0.05, inplace=False)
                  (lora_A): Linear(in_features=4096, out_features=64, bias=False)
                  (lora_B): Linear(in_features=64, out_features=4096, bias=False)
                )
                (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
                (v_proj): Linear(
                  in_features=4096, out_features=1024, bias=False
                  (lora_dropout): Dropout(p=0.05, inplace=False)
                  (lora_A): Linear(in_features=4096, out_features=64, bias=False)
                  (lora_B): Linear(in_features=64, out_features=1024, bias=False)
                )
                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
                (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
                (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
                (act_fn): SiLU()
              )
              (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
              (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
            )
          )
          (norm): LlamaRMSNorm((4096,), eps=1e-05)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (lm_head): CastOutputToFloat(
          (0): Linear(in_features=4096, out_features=128256, bias=False)
        )
      )
    )
  )
  (visual_encoder): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0-38): 39 x Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (ln_vision): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)
  (local_attn): LocalAttention(
    (txt): Linear(in_features=4096, out_features=1408, bias=True)
    (img): Linear(in_features=1408, out_features=1408, bias=True)
  )
  (llama_proj): Linear(in_features=5632, out_features=4096, bias=True)
)
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.28.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.28.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.28.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.28.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.29.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.29.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.29.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.29.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.30.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.30.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.30.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.30.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.31.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.31.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.31.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.31.self_attn.v_proj.lora_B.weight
module.local_attn.txt.weight
module.local_attn.txt.bias
module.local_attn.img.weight
module.local_attn.img.bias
module.llama_proj.weight
module.llama_proj.bias
resume the checkpoint
batch sizes [[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]]
Train: data epoch: [14]  [   0/1000]  eta: 12:44:16  lr: 0.000008  loss: 0.4024  time: 45.8561  data: 0.0000  max mem: 29406
Train: data epoch: [14]  [  50/1000]  eta: 1:07:28  lr: 0.000008  loss: 1.2318  time: 2.4106  data: 0.0000  max mem: 30423
Train: data epoch: [14]  [ 100/1000]  eta: 0:39:33  lr: 0.000008  loss: 1.4584  time: 0.4578  data: 0.0000  max mem: 30509
Train: data epoch: [14]  [ 150/1000]  eta: 0:27:33  lr: 0.000008  loss: 1.8091  time: 0.4564  data: 0.0000  max mem: 30606
Train: data epoch: [14]  [ 200/1000]  eta: 0:21:00  lr: 0.000008  loss: 0.6788  time: 0.4503  data: 0.0000  max mem: 30606
Train: data epoch: [14]  [ 250/1000]  eta: 0:16:54  lr: 0.000008  loss: 1.3056  time: 0.4627  data: 0.0000  max mem: 30645
Train: data epoch: [14]  [ 300/1000]  eta: 0:14:10  lr: 0.000008  loss: 2.7867  time: 0.4558  data: 0.0000  max mem: 30933
Train: data epoch: [14]  [ 350/1000]  eta: 0:12:01  lr: 0.000008  loss: 0.6854  time: 0.4533  data: 0.0000  max mem: 30976
Train: data epoch: [14]  [ 400/1000]  eta: 0:10:18  lr: 0.000008  loss: 1.4281  time: 0.4846  data: 0.0000  max mem: 30976
Train: data epoch: [14]  [ 450/1000]  eta: 0:08:53  lr: 0.000008  loss: 0.1721  time: 0.4500  data: 0.0000  max mem: 30976
Train: data epoch: [14]  [ 500/1000]  eta: 0:07:39  lr: 0.000008  loss: 0.2231  time: 0.4783  data: 0.0000  max mem: 30976
Train: data epoch: [14]  [ 550/1000]  eta: 0:06:34  lr: 0.000008  loss: 0.3788  time: 0.4521  data: 0.0000  max mem: 30976
Train: data epoch: [14]  [ 600/1000]  eta: 0:05:38  lr: 0.000008  loss: 2.9649  time: 0.4514  data: 0.0000  max mem: 30976
Train: data epoch: [14]  [ 650/1000]  eta: 0:04:47  lr: 0.000008  loss: 1.2655  time: 0.6091  data: 0.0000  max mem: 30976
Train: data epoch: [14]  [ 700/1000]  eta: 0:03:59  lr: 0.000008  loss: 0.1577  time: 0.4525  data: 0.0000  max mem: 30976
Train: data epoch: [14]  [ 750/1000]  eta: 0:03:14  lr: 0.000008  loss: 1.1076  time: 0.4601  data: 0.0000  max mem: 30976
Train: data epoch: [14]  [ 800/1000]  eta: 0:02:31  lr: 0.000008  loss: 1.3789  time: 0.4537  data: 0.0000  max mem: 30976
Train: data epoch: [14]  [ 850/1000]  eta: 0:01:51  lr: 0.000008  loss: 1.7241  time: 0.5134  data: 0.0000  max mem: 31379
Train: data epoch: [14]  [ 900/1000]  eta: 0:01:13  lr: 0.000008  loss: 1.0598  time: 0.4557  data: 0.0000  max mem: 31379
Train: data epoch: [14]  [ 950/1000]  eta: 0:00:35  lr: 0.000008  loss: 0.6435  time: 0.4520  data: 0.0000  max mem: 31379
Train: data epoch: [14]  [ 999/1000]  eta: 0:00:00  lr: 0.000008  loss: 1.8405  time: 0.6200  data: 0.0000  max mem: 31379
Train: data epoch: [14] Total time: 0:11:50 (0.7107 s / it)
Train: data epoch: [15]  [   0/1000]  eta: 0:07:27  lr: 0.000008  loss: 2.4957  time: 0.4475  data: 0.0000  max mem: 31379
Train: data epoch: [15]  [  50/1000]  eta: 0:07:08  lr: 0.000008  loss: 0.5891  time: 0.4510  data: 0.0000  max mem: 31379
Train: data epoch: [15]  [ 100/1000]  eta: 0:09:10  lr: 0.000008  loss: 1.3099  time: 0.4519  data: 0.0000  max mem: 31379
Train: data epoch: [15]  [ 150/1000]  eta: 0:07:54  lr: 0.000008  loss: 1.2692  time: 0.4507  data: 0.0000  max mem: 31379
Train: data epoch: [15]  [ 200/1000]  eta: 0:07:05  lr: 0.000008  loss: 1.1573  time: 0.4507  data: 0.0000  max mem: 31379
Train: data epoch: [15]  [ 250/1000]  eta: 0:06:27  lr: 0.000008  loss: 1.2444  time: 0.4634  data: 0.0000  max mem: 31379
Train: data epoch: [15]  [ 300/1000]  eta: 0:06:20  lr: 0.000008  loss: 0.3715  time: 0.4533  data: 0.0000  max mem: 31379
Train: data epoch: [15]  [ 350/1000]  eta: 0:05:44  lr: 0.000008  loss: 1.4102  time: 0.4529  data: 0.0000  max mem: 31379
Train: data epoch: [15]  [ 400/1000]  eta: 0:05:38  lr: 0.000008  loss: 1.3706  time: 1.3223  data: 0.0000  max mem: 31379
Train: data epoch: [15]  [ 450/1000]  eta: 0:05:03  lr: 0.000008  loss: 2.6270  time: 0.4517  data: 0.0000  max mem: 31379
Train: data epoch: [15]  [ 500/1000]  eta: 0:04:31  lr: 0.000008  loss: 0.7478  time: 0.4510  data: 0.0000  max mem: 31379
Train: data epoch: [15]  [ 550/1000]  eta: 0:04:00  lr: 0.000008  loss: 0.2811  time: 0.4526  data: 0.0000  max mem: 31379
Train: data epoch: [15]  [ 600/1000]  eta: 0:03:43  lr: 0.000008  loss: 0.9578  time: 0.4686  data: 0.0000  max mem: 31379
Train: data epoch: [15]  [ 650/1000]  eta: 0:03:12  lr: 0.000008  loss: 3.0172  time: 0.4559  data: 0.0000  max mem: 31379
Train: data epoch: [15]  [ 700/1000]  eta: 0:02:43  lr: 0.000008  loss: 2.3300  time: 0.4530  data: 0.0000  max mem: 31379
Train: data epoch: [15]  [ 750/1000]  eta: 0:02:19  lr: 0.000008  loss: 1.3254  time: 0.4517  data: 0.0000  max mem: 31379
Train: data epoch: [15]  [ 800/1000]  eta: 0:01:50  lr: 0.000008  loss: 1.8289  time: 0.4518  data: 0.0000  max mem: 31379
Train: data epoch: [15]  [ 850/1000]  eta: 0:01:21  lr: 0.000008  loss: 1.2608  time: 0.4518  data: 0.0000  max mem: 31379
Train: data epoch: [15]  [ 900/1000]  eta: 0:00:55  lr: 0.000008  loss: 1.3450  time: 0.4531  data: 0.0000  max mem: 31379
Train: data epoch: [15]  [ 950/1000]  eta: 0:00:27  lr: 0.000008  loss: 1.2530  time: 0.4507  data: 0.0000  max mem: 31379
Train: data epoch: [15]  [ 999/1000]  eta: 0:00:00  lr: 0.000008  loss: 1.2476  time: 0.4965  data: 0.0000  max mem: 31379
Train: data epoch: [15] Total time: 0:09:26 (0.5662 s / it)
Train: data epoch: [16]  [   0/1000]  eta: 0:08:11  lr: 0.000008  loss: 1.2155  time: 0.4912  data: 0.0000  max mem: 31379
Train: data epoch: [16]  [  50/1000]  eta: 0:07:12  lr: 0.000008  loss: 1.4525  time: 0.4513  data: 0.0000  max mem: 31379
Train: data epoch: [16]  [ 100/1000]  eta: 0:09:08  lr: 0.000008  loss: 2.7379  time: 0.4615  data: 0.0000  max mem: 31379
Train: data epoch: [16]  [ 150/1000]  eta: 0:09:37  lr: 0.000008  loss: 1.1636  time: 0.4512  data: 0.0000  max mem: 31379
Train: data epoch: [16]  [ 200/1000]  eta: 0:09:26  lr: 0.000008  loss: 0.6018  time: 1.3092  data: 0.0000  max mem: 31379
Train: data epoch: [16]  [ 250/1000]  eta: 0:09:01  lr: 0.000008  loss: 1.4094  time: 0.4642  data: 0.0000  max mem: 31379
Train: data epoch: [16]  [ 300/1000]  eta: 0:09:01  lr: 0.000008  loss: 0.1447  time: 1.2235  data: 0.0000  max mem: 31379
Train: data epoch: [16]  [ 350/1000]  eta: 0:08:21  lr: 0.000008  loss: 1.3413  time: 1.1220  data: 0.0000  max mem: 31379
Train: data epoch: [16]  [ 400/1000]  eta: 0:08:27  lr: 0.000008  loss: 1.7171  time: 1.2702  data: 0.0000  max mem: 31379
Train: data epoch: [16]  [ 450/1000]  eta: 0:08:39  lr: 0.000008  loss: 1.3776  time: 1.9192  data: 0.0000  max mem: 31379
Train: data epoch: [16]  [ 500/1000]  eta: 0:09:18  lr: 0.000008  loss: 0.1764  time: 2.6782  data: 0.0000  max mem: 31379
Train: data epoch: [16]  [ 550/1000]  eta: 0:08:19  lr: 0.000008  loss: 2.6815  time: 0.4523  data: 0.0000  max mem: 31379
Train: data epoch: [16]  [ 600/1000]  eta: 0:07:02  lr: 0.000008  loss: 1.3053  time: 0.4543  data: 0.0000  max mem: 31379
Train: data epoch: [16]  [ 650/1000]  eta: 0:05:53  lr: 0.000008  loss: 2.6607  time: 0.4519  data: 0.0000  max mem: 31379
Train: data epoch: [16]  [ 700/1000]  eta: 0:04:50  lr: 0.000008  loss: 2.4167  time: 0.4517  data: 0.0000  max mem: 31379
Train: data epoch: [16]  [ 750/1000]  eta: 0:03:53  lr: 0.000008  loss: 1.2370  time: 0.4507  data: 0.0000  max mem: 31379
Train: data epoch: [16]  [ 800/1000]  eta: 0:03:00  lr: 0.000008  loss: 1.2869  time: 0.4516  data: 0.0000  max mem: 31379
Train: data epoch: [16]  [ 850/1000]  eta: 0:02:11  lr: 0.000008  loss: 1.9758  time: 0.4512  data: 0.0000  max mem: 31379
Train: data epoch: [16]  [ 900/1000]  eta: 0:01:25  lr: 0.000008  loss: 1.3076  time: 0.4508  data: 0.0000  max mem: 31379
Train: data epoch: [16]  [ 950/1000]  eta: 0:00:41  lr: 0.000008  loss: 1.0520  time: 0.4510  data: 0.0000  max mem: 31522
Train: data epoch: [16]  [ 999/1000]  eta: 0:00:00  lr: 0.000008  loss: 1.2166  time: 0.4999  data: 0.0000  max mem: 31522
Train: data epoch: [16] Total time: 0:13:39 (0.8199 s / it)
Train: data epoch: [17]  [   0/1000]  eta: 0:07:24  lr: 0.000008  loss: 1.0640  time: 0.4446  data: 0.0000  max mem: 31522
Train: data epoch: [17]  [  50/1000]  eta: 0:07:08  lr: 0.000008  loss: 1.3007  time: 0.4510  data: 0.0000  max mem: 31522
Train: data epoch: [17]  [ 100/1000]  eta: 0:06:46  lr: 0.000008  loss: 1.2207  time: 0.4529  data: 0.0000  max mem: 31522
Train: data epoch: [17]  [ 150/1000]  eta: 0:06:24  lr: 0.000008  loss: 0.6773  time: 0.4538  data: 0.0000  max mem: 31522
Train: data epoch: [17]  [ 200/1000]  eta: 0:06:01  lr: 0.000008  loss: 0.4057  time: 0.4529  data: 0.0000  max mem: 31522
Train: data epoch: [17]  [ 250/1000]  eta: 0:05:59  lr: 0.000008  loss: 0.1203  time: 0.7916  data: 0.0000  max mem: 31522
Train: data epoch: [17]  [ 300/1000]  eta: 0:05:47  lr: 0.000008  loss: 1.1489  time: 0.4530  data: 0.0000  max mem: 31522
Train: data epoch: [17]  [ 350/1000]  eta: 0:05:18  lr: 0.000008  loss: 1.1731  time: 0.4525  data: 0.0000  max mem: 31522
Train: data epoch: [17]  [ 400/1000]  eta: 0:04:50  lr: 0.000008  loss: 1.2718  time: 0.4532  data: 0.0000  max mem: 31522
Train: data epoch: [17]  [ 450/1000]  eta: 0:04:24  lr: 0.000008  loss: 0.1908  time: 0.4509  data: 0.0000  max mem: 31522
Train: data epoch: [17]  [ 500/1000]  eta: 0:03:59  lr: 0.000008  loss: 0.7661  time: 0.4515  data: 0.0000  max mem: 31522
Train: data epoch: [17]  [ 550/1000]  eta: 0:03:34  lr: 0.000008  loss: 1.9417  time: 0.4510  data: 0.0000  max mem: 31522
Train: data epoch: [17]  [ 600/1000]  eta: 0:03:09  lr: 0.000008  loss: 0.6502  time: 0.4544  data: 0.0000  max mem: 31522
Train: data epoch: [17]  [ 650/1000]  eta: 0:02:45  lr: 0.000008  loss: 1.3331  time: 0.4540  data: 0.0000  max mem: 31522
Train: data epoch: [17]  [ 700/1000]  eta: 0:02:21  lr: 0.000007  loss: 2.6302  time: 0.4526  data: 0.0000  max mem: 31522
Train: data epoch: [17]  [ 750/1000]  eta: 0:01:57  lr: 0.000007  loss: 0.2212  time: 0.4516  data: 0.0000  max mem: 31522
Train: data epoch: [17]  [ 800/1000]  eta: 0:01:33  lr: 0.000007  loss: 2.5322  time: 0.4517  data: 0.0000  max mem: 31522
Train: data epoch: [17]  [ 850/1000]  eta: 0:01:10  lr: 0.000007  loss: 1.3482  time: 0.4955  data: 0.0000  max mem: 31522
Train: data epoch: [17]  [ 900/1000]  eta: 0:00:47  lr: 0.000007  loss: 1.2596  time: 0.4519  data: 0.0000  max mem: 31522
Train: data epoch: [17]  [ 950/1000]  eta: 0:00:23  lr: 0.000007  loss: 1.2681  time: 0.5177  data: 0.0000  max mem: 31522
Train: data epoch: [17]  [ 999/1000]  eta: 0:00:00  lr: 0.000007  loss: 1.4349  time: 0.4933  data: 0.0000  max mem: 31522
Train: data epoch: [17] Total time: 0:07:54 (0.4741 s / it)
Train: data epoch: [18]  [   0/1000]  eta: 0:09:47  lr: 0.000007  loss: 2.8115  time: 0.5879  data: 0.0000  max mem: 31522
Train: data epoch: [18]  [  50/1000]  eta: 0:08:12  lr: 0.000007  loss: 1.2288  time: 0.4521  data: 0.0000  max mem: 31522
Train: data epoch: [18]  [ 100/1000]  eta: 0:07:18  lr: 0.000007  loss: 1.2534  time: 0.4591  data: 0.0000  max mem: 31522
Train: data epoch: [18]  [ 150/1000]  eta: 0:06:43  lr: 0.000007  loss: 0.9227  time: 0.4522  data: 0.0000  max mem: 31522
Train: data epoch: [18]  [ 200/1000]  eta: 0:06:15  lr: 0.000007  loss: 1.3393  time: 0.4516  data: 0.0000  max mem: 31522
Train: data epoch: [18]  [ 250/1000]  eta: 0:05:49  lr: 0.000007  loss: 2.7800  time: 0.4521  data: 0.0000  max mem: 31522
Train: data epoch: [18]  [ 300/1000]  eta: 0:05:24  lr: 0.000007  loss: 0.1245  time: 0.4516  data: 0.0000  max mem: 31522
Train: data epoch: [18]  [ 350/1000]  eta: 0:05:02  lr: 0.000007  loss: 1.7876  time: 0.4734  data: 0.0000  max mem: 31522
Train: data epoch: [18]  [ 400/1000]  eta: 0:04:37  lr: 0.000007  loss: 1.2820  time: 0.4511  data: 0.0000  max mem: 31522
Train: data epoch: [18]  [ 450/1000]  eta: 0:04:15  lr: 0.000007  loss: 0.2998  time: 0.5042  data: 0.0000  max mem: 31522
Train: data epoch: [18]  [ 500/1000]  eta: 0:03:52  lr: 0.000007  loss: 1.0945  time: 0.4511  data: 0.0000  max mem: 31522
Train: data epoch: [18]  [ 550/1000]  eta: 0:03:30  lr: 0.000007  loss: 1.1809  time: 0.4681  data: 0.0000  max mem: 31522
Train: data epoch: [18]  [ 600/1000]  eta: 0:03:07  lr: 0.000007  loss: 1.5114  time: 0.5149  data: 0.0000  max mem: 31522
Train: data epoch: [18]  [ 650/1000]  eta: 0:02:46  lr: 0.000007  loss: 1.1575  time: 0.6881  data: 0.0000  max mem: 31522
Train: data epoch: [18]  [ 700/1000]  eta: 0:02:22  lr: 0.000007  loss: 2.2599  time: 0.4532  data: 0.0000  max mem: 31522
Train: data epoch: [18]  [ 750/1000]  eta: 0:01:58  lr: 0.000007  loss: 1.5316  time: 0.4542  data: 0.0000  max mem: 31522
Train: data epoch: [18]  [ 800/1000]  eta: 0:01:34  lr: 0.000007  loss: 2.1379  time: 0.4503  data: 0.0000  max mem: 31522
Train: data epoch: [18]  [ 850/1000]  eta: 0:01:10  lr: 0.000007  loss: 1.2457  time: 0.4512  data: 0.0000  max mem: 31522
Train: data epoch: [18]  [ 900/1000]  eta: 0:00:47  lr: 0.000007  loss: 1.2238  time: 0.4519  data: 0.0000  max mem: 31522
Train: data epoch: [18]  [ 950/1000]  eta: 0:00:23  lr: 0.000007  loss: 1.9832  time: 0.4519  data: 0.0000  max mem: 31522
Train: data epoch: [18]  [ 999/1000]  eta: 0:00:00  lr: 0.000007  loss: 1.2219  time: 0.4952  data: 0.0000  max mem: 31522
Train: data epoch: [18] Total time: 0:07:55 (0.4752 s / it)
Train: data epoch: [19]  [   0/1000]  eta: 0:08:02  lr: 0.000007  loss: 2.4493  time: 0.4828  data: 0.0000  max mem: 31522
Train: data epoch: [19]  [  50/1000]  eta: 0:07:30  lr: 0.000007  loss: 0.2155  time: 0.4509  data: 0.0000  max mem: 31522
Train: data epoch: [19]  [ 100/1000]  eta: 0:07:04  lr: 0.000007  loss: 0.3048  time: 0.4518  data: 0.0000  max mem: 31522
Train: data epoch: [19]  [ 150/1000]  eta: 0:07:05  lr: 0.000007  loss: 0.3697  time: 0.6804  data: 0.0000  max mem: 31522
Train: data epoch: [19]  [ 200/1000]  eta: 0:06:30  lr: 0.000007  loss: 0.5281  time: 0.4530  data: 0.0000  max mem: 31522
Train: data epoch: [19]  [ 250/1000]  eta: 0:06:14  lr: 0.000007  loss: 1.3496  time: 0.6759  data: 0.0000  max mem: 31522
Train: data epoch: [19]  [ 300/1000]  eta: 0:05:44  lr: 0.000007  loss: 1.9779  time: 0.4541  data: 0.0000  max mem: 31567
Train: data epoch: [19]  [ 350/1000]  eta: 0:05:16  lr: 0.000007  loss: 1.3243  time: 0.4516  data: 0.0000  max mem: 31567
Train: data epoch: [19]  [ 400/1000]  eta: 0:04:49  lr: 0.000007  loss: 0.2013  time: 0.4541  data: 0.0000  max mem: 31567
Train: data epoch: [19]  [ 450/1000]  eta: 0:04:24  lr: 0.000007  loss: 1.1046  time: 0.4534  data: 0.0000  max mem: 31567
Train: data epoch: [19]  [ 500/1000]  eta: 0:04:05  lr: 0.000007  loss: 2.0874  time: 0.6161  data: 0.0000  max mem: 31567
Train: data epoch: [19]  [ 550/1000]  eta: 0:03:48  lr: 0.000007  loss: 1.4526  time: 1.0214  data: 0.0000  max mem: 31567
Train: data epoch: [19]  [ 600/1000]  eta: 0:03:21  lr: 0.000007  loss: 1.8425  time: 0.4587  data: 0.0000  max mem: 31567
Train: data epoch: [19]  [ 650/1000]  eta: 0:02:54  lr: 0.000007  loss: 2.3483  time: 0.4531  data: 0.0000  max mem: 31567
Train: data epoch: [19]  [ 700/1000]  eta: 0:02:28  lr: 0.000007  loss: 0.9467  time: 0.4521  data: 0.0000  max mem: 31567
Train: data epoch: [19]  [ 750/1000]  eta: 0:02:05  lr: 0.000007  loss: 3.1208  time: 0.4534  data: 0.0000  max mem: 31567
Train: data epoch: [19]  [ 800/1000]  eta: 0:01:42  lr: 0.000007  loss: 0.3444  time: 1.0338  data: 0.0000  max mem: 31659
Train: data epoch: [19]  [ 850/1000]  eta: 0:01:16  lr: 0.000007  loss: 1.1246  time: 0.4517  data: 0.0000  max mem: 31659
Train: data epoch: [19]  [ 900/1000]  eta: 0:00:50  lr: 0.000007  loss: 1.8978  time: 0.4519  data: 0.0000  max mem: 31659
Train: data epoch: [19]  [ 950/1000]  eta: 0:00:26  lr: 0.000007  loss: 1.3172  time: 0.4618  data: 0.0000  max mem: 31659
Train: data epoch: [19]  [ 999/1000]  eta: 0:00:00  lr: 0.000007  loss: 1.0655  time: 0.5255  data: 0.0000  max mem: 31659
Train: data epoch: [19] Total time: 0:08:41 (0.5219 s / it)
Train: data epoch: [20]  [   0/1000]  eta: 0:09:09  lr: 0.000007  loss: 1.2109  time: 0.5491  data: 0.0000  max mem: 31659
Train: data epoch: [20]  [  50/1000]  eta: 0:07:10  lr: 0.000007  loss: 1.2931  time: 0.4508  data: 0.0000  max mem: 31659
Train: data epoch: [20]  [ 100/1000]  eta: 0:10:07  lr: 0.000007  loss: 1.4280  time: 0.4604  data: 0.0000  max mem: 31659
Train: data epoch: [20]  [ 150/1000]  eta: 0:08:30  lr: 0.000007  loss: 0.9448  time: 0.4507  data: 0.0000  max mem: 31659
Train: data epoch: [20]  [ 200/1000]  eta: 0:08:12  lr: 0.000007  loss: 1.2980  time: 0.6912  data: 0.0000  max mem: 31659
Train: data epoch: [20]  [ 250/1000]  eta: 0:07:17  lr: 0.000007  loss: 2.2368  time: 0.4516  data: 0.0000  max mem: 31659
Train: data epoch: [20]  [ 300/1000]  eta: 0:07:14  lr: 0.000007  loss: 0.3049  time: 0.4534  data: 0.0000  max mem: 31659
Train: data epoch: [20]  [ 350/1000]  eta: 0:06:27  lr: 0.000007  loss: 1.1996  time: 0.4507  data: 0.0000  max mem: 31659
Train: data epoch: [20]  [ 400/1000]  eta: 0:06:16  lr: 0.000007  loss: 1.1964  time: 0.4542  data: 0.0000  max mem: 31659
Train: data epoch: [20]  [ 450/1000]  eta: 0:06:03  lr: 0.000007  loss: 0.3413  time: 1.6258  data: 0.0000  max mem: 31659
Train: data epoch: [20]  [ 500/1000]  eta: 0:05:45  lr: 0.000007  loss: 2.0345  time: 1.7334  data: 0.0000  max mem: 31659
Train: data epoch: [20]  [ 550/1000]  eta: 0:05:07  lr: 0.000007  loss: 0.2145  time: 0.8421  data: 0.0000  max mem: 31659
Train: data epoch: [20]  [ 600/1000]  eta: 0:04:30  lr: 0.000007  loss: 1.4212  time: 0.8270  data: 0.0000  max mem: 32040
Train: data epoch: [20]  [ 650/1000]  eta: 0:03:54  lr: 0.000007  loss: 1.3503  time: 0.4512  data: 0.0000  max mem: 32040
Train: data epoch: [20]  [ 700/1000]  eta: 0:03:20  lr: 0.000007  loss: 1.2985  time: 0.4526  data: 0.0000  max mem: 32040
Train: data epoch: [20]  [ 750/1000]  eta: 0:02:46  lr: 0.000007  loss: 1.0815  time: 0.6455  data: 0.0000  max mem: 32040
Train: data epoch: [20]  [ 800/1000]  eta: 0:02:11  lr: 0.000007  loss: 1.3126  time: 0.5369  data: 0.0000  max mem: 32040
Train: data epoch: [20]  [ 850/1000]  eta: 0:01:38  lr: 0.000007  loss: 0.6016  time: 0.6788  data: 0.0000  max mem: 32040
Train: data epoch: [20]  [ 900/1000]  eta: 0:01:05  lr: 0.000007  loss: 0.3077  time: 0.7896  data: 0.0000  max mem: 32040
Train: data epoch: [20]  [ 950/1000]  eta: 0:00:33  lr: 0.000007  loss: 0.4447  time: 0.9117  data: 0.0000  max mem: 32040
Train: data epoch: [20]  [ 999/1000]  eta: 0:00:00  lr: 0.000007  loss: 1.1899  time: 0.4614  data: 0.0000  max mem: 32040
Train: data epoch: [20] Total time: 0:11:15 (0.6752 s / it)
Train: data epoch: [21]  [   0/1000]  eta: 0:07:23  lr: 0.000007  loss: 1.2113  time: 0.4436  data: 0.0000  max mem: 32040
Train: data epoch: [21]  [  50/1000]  eta: 0:07:13  lr: 0.000007  loss: 1.8858  time: 0.4519  data: 0.0000  max mem: 32040
Train: data epoch: [21]  [ 100/1000]  eta: 0:06:48  lr: 0.000007  loss: 2.0337  time: 0.4533  data: 0.0000  max mem: 32040
Train: data epoch: [21]  [ 150/1000]  eta: 0:06:25  lr: 0.000007  loss: 1.2794  time: 0.4523  data: 0.0000  max mem: 32040
Train: data epoch: [21]  [ 200/1000]  eta: 0:06:02  lr: 0.000007  loss: 0.2152  time: 0.4514  data: 0.0000  max mem: 32040
Train: data epoch: [21]  [ 250/1000]  eta: 0:05:39  lr: 0.000007  loss: 0.1854  time: 0.4519  data: 0.0000  max mem: 32040
Train: data epoch: [21]  [ 300/1000]  eta: 0:05:16  lr: 0.000007  loss: 1.3622  time: 0.4519  data: 0.0000  max mem: 32040
Train: data epoch: [21]  [ 350/1000]  eta: 0:04:54  lr: 0.000007  loss: 2.1686  time: 0.4504  data: 0.0000  max mem: 32040
Train: data epoch: [21]  [ 400/1000]  eta: 0:04:31  lr: 0.000007  loss: 1.2879  time: 0.4521  data: 0.0000  max mem: 32040
Train: data epoch: [21]  [ 450/1000]  eta: 0:04:08  lr: 0.000006  loss: 1.6756  time: 0.4527  data: 0.0000  max mem: 32040
Train: data epoch: [21]  [ 500/1000]  eta: 0:03:46  lr: 0.000006  loss: 0.6432  time: 0.4546  data: 0.0000  max mem: 32040
Train: data epoch: [21]  [ 550/1000]  eta: 0:03:23  lr: 0.000006  loss: 1.2050  time: 0.4503  data: 0.0000  max mem: 32040
Train: data epoch: [21]  [ 600/1000]  eta: 0:03:03  lr: 0.000006  loss: 1.6272  time: 0.6536  data: 0.0000  max mem: 32040
Train: data epoch: [21]  [ 650/1000]  eta: 0:02:42  lr: 0.000006  loss: 0.9439  time: 0.4519  data: 0.0000  max mem: 32040
Train: data epoch: [21]  [ 700/1000]  eta: 0:02:19  lr: 0.000006  loss: 1.1569  time: 0.4563  data: 0.0000  max mem: 32040
Train: data epoch: [21]  [ 750/1000]  eta: 0:01:55  lr: 0.000006  loss: 0.0994  time: 0.4509  data: 0.0000  max mem: 32040
Train: data epoch: [21]  [ 800/1000]  eta: 0:01:32  lr: 0.000006  loss: 0.2045  time: 0.4500  data: 0.0000  max mem: 32040
Train: data epoch: [21]  [ 850/1000]  eta: 0:01:09  lr: 0.000006  loss: 1.3459  time: 0.4516  data: 0.0000  max mem: 32040
Train: data epoch: [21]  [ 900/1000]  eta: 0:00:46  lr: 0.000006  loss: 1.2304  time: 0.4516  data: 0.0000  max mem: 32040
Train: data epoch: [21]  [ 950/1000]  eta: 0:00:23  lr: 0.000006  loss: 1.5548  time: 0.4506  data: 0.0000  max mem: 32040
Train: data epoch: [21]  [ 999/1000]  eta: 0:00:00  lr: 0.000006  loss: 0.3713  time: 0.4854  data: 0.0000  max mem: 32040
Train: data epoch: [21] Total time: 0:07:41 (0.4617 s / it)
Train: data epoch: [22]  [   0/1000]  eta: 0:07:45  lr: 0.000006  loss: 1.1971  time: 0.4656  data: 0.0000  max mem: 32040
Train: data epoch: [22]  [  50/1000]  eta: 0:07:10  lr: 0.000006  loss: 0.7705  time: 0.4562  data: 0.0000  max mem: 32040
Train: data epoch: [22]  [ 100/1000]  eta: 0:06:55  lr: 0.000006  loss: 0.2475  time: 0.4526  data: 0.0000  max mem: 32040
Train: data epoch: [22]  [ 150/1000]  eta: 0:06:36  lr: 0.000006  loss: 1.7302  time: 0.4956  data: 0.0000  max mem: 32040
Train: data epoch: [22]  [ 200/1000]  eta: 0:06:13  lr: 0.000006  loss: 0.3157  time: 0.4535  data: 0.0000  max mem: 32040
Train: data epoch: [22]  [ 250/1000]  eta: 0:05:51  lr: 0.000006  loss: 0.9640  time: 0.4772  data: 0.0000  max mem: 32040
Train: data epoch: [22]  [ 300/1000]  eta: 0:05:28  lr: 0.000006  loss: 2.1882  time: 0.4517  data: 0.0000  max mem: 32040
Train: data epoch: [22]  [ 350/1000]  eta: 0:05:10  lr: 0.000006  loss: 0.4716  time: 0.4517  data: 0.0000  max mem: 32040
Train: data epoch: [22]  [ 400/1000]  eta: 0:04:44  lr: 0.000006  loss: 1.2187  time: 0.4558  data: 0.0000  max mem: 32040
Train: data epoch: [22]  [ 450/1000]  eta: 0:04:22  lr: 0.000006  loss: 0.7296  time: 0.4520  data: 0.0000  max mem: 32040
Train: data epoch: [22]  [ 500/1000]  eta: 0:03:59  lr: 0.000006  loss: 1.2242  time: 0.5487  data: 0.0000  max mem: 32040
Train: data epoch: [22]  [ 550/1000]  eta: 0:03:34  lr: 0.000006  loss: 1.2572  time: 0.4512  data: 0.0000  max mem: 32040
Train: data epoch: [22]  [ 600/1000]  eta: 0:03:10  lr: 0.000006  loss: 2.3181  time: 0.4903  data: 0.0000  max mem: 32040
Train: data epoch: [22]  [ 650/1000]  eta: 0:02:46  lr: 0.000006  loss: 0.8339  time: 0.4530  data: 0.0000  max mem: 32040
Train: data epoch: [22]  [ 700/1000]  eta: 0:02:22  lr: 0.000006  loss: 1.1716  time: 0.4539  data: 0.0000  max mem: 32040
Train: data epoch: [22]  [ 750/1000]  eta: 0:01:58  lr: 0.000006  loss: 0.3681  time: 0.4717  data: 0.0000  max mem: 32040
Train: data epoch: [22]  [ 800/1000]  eta: 0:01:34  lr: 0.000006  loss: 0.9669  time: 0.4626  data: 0.0000  max mem: 32040
Train: data epoch: [22]  [ 850/1000]  eta: 0:01:11  lr: 0.000006  loss: 1.3038  time: 0.4943  data: 0.0000  max mem: 32040
Train: data epoch: [22]  [ 900/1000]  eta: 0:00:47  lr: 0.000006  loss: 1.9469  time: 0.4530  data: 0.0000  max mem: 32040
Train: data epoch: [22]  [ 950/1000]  eta: 0:00:24  lr: 0.000006  loss: 2.1175  time: 1.4543  data: 0.0000  max mem: 32040
Train: data epoch: [22]  [ 999/1000]  eta: 0:00:00  lr: 0.000006  loss: 1.2768  time: 0.5066  data: 0.0000  max mem: 32040
Train: data epoch: [22] Total time: 0:08:17 (0.4970 s / it)
Train: data epoch: [23]  [   0/1000]  eta: 0:11:25  lr: 0.000006  loss: 2.4465  time: 0.6858  data: 0.0000  max mem: 32040
Train: data epoch: [23]  [  50/1000]  eta: 0:07:14  lr: 0.000006  loss: 1.4240  time: 0.4511  data: 0.0000  max mem: 32040
Train: data epoch: [23]  [ 100/1000]  eta: 0:06:49  lr: 0.000006  loss: 0.3003  time: 0.4530  data: 0.0000  max mem: 32040
Train: data epoch: [23]  [ 150/1000]  eta: 0:06:25  lr: 0.000006  loss: 1.8413  time: 0.4513  data: 0.0000  max mem: 32040
Train: data epoch: [23]  [ 200/1000]  eta: 0:06:02  lr: 0.000006  loss: 1.9219  time: 0.4526  data: 0.0000  max mem: 32040
Train: data epoch: [23]  [ 250/1000]  eta: 0:06:01  lr: 0.000006  loss: 1.2464  time: 0.8102  data: 0.0000  max mem: 32040
Train: data epoch: [23]  [ 300/1000]  eta: 0:05:57  lr: 0.000006  loss: 1.2279  time: 0.8210  data: 0.0000  max mem: 32040
Train: data epoch: [23]  [ 350/1000]  eta: 0:05:26  lr: 0.000006  loss: 1.0830  time: 0.4497  data: 0.0000  max mem: 32040
Train: data epoch: [23]  [ 400/1000]  eta: 0:04:57  lr: 0.000006  loss: 1.8044  time: 0.4517  data: 0.0000  max mem: 32040
Train: data epoch: [23]  [ 450/1000]  eta: 0:04:30  lr: 0.000006  loss: 0.0755  time: 0.4504  data: 0.0000  max mem: 32040
Train: data epoch: [23]  [ 500/1000]  eta: 0:04:03  lr: 0.000006  loss: 0.3556  time: 0.4520  data: 0.0000  max mem: 32040
Train: data epoch: [23]  [ 550/1000]  eta: 0:03:43  lr: 0.000006  loss: 0.3480  time: 0.4517  data: 0.0000  max mem: 32040
Train: data epoch: [23]  [ 600/1000]  eta: 0:03:27  lr: 0.000006  loss: 2.3548  time: 0.4544  data: 0.0000  max mem: 32040
Train: data epoch: [23]  [ 650/1000]  eta: 0:02:59  lr: 0.000006  loss: 0.4483  time: 0.4534  data: 0.0000  max mem: 32040
Train: data epoch: [23]  [ 700/1000]  eta: 0:02:32  lr: 0.000006  loss: 1.2819  time: 0.4524  data: 0.0000  max mem: 32040
Train: data epoch: [23]  [ 750/1000]  eta: 0:02:06  lr: 0.000006  loss: 1.4236  time: 0.4537  data: 0.0000  max mem: 32040
Train: data epoch: [23]  [ 800/1000]  eta: 0:01:45  lr: 0.000006  loss: 0.2926  time: 0.4624  data: 0.0000  max mem: 32040
Train: data epoch: [23]  [ 850/1000]  eta: 0:01:18  lr: 0.000006  loss: 2.1062  time: 0.4525  data: 0.0000  max mem: 32040
Train: data epoch: [23]  [ 900/1000]  eta: 0:00:51  lr: 0.000006  loss: 1.4972  time: 0.4520  data: 0.0000  max mem: 32040
Train: data epoch: [23]  [ 950/1000]  eta: 0:00:25  lr: 0.000006  loss: 1.7918  time: 0.4508  data: 0.0000  max mem: 32040
Train: data epoch: [23]  [ 999/1000]  eta: 0:00:00  lr: 0.000006  loss: 2.3829  time: 0.4968  data: 0.0000  max mem: 32040
Train: data epoch: [23] Total time: 0:08:49 (0.5291 s / it)
Train: data epoch: [24]  [   0/1000]  eta: 0:09:21  lr: 0.000006  loss: 0.7534  time: 0.5620  data: 0.0000  max mem: 32040
Train: data epoch: [24]  [  50/1000]  eta: 0:07:12  lr: 0.000006  loss: 1.4549  time: 0.4525  data: 0.0000  max mem: 32040
Train: data epoch: [24]  [ 100/1000]  eta: 0:06:47  lr: 0.000006  loss: 2.3613  time: 0.4518  data: 0.0000  max mem: 32040
Train: data epoch: [24]  [ 150/1000]  eta: 0:07:29  lr: 0.000006  loss: 1.6962  time: 0.4518  data: 0.0000  max mem: 32040
Train: data epoch: [24]  [ 200/1000]  eta: 0:06:48  lr: 0.000006  loss: 1.0963  time: 0.4534  data: 0.0000  max mem: 32040
Train: data epoch: [24]  [ 250/1000]  eta: 0:06:39  lr: 0.000006  loss: 1.8130  time: 0.8693  data: 0.0000  max mem: 32040
Train: data epoch: [24]  [ 300/1000]  eta: 0:06:03  lr: 0.000006  loss: 1.4309  time: 0.4522  data: 0.0000  max mem: 32040
Train: data epoch: [24]  [ 350/1000]  eta: 0:05:56  lr: 0.000006  loss: 1.1004  time: 0.4513  data: 0.0000  max mem: 32040
Train: data epoch: [24]  [ 400/1000]  eta: 0:05:21  lr: 0.000006  loss: 0.4904  time: 0.4530  data: 0.0000  max mem: 32040
Train: data epoch: [24]  [ 450/1000]  eta: 0:05:03  lr: 0.000006  loss: 1.1517  time: 0.4525  data: 0.0000  max mem: 32040
Train: data epoch: [24]  [ 500/1000]  eta: 0:04:30  lr: 0.000006  loss: 0.6216  time: 0.4561  data: 0.0000  max mem: 32040
Train: data epoch: [24]  [ 550/1000]  eta: 0:04:13  lr: 0.000006  loss: 2.1680  time: 0.4525  data: 0.0000  max mem: 32040
Train: data epoch: [24]  [ 600/1000]  eta: 0:03:41  lr: 0.000006  loss: 1.1673  time: 0.4605  data: 0.0000  max mem: 32040
Train: data epoch: [24]  [ 650/1000]  eta: 0:03:18  lr: 0.000006  loss: 1.2355  time: 0.4517  data: 0.0000  max mem: 32040
Train: data epoch: [24]  [ 700/1000]  eta: 0:02:52  lr: 0.000006  loss: 2.0751  time: 1.0759  data: 0.0000  max mem: 32040
Train: data epoch: [24]  [ 750/1000]  eta: 0:02:24  lr: 0.000006  loss: 1.4144  time: 0.8228  data: 0.0000  max mem: 32040
Train: data epoch: [24]  [ 800/1000]  eta: 0:01:54  lr: 0.000006  loss: 0.2432  time: 0.4624  data: 0.0000  max mem: 32040
Train: data epoch: [24]  [ 850/1000]  eta: 0:01:27  lr: 0.000006  loss: 0.3575  time: 0.4526  data: 0.0000  max mem: 32040
Train: data epoch: [24]  [ 900/1000]  eta: 0:00:59  lr: 0.000006  loss: 1.3200  time: 0.4717  data: 0.0000  max mem: 32040
Train: data epoch: [24]  [ 950/1000]  eta: 0:00:30  lr: 0.000006  loss: 1.1695  time: 0.4510  data: 0.0000  max mem: 32040
Train: data epoch: [24]  [ 999/1000]  eta: 0:00:00  lr: 0.000006  loss: 2.9659  time: 1.0717  data: 0.0000  max mem: 32040
Train: data epoch: [24] Total time: 0:10:27 (0.6276 s / it)
Train: data epoch: [25]  [   0/1000]  eta: 0:09:08  lr: 0.000005  loss: 0.3200  time: 0.5480  data: 0.0000  max mem: 32040
Train: data epoch: [25]  [  50/1000]  eta: 0:18:35  lr: 0.000005  loss: 0.5305  time: 1.1139  data: 0.0000  max mem: 32040
Train: data epoch: [25]  [ 100/1000]  eta: 0:17:27  lr: 0.000005  loss: 1.3218  time: 1.5821  data: 0.0000  max mem: 32040
Train: data epoch: [25]  [ 150/1000]  eta: 0:19:19  lr: 0.000005  loss: 1.2251  time: 2.0998  data: 0.0000  max mem: 32040
Train: data epoch: [25]  [ 200/1000]  eta: 0:27:44  lr: 0.000005  loss: 1.2686  time: 5.8065  data: 0.0000  max mem: 32040
Train: data epoch: [25]  [ 250/1000]  eta: 0:23:13  lr: 0.000005  loss: 0.1519  time: 0.4515  data: 0.0000  max mem: 32040
Train: data epoch: [25]  [ 300/1000]  eta: 0:18:57  lr: 0.000005  loss: 0.1750  time: 0.4603  data: 0.0000  max mem: 32040
Train: data epoch: [25]  [ 350/1000]  eta: 0:17:51  lr: 0.000005  loss: 1.2360  time: 2.3024  data: 0.0000  max mem: 32040
Train: data epoch: [25]  [ 400/1000]  eta: 0:15:00  lr: 0.000005  loss: 1.5056  time: 0.4661  data: 0.0000  max mem: 32040
Train: data epoch: [25]  [ 450/1000]  eta: 0:15:30  lr: 0.000005  loss: 1.3178  time: 6.1703  data: 0.0000  max mem: 32040
Train: data epoch: [25]  [ 500/1000]  eta: 0:13:57  lr: 0.000005  loss: 1.1963  time: 0.4518  data: 0.0000  max mem: 32040
Train: data epoch: [25]  [ 550/1000]  eta: 0:13:31  lr: 0.000005  loss: 2.5165  time: 7.0146  data: 0.0000  max mem: 32040
Train: data epoch: [25]  [ 600/1000]  eta: 0:12:03  lr: 0.000005  loss: 1.2828  time: 1.5340  data: 0.0000  max mem: 32040
Train: data epoch: [25]  [ 650/1000]  eta: 0:10:21  lr: 0.000005  loss: 1.2377  time: 1.4648  data: 0.0000  max mem: 32040
Train: data epoch: [25]  [ 700/1000]  eta: 0:09:05  lr: 0.000005  loss: 0.9771  time: 1.9629  data: 0.0000  max mem: 32040
Train: data epoch: [25]  [ 750/1000]  eta: 0:07:29  lr: 0.000005  loss: 3.3346  time: 1.4464  data: 0.0002  max mem: 32040
Train: data epoch: [25]  [ 800/1000]  eta: 0:05:55  lr: 0.000005  loss: 0.1919  time: 1.5706  data: 0.0000  max mem: 32040
Train: data epoch: [25]  [ 850/1000]  eta: 0:04:24  lr: 0.000005  loss: 1.3045  time: 1.4138  data: 0.0000  max mem: 32040
Train: data epoch: [25]  [ 900/1000]  eta: 0:02:54  lr: 0.000005  loss: 1.1174  time: 1.3557  data: 0.0000  max mem: 32040
Train: data epoch: [25]  [ 950/1000]  eta: 0:01:32  lr: 0.000005  loss: 0.1461  time: 3.2299  data: 0.0000  max mem: 32040
Train: data epoch: [25]  [ 999/1000]  eta: 0:00:01  lr: 0.000005  loss: 0.2362  time: 0.5085  data: 0.0000  max mem: 32040
Train: data epoch: [25] Total time: 0:30:03 (1.8036 s / it)
Train: data epoch: [26]  [   0/1000]  eta: 2:46:53  lr: 0.000005  loss: 0.4804  time: 10.0138  data: 0.0014  max mem: 32040
Train: data epoch: [26]  [  50/1000]  eta: 0:35:42  lr: 0.000005  loss: 0.5337  time: 1.7298  data: 0.0000  max mem: 32040
| distributed init (rank 0, world 4): env://
| distributed init (rank 1, world 4): env://
| distributed init (rank 2, world 4): env://
| distributed init (rank 3, world 4): env://
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/coco_captions/coco_karpathy_train.json
vis_path:/home/users/nus/idmwyk/scratch/temp/dataset/coco/images/train2014
loading dataset refcoco into memory...
creating index...
index created.
DONE (t=2.79s)
loading dataset refcoco+ into memory...
creating index...
index created.
DONE (t=2.95s)
loading dataset refcocog into memory...
creating index...
index created.
DONE (t=2.78s)
loading dataset refcoco into memory...
creating index...
index created.
DONE (t=2.95s)
loading dataset refcoco+ into memory...
creating index...
index created.
DONE (t=2.53s)
loading dataset refcocog into memory...
creating index...
index created.
DONE (t=2.30s)
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/gqa/train_balanced_questions.json
vis_path:/home/users/nus/idmwyk/scratch/temp/dataset/gqa/images/train2014
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/vqav2/vqa_train.json
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/vqav2/vqa_val.json
vis_path:/home/users/nus/idmwyk/scratch/temp/dataset/coco/images/train2014
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/okvqa/okvqa_train.json
vis_path:/home/users/nus/idmwyk/scratch/temp/dataset/coco/images/train2014
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/aokvqa/aokvqa_v1p0_train.json
vis_path:/home/users/nus/idmwyk/scratch/temp/dataset/coco/images/train2014
len(exist_annotation):11106
trainable params: 27262976 || all params: 8057524224 || trainable%: 0.33835425426081844
Position interpolate from 16x16 to 32x32
Load Minigpt-4-LLM Checkpoint: /home/users/nus/idmwyk/scratch/temp/output/saved-ckpt/minigptv4/stage1/20240907183/checkpoint_49.pth
model arch:
 MiniGPTv4(
  (llama_model): PeftModelForCausalLM(
    (base_model): LoraModel(
      (model): LlamaForCausalLM(
        (model): LlamaModel(
          (embed_tokens): Embedding(128256, 4096)
          (layers): ModuleList(
            (0-31): 32 x LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(
                  in_features=4096, out_features=4096, bias=False
                  (lora_dropout): Dropout(p=0.05, inplace=False)
                  (lora_A): Linear(in_features=4096, out_features=64, bias=False)
                  (lora_B): Linear(in_features=64, out_features=4096, bias=False)
                )
                (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
                (v_proj): Linear(
                  in_features=4096, out_features=1024, bias=False
                  (lora_dropout): Dropout(p=0.05, inplace=False)
                  (lora_A): Linear(in_features=4096, out_features=64, bias=False)
                  (lora_B): Linear(in_features=64, out_features=1024, bias=False)
                )
                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
                (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
                (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
                (act_fn): SiLU()
              )
              (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
              (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
            )
          )
          (norm): LlamaRMSNorm((4096,), eps=1e-05)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (lm_head): CastOutputToFloat(
          (0): Linear(in_features=4096, out_features=128256, bias=False)
        )
      )
    )
  )
  (visual_encoder): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0-38): 39 x Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (ln_vision): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)
  (local_attn): LocalAttention(
    (txt): Linear(in_features=4096, out_features=1408, bias=True)
    (img): Linear(in_features=1408, out_features=1408, bias=True)
  )
  (llama_proj): Linear(in_features=5632, out_features=4096, bias=True)
)
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.28.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.28.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.28.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.28.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.29.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.29.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.29.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.29.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.30.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.30.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.30.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.30.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.31.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.31.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.31.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.31.self_attn.v_proj.lora_B.weight
module.local_attn.txt.weight
module.local_attn.txt.bias
module.local_attn.img.weight
module.local_attn.img.bias
module.llama_proj.weight
module.llama_proj.bias
resume the checkpoint
batch sizes [[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]]
Train: data epoch: [26]  [   0/1000]  eta: 15:22:56  lr: 0.000005  loss: 0.3853  time: 55.3769  data: 0.0000  max mem: 29407
Train: data epoch: [26]  [  50/1000]  eta: 1:08:01  lr: 0.000005  loss: 1.2258  time: 1.9977  data: 0.0000  max mem: 30426
Train: data epoch: [26]  [ 100/1000]  eta: 0:38:58  lr: 0.000005  loss: 1.4752  time: 0.4561  data: 0.0000  max mem: 30509
Train: data epoch: [26]  [ 150/1000]  eta: 0:27:16  lr: 0.000005  loss: 1.7464  time: 0.5027  data: 0.0000  max mem: 30603
Train: data epoch: [26]  [ 200/1000]  eta: 0:21:13  lr: 0.000005  loss: 0.6685  time: 0.6832  data: 0.0000  max mem: 30603
Train: data epoch: [26]  [ 250/1000]  eta: 0:17:04  lr: 0.000005  loss: 1.3256  time: 0.4519  data: 0.0000  max mem: 30647
Train: data epoch: [26]  [ 300/1000]  eta: 0:14:11  lr: 0.000005  loss: 2.7193  time: 0.4509  data: 0.0000  max mem: 30933
Train: data epoch: [26]  [ 350/1000]  eta: 0:12:00  lr: 0.000005  loss: 0.6730  time: 0.4549  data: 0.0000  max mem: 30976
Train: data epoch: [26]  [ 400/1000]  eta: 0:10:17  lr: 0.000005  loss: 1.4160  time: 0.4990  data: 0.0000  max mem: 30976
Train: data epoch: [26]  [ 450/1000]  eta: 0:08:53  lr: 0.000005  loss: 0.1792  time: 0.4524  data: 0.0000  max mem: 30976
Train: data epoch: [26]  [ 500/1000]  eta: 0:07:38  lr: 0.000005  loss: 0.2205  time: 0.4521  data: 0.0000  max mem: 30976
Train: data epoch: [26]  [ 550/1000]  eta: 0:06:34  lr: 0.000005  loss: 0.3628  time: 0.4529  data: 0.0000  max mem: 30976
Train: data epoch: [26]  [ 600/1000]  eta: 0:05:41  lr: 0.000005  loss: 2.9695  time: 0.4524  data: 0.0000  max mem: 30976
Train: data epoch: [26]  [ 650/1000]  eta: 0:04:47  lr: 0.000005  loss: 1.2456  time: 0.4499  data: 0.0000  max mem: 30976
Train: data epoch: [26]  [ 700/1000]  eta: 0:03:58  lr: 0.000005  loss: 0.1321  time: 0.4514  data: 0.0000  max mem: 30976
Train: data epoch: [26]  [ 750/1000]  eta: 0:03:13  lr: 0.000005  loss: 1.0991  time: 0.4523  data: 0.0000  max mem: 30976
Train: data epoch: [26]  [ 800/1000]  eta: 0:02:32  lr: 0.000005  loss: 1.3076  time: 0.7932  data: 0.0000  max mem: 30976
Train: data epoch: [26]  [ 850/1000]  eta: 0:01:51  lr: 0.000005  loss: 1.7407  time: 0.4505  data: 0.0000  max mem: 31379
Train: data epoch: [26]  [ 900/1000]  eta: 0:01:14  lr: 0.000005  loss: 0.9710  time: 1.2625  data: 0.0000  max mem: 31379
Train: data epoch: [26]  [ 950/1000]  eta: 0:00:36  lr: 0.000005  loss: 0.6119  time: 0.4536  data: 0.0000  max mem: 31379
Train: data epoch: [26]  [ 999/1000]  eta: 0:00:00  lr: 0.000005  loss: 1.7040  time: 0.7371  data: 0.0000  max mem: 31379
Train: data epoch: [26] Total time: 0:12:05 (0.7259 s / it)
Train: data epoch: [27]  [   0/1000]  eta: 0:07:49  lr: 0.000005  loss: 2.3690  time: 0.4698  data: 0.0000  max mem: 31379
Train: data epoch: [27]  [  50/1000]  eta: 0:07:08  lr: 0.000005  loss: 0.5670  time: 0.4512  data: 0.0000  max mem: 31379
Train: data epoch: [27]  [ 100/1000]  eta: 0:06:46  lr: 0.000005  loss: 1.3256  time: 0.4508  data: 0.0000  max mem: 31379
Train: data epoch: [27]  [ 150/1000]  eta: 0:06:24  lr: 0.000005  loss: 1.2509  time: 0.4518  data: 0.0000  max mem: 31379
Train: data epoch: [27]  [ 200/1000]  eta: 0:06:01  lr: 0.000005  loss: 0.9501  time: 0.4508  data: 0.0000  max mem: 31379
Train: data epoch: [27]  [ 250/1000]  eta: 0:06:20  lr: 0.000005  loss: 1.2117  time: 0.4586  data: 0.0000  max mem: 31379
Train: data epoch: [27]  [ 300/1000]  eta: 0:05:48  lr: 0.000005  loss: 0.3283  time: 0.4510  data: 0.0000  max mem: 31379
Train: data epoch: [27]  [ 350/1000]  eta: 0:05:45  lr: 0.000005  loss: 1.3948  time: 0.4551  data: 0.0000  max mem: 31379
Train: data epoch: [27]  [ 400/1000]  eta: 0:05:13  lr: 0.000005  loss: 1.3459  time: 0.4530  data: 0.0000  max mem: 31379
Train: data epoch: [27]  [ 450/1000]  eta: 0:04:42  lr: 0.000005  loss: 2.5457  time: 0.4515  data: 0.0000  max mem: 31379
Train: data epoch: [27]  [ 500/1000]  eta: 0:04:14  lr: 0.000005  loss: 0.6890  time: 0.4517  data: 0.0000  max mem: 31379
Train: data epoch: [27]  [ 550/1000]  eta: 0:03:49  lr: 0.000005  loss: 0.2676  time: 0.4534  data: 0.0000  max mem: 31379
Train: data epoch: [27]  [ 600/1000]  eta: 0:03:27  lr: 0.000005  loss: 0.9181  time: 0.4550  data: 0.0000  max mem: 31379
Train: data epoch: [27]  [ 650/1000]  eta: 0:03:01  lr: 0.000005  loss: 3.0324  time: 0.4568  data: 0.0000  max mem: 31379
Train: data epoch: [27]  [ 700/1000]  eta: 0:02:34  lr: 0.000005  loss: 2.2998  time: 0.4894  data: 0.0000  max mem: 31379
Train: data epoch: [27]  [ 750/1000]  eta: 0:02:12  lr: 0.000005  loss: 1.3145  time: 0.4515  data: 0.0000  max mem: 31379
Train: data epoch: [27]  [ 800/1000]  eta: 0:01:44  lr: 0.000005  loss: 1.7617  time: 0.4551  data: 0.0000  max mem: 31379
Train: data epoch: [27]  [ 850/1000]  eta: 0:01:18  lr: 0.000005  loss: 1.2056  time: 0.5583  data: 0.0000  max mem: 31379
Train: data epoch: [27]  [ 900/1000]  eta: 0:00:53  lr: 0.000005  loss: 1.3147  time: 0.9899  data: 0.0000  max mem: 31379
Train: data epoch: [27]  [ 950/1000]  eta: 0:00:26  lr: 0.000005  loss: 1.2258  time: 0.4509  data: 0.0000  max mem: 31379
Train: data epoch: [27]  [ 999/1000]  eta: 0:00:00  lr: 0.000005  loss: 1.2212  time: 0.5149  data: 0.0000  max mem: 31379
Train: data epoch: [27] Total time: 0:08:47 (0.5280 s / it)
Train: data epoch: [28]  [   0/1000]  eta: 0:07:29  lr: 0.000005  loss: 1.2283  time: 0.4497  data: 0.0000  max mem: 31379
Train: data epoch: [28]  [  50/1000]  eta: 0:08:07  lr: 0.000005  loss: 1.4275  time: 0.4515  data: 0.0000  max mem: 31379
Train: data epoch: [28]  [ 100/1000]  eta: 0:12:36  lr: 0.000005  loss: 2.6711  time: 0.4620  data: 0.0000  max mem: 31379
Train: data epoch: [28]  [ 150/1000]  eta: 0:10:04  lr: 0.000005  loss: 1.1468  time: 0.4522  data: 0.0000  max mem: 31379
Train: data epoch: [28]  [ 200/1000]  eta: 0:08:38  lr: 0.000005  loss: 0.5855  time: 0.4581  data: 0.0000  max mem: 31379
Train: data epoch: [28]  [ 250/1000]  eta: 0:07:37  lr: 0.000005  loss: 1.3816  time: 0.4529  data: 0.0000  max mem: 31379
Train: data epoch: [28]  [ 300/1000]  eta: 0:07:45  lr: 0.000005  loss: 0.1408  time: 0.4718  data: 0.0000  max mem: 31379
Train: data epoch: [28]  [ 350/1000]  eta: 0:06:52  lr: 0.000005  loss: 1.3268  time: 0.4508  data: 0.0000  max mem: 31379
Train: data epoch: [28]  [ 400/1000]  eta: 0:06:25  lr: 0.000005  loss: 1.6491  time: 0.4602  data: 0.0000  max mem: 31379
Train: data epoch: [28]  [ 450/1000]  eta: 0:05:52  lr: 0.000005  loss: 1.3340  time: 0.8830  data: 0.0000  max mem: 31379
Train: data epoch: [28]  [ 500/1000]  eta: 0:05:36  lr: 0.000005  loss: 0.1522  time: 1.7140  data: 0.0000  max mem: 31379
Train: data epoch: [28]  [ 550/1000]  eta: 0:04:53  lr: 0.000005  loss: 2.6082  time: 0.4530  data: 0.0000  max mem: 31379
Train: data epoch: [28]  [ 600/1000]  eta: 0:04:34  lr: 0.000004  loss: 1.2904  time: 1.9316  data: 0.0000  max mem: 31379
Train: data epoch: [28]  [ 650/1000]  eta: 0:03:56  lr: 0.000004  loss: 2.6513  time: 0.6980  data: 0.0000  max mem: 31379
Train: data epoch: [28]  [ 700/1000]  eta: 0:03:21  lr: 0.000004  loss: 2.3787  time: 0.4578  data: 0.0000  max mem: 31379
Train: data epoch: [28]  [ 750/1000]  eta: 0:02:58  lr: 0.000004  loss: 1.1947  time: 0.4501  data: 0.0000  max mem: 31379
Train: data epoch: [28]  [ 800/1000]  eta: 0:02:26  lr: 0.000004  loss: 1.2739  time: 0.4562  data: 0.0000  max mem: 31379
Train: data epoch: [28]  [ 850/1000]  eta: 0:01:54  lr: 0.000004  loss: 1.9049  time: 0.4528  data: 0.0000  max mem: 31379
Train: data epoch: [28]  [ 900/1000]  eta: 0:01:23  lr: 0.000004  loss: 1.3343  time: 3.6366  data: 0.0000  max mem: 31379
Train: data epoch: [28]  [ 950/1000]  eta: 0:00:45  lr: 0.000004  loss: 1.0261  time: 2.9823  data: 0.0000  max mem: 31523
Train: data epoch: [28]  [ 999/1000]  eta: 0:00:01  lr: 0.000004  loss: 1.1892  time: 3.0641  data: 0.0000  max mem: 31523
Train: data epoch: [28] Total time: 0:17:01 (1.0212 s / it)
Train: data epoch: [29]  [   0/1000]  eta: 0:17:47  lr: 0.000004  loss: 1.0773  time: 1.0670  data: 0.0000  max mem: 31523
Train: data epoch: [29]  [  50/1000]  eta: 1:38:24  lr: 0.000004  loss: 1.2650  time: 6.8426  data: 0.0000  max mem: 31523
Train: data epoch: [29]  [ 100/1000]  eta: 1:43:32  lr: 0.000004  loss: 1.1740  time: 5.5434  data: 0.0000  max mem: 31523
Train: data epoch: [29]  [ 150/1000]  eta: 1:51:59  lr: 0.000004  loss: 0.6607  time: 10.3367  data: 0.0000  max mem: 31523
Train: data epoch: [29]  [ 200/1000]  eta: 1:41:04  lr: 0.000004  loss: 0.3812  time: 0.4575  data: 0.0000  max mem: 31523
Train: data epoch: [29]  [ 250/1000]  eta: 1:17:01  lr: 0.000004  loss: 0.1165  time: 0.4532  data: 0.0000  max mem: 31523
Train: data epoch: [29]  [ 300/1000]  eta: 1:00:49  lr: 0.000004  loss: 1.1530  time: 0.4525  data: 0.0000  max mem: 31523
Train: data epoch: [29]  [ 350/1000]  eta: 0:49:08  lr: 0.000004  loss: 1.1578  time: 0.4520  data: 0.0000  max mem: 31523
Train: data epoch: [29]  [ 400/1000]  eta: 0:40:16  lr: 0.000004  loss: 1.2598  time: 0.4589  data: 0.0000  max mem: 31523
Train: data epoch: [29]  [ 450/1000]  eta: 0:33:16  lr: 0.000004  loss: 0.1795  time: 0.4505  data: 0.0000  max mem: 31523
Train: data epoch: [29]  [ 500/1000]  eta: 0:27:36  lr: 0.000004  loss: 0.6674  time: 0.4505  data: 0.0000  max mem: 31523
Train: data epoch: [29]  [ 550/1000]  eta: 0:22:53  lr: 0.000004  loss: 1.9029  time: 0.4506  data: 0.0000  max mem: 31523
Train: data epoch: [29]  [ 600/1000]  eta: 0:18:54  lr: 0.000004  loss: 0.5954  time: 0.4539  data: 0.0000  max mem: 31523
Train: data epoch: [29]  [ 650/1000]  eta: 0:15:47  lr: 0.000004  loss: 1.2956  time: 0.6731  data: 0.0000  max mem: 31523
Train: data epoch: [29]  [ 700/1000]  eta: 0:12:44  lr: 0.000004  loss: 2.5367  time: 0.4903  data: 0.0000  max mem: 31523
Train: data epoch: [29]  [ 750/1000]  eta: 0:10:02  lr: 0.000004  loss: 0.1944  time: 0.4517  data: 0.0000  max mem: 31523
Train: data epoch: [29]  [ 800/1000]  eta: 0:07:37  lr: 0.000004  loss: 2.4942  time: 0.4514  data: 0.0000  max mem: 31523
Train: data epoch: [29]  [ 850/1000]  eta: 0:05:26  lr: 0.000004  loss: 1.2920  time: 0.4515  data: 0.0000  max mem: 31523
Train: data epoch: [29]  [ 900/1000]  eta: 0:03:28  lr: 0.000004  loss: 1.2197  time: 0.4524  data: 0.0000  max mem: 31523
Train: data epoch: [29]  [ 950/1000]  eta: 0:01:39  lr: 0.000004  loss: 1.2292  time: 0.4503  data: 0.0000  max mem: 31523
Train: data epoch: [29]  [ 999/1000]  eta: 0:00:01  lr: 0.000004  loss: 1.4272  time: 1.4266  data: 0.0000  max mem: 31523
Train: data epoch: [29] Total time: 0:32:29 (1.9491 s / it)
Train: data epoch: [30]  [   0/1000]  eta: 0:07:47  lr: 0.000004  loss: 2.8009  time: 0.4676  data: 0.0000  max mem: 31523
Train: data epoch: [30]  [  50/1000]  eta: 0:07:48  lr: 0.000004  loss: 1.1857  time: 0.5549  data: 0.0000  max mem: 31523
Train: data epoch: [30]  [ 100/1000]  eta: 0:07:30  lr: 0.000004  loss: 1.2488  time: 0.4522  data: 0.0000  max mem: 31523
Train: data epoch: [30]  [ 150/1000]  eta: 0:06:52  lr: 0.000004  loss: 0.8965  time: 0.4550  data: 0.0000  max mem: 31523
Train: data epoch: [30]  [ 200/1000]  eta: 0:06:25  lr: 0.000004  loss: 1.3074  time: 0.5011  data: 0.0000  max mem: 31523
Train: data epoch: [30]  [ 250/1000]  eta: 0:05:57  lr: 0.000004  loss: 2.6947  time: 0.4525  data: 0.0000  max mem: 31523
Train: data epoch: [30]  [ 300/1000]  eta: 0:05:51  lr: 0.000004  loss: 0.1234  time: 0.7774  data: 0.0000  max mem: 31523
Train: data epoch: [30]  [ 350/1000]  eta: 0:05:40  lr: 0.000004  loss: 1.7595  time: 0.4509  data: 0.0000  max mem: 31523
Train: data epoch: [30]  [ 400/1000]  eta: 0:05:14  lr: 0.000004  loss: 1.2807  time: 0.5913  data: 0.0000  max mem: 31523
Train: data epoch: [30]  [ 450/1000]  eta: 0:04:43  lr: 0.000004  loss: 0.2647  time: 0.4547  data: 0.0000  max mem: 31523
Train: data epoch: [30]  [ 500/1000]  eta: 0:04:20  lr: 0.000004  loss: 1.0504  time: 0.4580  data: 0.0000  max mem: 31523
Train: data epoch: [30]  [ 550/1000]  eta: 0:03:56  lr: 0.000004  loss: 1.1064  time: 0.4542  data: 0.0000  max mem: 31523
Train: data epoch: [30]  [ 600/1000]  eta: 0:03:27  lr: 0.000004  loss: 1.4391  time: 0.4530  data: 0.0000  max mem: 31523
Train: data epoch: [30]  [ 650/1000]  eta: 0:03:02  lr: 0.000004  loss: 1.0990  time: 0.4514  data: 0.0000  max mem: 31523
Train: data epoch: [30]  [ 700/1000]  eta: 0:02:39  lr: 0.000004  loss: 2.2406  time: 1.0201  data: 0.0000  max mem: 31523
Train: data epoch: [30]  [ 750/1000]  eta: 0:02:11  lr: 0.000004  loss: 1.4637  time: 0.4536  data: 0.0000  max mem: 31523
Train: data epoch: [30]  [ 800/1000]  eta: 0:01:44  lr: 0.000004  loss: 2.0790  time: 0.4500  data: 0.0000  max mem: 31523
Train: data epoch: [30]  [ 850/1000]  eta: 0:01:25  lr: 0.000004  loss: 1.2415  time: 0.4520  data: 0.0000  max mem: 31523
Train: data epoch: [30]  [ 900/1000]  eta: 0:00:56  lr: 0.000004  loss: 1.2227  time: 0.4730  data: 0.0000  max mem: 31523
Train: data epoch: [30]  [ 950/1000]  eta: 0:00:27  lr: 0.000004  loss: 1.9386  time: 0.4520  data: 0.0000  max mem: 31523
Train: data epoch: [30]  [ 999/1000]  eta: 0:00:00  lr: 0.000004  loss: 1.1958  time: 0.5717  data: 0.0000  max mem: 31523
Train: data epoch: [30] Total time: 0:09:31 (0.5710 s / it)
Train: data epoch: [31]  [   0/1000]  eta: 0:11:16  lr: 0.000004  loss: 2.4251  time: 0.6764  data: 0.0000  max mem: 31523
Train: data epoch: [31]  [  50/1000]  eta: 0:13:22  lr: 0.000004  loss: 0.1901  time: 1.4431  data: 0.0000  max mem: 31523
Train: data epoch: [31]  [ 100/1000]  eta: 0:09:46  lr: 0.000004  loss: 0.2781  time: 0.4595  data: 0.0000  max mem: 31523
Train: data epoch: [31]  [ 150/1000]  eta: 0:08:20  lr: 0.000004  loss: 0.3197  time: 0.4514  data: 0.0000  max mem: 31523
Train: data epoch: [31]  [ 200/1000]  eta: 0:08:53  lr: 0.000004  loss: 0.5323  time: 0.4511  data: 0.0000  max mem: 31523
Train: data epoch: [31]  [ 250/1000]  eta: 0:08:35  lr: 0.000004  loss: 1.3054  time: 0.4522  data: 0.0000  max mem: 31523
Train: data epoch: [31]  [ 300/1000]  eta: 0:07:47  lr: 0.000004  loss: 1.9652  time: 0.7355  data: 0.0000  max mem: 31567
Train: data epoch: [31]  [ 350/1000]  eta: 0:07:05  lr: 0.000004  loss: 1.2711  time: 0.4521  data: 0.0000  max mem: 31567
Train: data epoch: [31]  [ 400/1000]  eta: 0:07:10  lr: 0.000004  loss: 0.1884  time: 0.4673  data: 0.0000  max mem: 31567
Train: data epoch: [31]  [ 450/1000]  eta: 0:06:55  lr: 0.000004  loss: 1.0496  time: 1.0279  data: 0.0000  max mem: 31567
Train: data epoch: [31]  [ 500/1000]  eta: 0:06:13  lr: 0.000004  loss: 2.0428  time: 0.4533  data: 0.0000  max mem: 31567
Train: data epoch: [31]  [ 550/1000]  eta: 0:06:55  lr: 0.000004  loss: 1.3629  time: 2.7684  data: 0.0000  max mem: 31567
Train: data epoch: [31]  [ 600/1000]  eta: 0:06:12  lr: 0.000004  loss: 1.8479  time: 0.4608  data: 0.0000  max mem: 31567
Train: data epoch: [31]  [ 650/1000]  eta: 0:05:53  lr: 0.000004  loss: 2.3362  time: 2.4459  data: 0.0000  max mem: 31567
Train: data epoch: [31]  [ 700/1000]  eta: 0:05:35  lr: 0.000004  loss: 0.8144  time: 3.4608  data: 0.0000  max mem: 31567
Train: data epoch: [31]  [ 750/1000]  eta: 0:05:13  lr: 0.000004  loss: 3.0835  time: 3.9470  data: 0.0000  max mem: 31567
Train: data epoch: [31]  [ 800/1000]  eta: 0:04:43  lr: 0.000004  loss: 0.3406  time: 0.4894  data: 0.0000  max mem: 31659
Train: data epoch: [31]  [ 850/1000]  eta: 0:03:23  lr: 0.000004  loss: 1.0802  time: 0.4508  data: 0.0000  max mem: 31659
Train: data epoch: [31]  [ 900/1000]  eta: 0:02:10  lr: 0.000004  loss: 1.8391  time: 0.4515  data: 0.0000  max mem: 31659
Train: data epoch: [31]  [ 950/1000]  eta: 0:01:03  lr: 0.000004  loss: 1.2817  time: 0.4538  data: 0.0000  max mem: 31659
Train: data epoch: [31]  [ 999/1000]  eta: 0:00:01  lr: 0.000004  loss: 1.0386  time: 0.5121  data: 0.0000  max mem: 31659
Train: data epoch: [31] Total time: 0:20:31 (1.2311 s / it)
Train: data epoch: [32]  [   0/1000]  eta: 0:07:43  lr: 0.000004  loss: 1.1968  time: 0.4634  data: 0.0000  max mem: 31659
Train: data epoch: [32]  [  50/1000]  eta: 0:07:09  lr: 0.000004  loss: 1.2536  time: 0.4507  data: 0.0000  max mem: 31659
Train: data epoch: [32]  [ 100/1000]  eta: 0:06:46  lr: 0.000004  loss: 1.3966  time: 0.4527  data: 0.0000  max mem: 31659
Train: data epoch: [32]  [ 150/1000]  eta: 0:06:24  lr: 0.000004  loss: 0.9231  time: 0.4521  data: 0.0000  max mem: 31659
Train: data epoch: [32]  [ 200/1000]  eta: 0:06:01  lr: 0.000004  loss: 1.2983  time: 0.4513  data: 0.0000  max mem: 31659
Train: data epoch: [32]  [ 250/1000]  eta: 0:05:39  lr: 0.000004  loss: 2.2206  time: 0.4522  data: 0.0000  max mem: 31659
Train: data epoch: [32]  [ 300/1000]  eta: 0:05:16  lr: 0.000004  loss: 0.3099  time: 0.4522  data: 0.0000  max mem: 31659
Train: data epoch: [32]  [ 350/1000]  eta: 0:04:53  lr: 0.000003  loss: 1.1708  time: 0.4516  data: 0.0000  max mem: 31659
Train: data epoch: [32]  [ 400/1000]  eta: 0:05:07  lr: 0.000003  loss: 1.1794  time: 0.4540  data: 0.0000  max mem: 31659
Train: data epoch: [32]  [ 450/1000]  eta: 0:04:39  lr: 0.000003  loss: 0.3278  time: 0.4545  data: 0.0000  max mem: 31659
Train: data epoch: [32]  [ 500/1000]  eta: 0:04:11  lr: 0.000003  loss: 2.0259  time: 0.4632  data: 0.0000  max mem: 31659
Train: data epoch: [32]  [ 550/1000]  eta: 0:03:44  lr: 0.000003  loss: 0.2186  time: 0.4528  data: 0.0000  max mem: 31659
Train: data epoch: [32]  [ 600/1000]  eta: 0:03:18  lr: 0.000003  loss: 1.3264  time: 0.4836  data: 0.0000  max mem: 32035
Train: data epoch: [32]  [ 650/1000]  eta: 0:02:52  lr: 0.000003  loss: 1.3301  time: 0.4527  data: 0.0000  max mem: 32035
Train: data epoch: [32]  [ 700/1000]  eta: 0:02:26  lr: 0.000003  loss: 1.2820  time: 0.4521  data: 0.0000  max mem: 32035
Train: data epoch: [32]  [ 750/1000]  eta: 0:02:01  lr: 0.000003  loss: 1.0654  time: 0.4516  data: 0.0000  max mem: 32035
Train: data epoch: [32]  [ 800/1000]  eta: 0:01:37  lr: 0.000003  loss: 1.2885  time: 0.4509  data: 0.0000  max mem: 32035
Train: data epoch: [32]  [ 850/1000]  eta: 0:01:12  lr: 0.000003  loss: 0.4884  time: 0.4793  data: 0.0000  max mem: 32035
Train: data epoch: [32]  [ 900/1000]  eta: 0:00:48  lr: 0.000003  loss: 0.3023  time: 0.5280  data: 0.0000  max mem: 32035
| distributed init (rank 0, world 4): env://
| distributed init (rank 1, world 4): env://
| distributed init (rank 2, world 4): env://
| distributed init (rank 3, world 4): env://
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/coco_captions/coco_karpathy_train.json
vis_path:/home/users/nus/idmwyk/scratch/temp/dataset/coco/images/train2014
loading dataset refcoco into memory...
creating index...
index created.
DONE (t=5.76s)
loading dataset refcoco+ into memory...
creating index...
index created.
DONE (t=6.15s)
loading dataset refcocog into memory...
creating index...
index created.
DONE (t=4.87s)
loading dataset refcoco into memory...
creating index...
index created.
DONE (t=6.51s)
loading dataset refcoco+ into memory...
creating index...
index created.
DONE (t=2.67s)
loading dataset refcocog into memory...
creating index...
index created.
DONE (t=6.84s)
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/gqa/train_balanced_questions.json
vis_path:/home/users/nus/idmwyk/scratch/temp/dataset/gqa/images/train2014
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/vqav2/vqa_train.json
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/vqav2/vqa_val.json
vis_path:/home/users/nus/idmwyk/scratch/temp/dataset/coco/images/train2014
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/okvqa/okvqa_train.json
vis_path:/home/users/nus/idmwyk/scratch/temp/dataset/coco/images/train2014
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/aokvqa/aokvqa_v1p0_train.json
vis_path:/home/users/nus/idmwyk/scratch/temp/dataset/coco/images/train2014
len(exist_annotation):11106
trainable params: 27262976 || all params: 8057524224 || trainable%: 0.33835425426081844
Position interpolate from 16x16 to 32x32
Load Minigpt-4-LLM Checkpoint: /home/users/nus/idmwyk/scratch/temp/output/saved-ckpt/minigptv4/stage1/20240907183/checkpoint_49.pth
model arch:
 MiniGPTv4(
  (llama_model): PeftModelForCausalLM(
    (base_model): LoraModel(
      (model): LlamaForCausalLM(
        (model): LlamaModel(
          (embed_tokens): Embedding(128256, 4096)
          (layers): ModuleList(
            (0-31): 32 x LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(
                  in_features=4096, out_features=4096, bias=False
                  (lora_dropout): Dropout(p=0.05, inplace=False)
                  (lora_A): Linear(in_features=4096, out_features=64, bias=False)
                  (lora_B): Linear(in_features=64, out_features=4096, bias=False)
                )
                (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
                (v_proj): Linear(
                  in_features=4096, out_features=1024, bias=False
                  (lora_dropout): Dropout(p=0.05, inplace=False)
                  (lora_A): Linear(in_features=4096, out_features=64, bias=False)
                  (lora_B): Linear(in_features=64, out_features=1024, bias=False)
                )
                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
                (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
                (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
                (act_fn): SiLU()
              )
              (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
              (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
            )
          )
          (norm): LlamaRMSNorm((4096,), eps=1e-05)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (lm_head): CastOutputToFloat(
          (0): Linear(in_features=4096, out_features=128256, bias=False)
        )
      )
    )
  )
  (visual_encoder): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0-38): 39 x Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (ln_vision): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)
  (local_attn): LocalAttention(
    (txt): Linear(in_features=4096, out_features=1408, bias=True)
    (img): Linear(in_features=1408, out_features=1408, bias=True)
  )
  (llama_proj): Linear(in_features=5632, out_features=4096, bias=True)
)
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.28.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.28.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.28.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.28.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.29.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.29.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.29.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.29.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.30.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.30.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.30.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.30.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.31.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.31.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.31.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.31.self_attn.v_proj.lora_B.weight
module.local_attn.txt.weight
module.local_attn.txt.bias
module.local_attn.img.weight
module.local_attn.img.bias
module.llama_proj.weight
module.llama_proj.bias
resume the checkpoint
batch sizes [[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]]
Train: data epoch: [32]  [   0/1000]  eta: 21:11:06  lr: 0.000004  loss: 0.3614  time: 76.2661  data: 0.0000  max mem: 29408
Train: data epoch: [32]  [  50/1000]  eta: 1:08:54  lr: 0.000004  loss: 1.2073  time: 2.0230  data: 0.0000  max mem: 30425
Train: data epoch: [32]  [ 100/1000]  eta: 0:40:25  lr: 0.000004  loss: 1.4985  time: 0.4590  data: 0.0000  max mem: 30509
Train: data epoch: [32]  [ 150/1000]  eta: 0:28:10  lr: 0.000004  loss: 1.5459  time: 0.4960  data: 0.0000  max mem: 30607
Train: data epoch: [32]  [ 200/1000]  eta: 0:21:26  lr: 0.000004  loss: 0.6694  time: 0.4609  data: 0.0000  max mem: 30607
Train: data epoch: [32]  [ 250/1000]  eta: 0:17:17  lr: 0.000004  loss: 1.3146  time: 0.4843  data: 0.0000  max mem: 30647
Train: data epoch: [32]  [ 300/1000]  eta: 0:14:24  lr: 0.000004  loss: 2.6851  time: 0.4866  data: 0.0000  max mem: 30935
Train: data epoch: [32]  [ 350/1000]  eta: 0:12:10  lr: 0.000003  loss: 0.6279  time: 0.4575  data: 0.0000  max mem: 30976
Train: data epoch: [32]  [ 400/1000]  eta: 0:10:24  lr: 0.000003  loss: 1.3945  time: 0.4552  data: 0.0000  max mem: 30976
Train: data epoch: [32]  [ 450/1000]  eta: 0:08:56  lr: 0.000003  loss: 0.1001  time: 0.4535  data: 0.0000  max mem: 30976
Train: data epoch: [32]  [ 500/1000]  eta: 0:07:45  lr: 0.000003  loss: 0.2256  time: 0.5763  data: 0.0000  max mem: 30976
Train: data epoch: [32]  [ 550/1000]  eta: 0:06:39  lr: 0.000003  loss: 0.3300  time: 0.4548  data: 0.0000  max mem: 30976
Train: data epoch: [32]  [ 600/1000]  eta: 0:05:43  lr: 0.000003  loss: 2.9416  time: 0.4771  data: 0.0000  max mem: 30976
Train: data epoch: [32]  [ 650/1000]  eta: 0:04:50  lr: 0.000003  loss: 1.2358  time: 0.4529  data: 0.0000  max mem: 30976
Train: data epoch: [32]  [ 700/1000]  eta: 0:04:02  lr: 0.000003  loss: 0.0884  time: 0.4530  data: 0.0000  max mem: 30976
Train: data epoch: [32]  [ 750/1000]  eta: 0:03:16  lr: 0.000003  loss: 1.1166  time: 0.4596  data: 0.0000  max mem: 30976
Train: data epoch: [32]  [ 800/1000]  eta: 0:02:33  lr: 0.000003  loss: 1.2599  time: 0.4546  data: 0.0000  max mem: 30976
Train: data epoch: [32]  [ 850/1000]  eta: 0:01:55  lr: 0.000003  loss: 1.7167  time: 0.4527  data: 0.0000  max mem: 31379
Train: data epoch: [32]  [ 900/1000]  eta: 0:01:15  lr: 0.000003  loss: 0.8902  time: 0.4577  data: 0.0000  max mem: 31379
Train: data epoch: [32]  [ 950/1000]  eta: 0:00:37  lr: 0.000003  loss: 0.5911  time: 0.7303  data: 0.0000  max mem: 31379
Train: data epoch: [32]  [ 999/1000]  eta: 0:00:00  lr: 0.000003  loss: 1.7102  time: 0.7507  data: 0.0000  max mem: 31379
Train: data epoch: [32] Total time: 0:12:13 (0.7339 s / it)
Train: data epoch: [33]  [   0/1000]  eta: 0:07:55  lr: 0.000003  loss: 2.3505  time: 0.4759  data: 0.0000  max mem: 31379
Train: data epoch: [33]  [  50/1000]  eta: 0:08:18  lr: 0.000003  loss: 0.5741  time: 0.6346  data: 0.0000  max mem: 31379
Train: data epoch: [33]  [ 100/1000]  eta: 0:07:21  lr: 0.000003  loss: 1.3110  time: 0.4561  data: 0.0000  max mem: 31379
Train: data epoch: [33]  [ 150/1000]  eta: 0:06:46  lr: 0.000003  loss: 1.2380  time: 0.4540  data: 0.0000  max mem: 31379
Train: data epoch: [33]  [ 200/1000]  eta: 0:06:28  lr: 0.000003  loss: 0.8281  time: 0.5905  data: 0.0000  max mem: 31379
Train: data epoch: [33]  [ 250/1000]  eta: 0:06:43  lr: 0.000003  loss: 1.1899  time: 0.4616  data: 0.0000  max mem: 31379
Train: data epoch: [33]  [ 300/1000]  eta: 0:06:07  lr: 0.000003  loss: 0.2737  time: 0.4545  data: 0.0000  max mem: 31379
Train: data epoch: [33]  [ 350/1000]  eta: 0:06:11  lr: 0.000003  loss: 1.3506  time: 1.4474  data: 0.0000  max mem: 31379
Train: data epoch: [33]  [ 400/1000]  eta: 0:05:33  lr: 0.000003  loss: 1.3141  time: 0.4547  data: 0.0000  max mem: 31379
Train: data epoch: [33]  [ 450/1000]  eta: 0:04:59  lr: 0.000003  loss: 2.4897  time: 0.4544  data: 0.0000  max mem: 31379
Train: data epoch: [33]  [ 500/1000]  eta: 0:04:31  lr: 0.000003  loss: 0.7038  time: 0.4554  data: 0.0000  max mem: 31379
Train: data epoch: [33]  [ 550/1000]  eta: 0:04:13  lr: 0.000003  loss: 0.2547  time: 0.4661  data: 0.0000  max mem: 31379
Train: data epoch: [33]  [ 600/1000]  eta: 0:03:41  lr: 0.000003  loss: 0.9167  time: 0.4553  data: 0.0000  max mem: 31379
Train: data epoch: [33]  [ 650/1000]  eta: 0:03:21  lr: 0.000003  loss: 2.9752  time: 0.4567  data: 0.0000  max mem: 31379
Train: data epoch: [33]  [ 700/1000]  eta: 0:02:50  lr: 0.000003  loss: 2.2805  time: 0.4559  data: 0.0000  max mem: 31379
Train: data epoch: [33]  [ 750/1000]  eta: 0:02:25  lr: 0.000003  loss: 1.3043  time: 1.3328  data: 0.0000  max mem: 31379
Train: data epoch: [33]  [ 800/1000]  eta: 0:01:54  lr: 0.000003  loss: 1.7032  time: 0.4566  data: 0.0000  max mem: 31379
Train: data epoch: [33]  [ 850/1000]  eta: 0:01:26  lr: 0.000003  loss: 1.1921  time: 0.8308  data: 0.0000  max mem: 31379
Train: data epoch: [33]  [ 900/1000]  eta: 0:00:58  lr: 0.000003  loss: 1.2810  time: 1.2272  data: 0.0000  max mem: 31379
Train: data epoch: [33]  [ 950/1000]  eta: 0:00:29  lr: 0.000003  loss: 1.2149  time: 0.4525  data: 0.0000  max mem: 31379
Train: data epoch: [33]  [ 999/1000]  eta: 0:00:00  lr: 0.000003  loss: 1.2093  time: 0.5766  data: 0.0000  max mem: 31379
Train: data epoch: [33] Total time: 0:09:39 (0.5793 s / it)
Train: data epoch: [34]  [   0/1000]  eta: 0:07:54  lr: 0.000003  loss: 1.2350  time: 0.4748  data: 0.0000  max mem: 31379
Train: data epoch: [34]  [  50/1000]  eta: 0:17:16  lr: 0.000003  loss: 1.4219  time: 0.4532  data: 0.0000  max mem: 31379
Train: data epoch: [34]  [ 100/1000]  eta: 0:11:39  lr: 0.000003  loss: 2.5994  time: 0.4557  data: 0.0000  max mem: 31379
Train: data epoch: [34]  [ 150/1000]  eta: 0:12:34  lr: 0.000003  loss: 1.1272  time: 2.0943  data: 0.0000  max mem: 31379
Train: data epoch: [34]  [ 200/1000]  eta: 0:10:25  lr: 0.000003  loss: 0.5595  time: 0.4729  data: 0.0000  max mem: 31379
Train: data epoch: [34]  [ 250/1000]  eta: 0:10:11  lr: 0.000003  loss: 1.3707  time: 1.6643  data: 0.0000  max mem: 31379
Train: data epoch: [34]  [ 300/1000]  eta: 0:08:48  lr: 0.000003  loss: 0.1369  time: 0.4569  data: 0.0000  max mem: 31379
Train: data epoch: [34]  [ 350/1000]  eta: 0:08:40  lr: 0.000003  loss: 1.3237  time: 1.3234  data: 0.0000  max mem: 31379
Train: data epoch: [34]  [ 400/1000]  eta: 0:08:00  lr: 0.000003  loss: 1.5716  time: 1.3275  data: 0.0000  max mem: 31379
Train: data epoch: [34]  [ 450/1000]  eta: 0:07:12  lr: 0.000003  loss: 1.2977  time: 0.4730  data: 0.0000  max mem: 31379
Train: data epoch: [34]  [ 500/1000]  eta: 0:06:48  lr: 0.000003  loss: 0.1451  time: 0.4540  data: 0.0000  max mem: 31379
Train: data epoch: [34]  [ 550/1000]  eta: 0:06:39  lr: 0.000003  loss: 2.5566  time: 1.9271  data: 0.0000  max mem: 31379
Train: data epoch: [34]  [ 600/1000]  eta: 0:05:59  lr: 0.000003  loss: 1.2883  time: 0.4623  data: 0.0000  max mem: 31379
Train: data epoch: [34]  [ 650/1000]  eta: 0:05:51  lr: 0.000003  loss: 2.6331  time: 1.9099  data: 0.0000  max mem: 31379
Train: data epoch: [34]  [ 700/1000]  eta: 0:05:48  lr: 0.000003  loss: 2.3714  time: 4.3709  data: 0.0000  max mem: 31379
Train: data epoch: [34]  [ 750/1000]  eta: 0:07:14  lr: 0.000003  loss: 1.1919  time: 13.2212  data: 0.0000  max mem: 31379
Train: data epoch: [34]  [ 800/1000]  eta: 0:10:13  lr: 0.000003  loss: 1.2608  time: 16.3979  data: 0.0000  max mem: 31379
Train: data epoch: [34]  [ 850/1000]  eta: 0:08:59  lr: 0.000003  loss: 1.8617  time: 9.5349  data: 0.0000  max mem: 31379
Train: data epoch: [34]  [ 900/1000]  eta: 0:06:27  lr: 0.000003  loss: 1.3535  time: 7.3466  data: 0.0000  max mem: 31379
Train: data epoch: [34]  [ 950/1000]  eta: 0:03:30  lr: 0.000003  loss: 1.0065  time: 9.1095  data: 0.0000  max mem: 31523
Train: data epoch: [34]  [ 999/1000]  eta: 0:00:04  lr: 0.000003  loss: 1.1845  time: 0.5606  data: 0.0000  max mem: 31523
Train: data epoch: [34] Total time: 1:07:20 (4.0408 s / it)
Train: data epoch: [35]  [   0/1000]  eta: 8:57:15  lr: 0.000003  loss: 1.0312  time: 32.2354  data: 0.0000  max mem: 31523
Train: data epoch: [35]  [  50/1000]  eta: 1:21:50  lr: 0.000003  loss: 1.2318  time: 0.4507  data: 0.0000  max mem: 31523
Train: data epoch: [35]  [ 100/1000]  eta: 0:54:19  lr: 0.000003  loss: 1.1517  time: 0.4551  data: 0.0000  max mem: 31523
Train: data epoch: [35]  [ 150/1000]  eta: 0:36:26  lr: 0.000003  loss: 0.6307  time: 0.4552  data: 0.0000  max mem: 31523
Train: data epoch: [35]  [ 200/1000]  eta: 0:27:16  lr: 0.000003  loss: 0.3285  time: 0.4543  data: 0.0000  max mem: 31523
Train: data epoch: [35]  [ 250/1000]  eta: 0:21:36  lr: 0.000003  loss: 0.1101  time: 0.4539  data: 0.0000  max mem: 31523
Train: data epoch: [35]  [ 300/1000]  eta: 0:17:42  lr: 0.000003  loss: 1.1491  time: 0.4544  data: 0.0000  max mem: 31523
Train: data epoch: [35]  [ 350/1000]  eta: 0:14:47  lr: 0.000003  loss: 1.1350  time: 0.4551  data: 0.0000  max mem: 31523
Train: data epoch: [35]  [ 400/1000]  eta: 0:12:31  lr: 0.000003  loss: 1.2342  time: 0.4547  data: 0.0000  max mem: 31523
Train: data epoch: [35]  [ 450/1000]  eta: 0:12:30  lr: 0.000003  loss: 0.1791  time: 0.4511  data: 0.0000  max mem: 31523
Train: data epoch: [35]  [ 500/1000]  eta: 0:10:36  lr: 0.000003  loss: 0.6327  time: 0.4532  data: 0.0000  max mem: 31523
Train: data epoch: [35]  [ 550/1000]  eta: 0:08:59  lr: 0.000003  loss: 1.8824  time: 0.4523  data: 0.0000  max mem: 31523
Train: data epoch: [35]  [ 600/1000]  eta: 0:07:35  lr: 0.000003  loss: 0.5760  time: 0.4557  data: 0.0000  max mem: 31523
Train: data epoch: [35]  [ 650/1000]  eta: 0:07:18  lr: 0.000003  loss: 1.2759  time: 0.4727  data: 0.0000  max mem: 31523
Train: data epoch: [35]  [ 700/1000]  eta: 0:06:12  lr: 0.000003  loss: 2.4965  time: 2.0895  data: 0.0000  max mem: 31523
Train: data epoch: [35]  [ 750/1000]  eta: 0:04:57  lr: 0.000003  loss: 0.1754  time: 0.4519  data: 0.0000  max mem: 31523
Train: data epoch: [35]  [ 800/1000]  eta: 0:03:48  lr: 0.000003  loss: 2.4603  time: 0.4522  data: 0.0000  max mem: 31523
Train: data epoch: [35]  [ 850/1000]  eta: 0:03:13  lr: 0.000003  loss: 1.2672  time: 0.4514  data: 0.0000  max mem: 31523
Train: data epoch: [35]  [ 900/1000]  eta: 0:02:15  lr: 0.000003  loss: 1.2114  time: 0.4600  data: 0.0000  max mem: 31523
Train: data epoch: [35]  [ 950/1000]  eta: 0:01:19  lr: 0.000003  loss: 1.2116  time: 0.4507  data: 0.0000  max mem: 31523
Train: data epoch: [35]  [ 999/1000]  eta: 0:00:01  lr: 0.000003  loss: 1.4214  time: 0.5305  data: 0.0000  max mem: 31523
Train: data epoch: [35] Total time: 0:25:50 (1.5506 s / it)
Train: data epoch: [36]  [   0/1000]  eta: 0:11:38  lr: 0.000003  loss: 2.7954  time: 0.6981  data: 0.0000  max mem: 31523
Train: data epoch: [36]  [  50/1000]  eta: 0:07:14  lr: 0.000003  loss: 1.1628  time: 0.4538  data: 0.0000  max mem: 31523
Train: data epoch: [36]  [ 100/1000]  eta: 0:06:50  lr: 0.000003  loss: 1.2248  time: 0.4532  data: 0.0000  max mem: 31523
Train: data epoch: [36]  [ 150/1000]  eta: 0:06:26  lr: 0.000003  loss: 0.8738  time: 0.4542  data: 0.0000  max mem: 31523
Train: data epoch: [36]  [ 200/1000]  eta: 0:06:03  lr: 0.000003  loss: 1.2802  time: 0.4546  data: 0.0000  max mem: 31523
Train: data epoch: [36]  [ 250/1000]  eta: 0:05:41  lr: 0.000003  loss: 2.6699  time: 0.4542  data: 0.0000  max mem: 31523
Train: data epoch: [36]  [ 300/1000]  eta: 0:05:18  lr: 0.000003  loss: 0.1230  time: 0.4536  data: 0.0000  max mem: 31523
Train: data epoch: [36]  [ 350/1000]  eta: 0:04:55  lr: 0.000003  loss: 1.7534  time: 0.4534  data: 0.0000  max mem: 31523
Train: data epoch: [36]  [ 400/1000]  eta: 0:05:14  lr: 0.000003  loss: 1.2797  time: 1.8507  data: 0.0000  max mem: 31523
Train: data epoch: [36]  [ 450/1000]  eta: 0:04:45  lr: 0.000003  loss: 0.2354  time: 0.4562  data: 0.0000  max mem: 31523
Train: data epoch: [36]  [ 500/1000]  eta: 0:04:16  lr: 0.000003  loss: 1.0329  time: 0.4615  data: 0.0000  max mem: 31523
Train: data epoch: [36]  [ 550/1000]  eta: 0:03:48  lr: 0.000003  loss: 1.0588  time: 0.4532  data: 0.0000  max mem: 31523
Train: data epoch: [36]  [ 600/1000]  eta: 0:03:21  lr: 0.000003  loss: 1.3599  time: 0.4529  data: 0.0000  max mem: 31523
Train: data epoch: [36]  [ 650/1000]  eta: 0:06:05  lr: 0.000002  loss: 1.0873  time: 11.9640  data: 0.0000  max mem: 31523
Train: data epoch: [36]  [ 700/1000]  eta: 0:05:01  lr: 0.000002  loss: 2.2385  time: 0.4697  data: 0.0000  max mem: 31523
Train: data epoch: [36]  [ 750/1000]  eta: 0:04:28  lr: 0.000002  loss: 1.4097  time: 4.3939  data: 0.0000  max mem: 31523
Train: data epoch: [36]  [ 800/1000]  eta: 0:03:26  lr: 0.000002  loss: 2.0298  time: 0.4660  data: 0.0000  max mem: 31523
Train: data epoch: [36]  [ 850/1000]  eta: 0:02:29  lr: 0.000002  loss: 1.2404  time: 0.4525  data: 0.0000  max mem: 31523
Train: data epoch: [36]  [ 900/1000]  eta: 0:01:36  lr: 0.000002  loss: 1.2199  time: 0.4675  data: 0.0000  max mem: 31523
Train: data epoch: [36]  [ 950/1000]  eta: 0:00:47  lr: 0.000002  loss: 1.9248  time: 0.4701  data: 0.0000  max mem: 31523
Train: data epoch: [36]  [ 999/1000]  eta: 0:00:00  lr: 0.000002  loss: 1.1773  time: 0.5920  data: 0.0000  max mem: 31523
Train: data epoch: [36] Total time: 0:15:23 (0.9238 s / it)
Train: data epoch: [37]  [   0/1000]  eta: 0:07:48  lr: 0.000002  loss: 2.4086  time: 0.4682  data: 0.0000  max mem: 31523
Train: data epoch: [37]  [  50/1000]  eta: 0:07:11  lr: 0.000002  loss: 0.1661  time: 0.4531  data: 0.0000  max mem: 31523
Train: data epoch: [37]  [ 100/1000]  eta: 0:12:23  lr: 0.000002  loss: 0.2570  time: 0.4538  data: 0.0000  max mem: 31523
Train: data epoch: [37]  [ 150/1000]  eta: 0:09:59  lr: 0.000002  loss: 0.2983  time: 0.4530  data: 0.0000  max mem: 31523
Train: data epoch: [37]  [ 200/1000]  eta: 0:08:36  lr: 0.000002  loss: 0.5002  time: 0.4761  data: 0.0000  max mem: 31523
Train: data epoch: [37]  [ 250/1000]  eta: 0:07:35  lr: 0.000002  loss: 1.2900  time: 0.4544  data: 0.0000  max mem: 31523
Train: data epoch: [37]  [ 300/1000]  eta: 0:06:48  lr: 0.000002  loss: 1.9557  time: 0.4544  data: 0.0000  max mem: 31568
Train: data epoch: [37]  [ 350/1000]  eta: 0:06:31  lr: 0.000002  loss: 1.2352  time: 0.8095  data: 0.0000  max mem: 31568
Train: data epoch: [37]  [ 400/1000]  eta: 0:06:10  lr: 0.000002  loss: 0.1747  time: 0.4640  data: 0.0000  max mem: 31568
Train: data epoch: [37]  [ 450/1000]  eta: 0:05:30  lr: 0.000002  loss: 1.0200  time: 0.4801  data: 0.0000  max mem: 31568
Train: data epoch: [37]  [ 500/1000]  eta: 0:04:58  lr: 0.000002  loss: 2.0471  time: 0.7144  data: 0.0000  max mem: 31568
Train: data epoch: [37]  [ 550/1000]  eta: 0:04:33  lr: 0.000002  loss: 1.3187  time: 1.1112  data: 0.0000  max mem: 31568
Train: data epoch: [37]  [ 600/1000]  eta: 0:03:57  lr: 0.000002  loss: 1.8739  time: 0.4542  data: 0.0000  max mem: 31568
Train: data epoch: [37]  [ 650/1000]  eta: 0:03:30  lr: 0.000002  loss: 2.3154  time: 0.8301  data: 0.0000  max mem: 31568
Train: data epoch: [37]  [ 700/1000]  eta: 0:03:05  lr: 0.000002  loss: 0.6830  time: 0.4660  data: 0.0000  max mem: 31568
Train: data epoch: [37]  [ 750/1000]  eta: 0:02:31  lr: 0.000002  loss: 3.0797  time: 0.4543  data: 0.0000  max mem: 31568
Train: data epoch: [37]  [ 800/1000]  eta: 0:02:00  lr: 0.000002  loss: 0.3378  time: 0.5966  data: 0.0000  max mem: 31659
Train: data epoch: [37]  [ 850/1000]  eta: 0:01:30  lr: 0.000002  loss: 1.0594  time: 0.4537  data: 0.0000  max mem: 31659
Train: data epoch: [37]  [ 900/1000]  eta: 0:01:02  lr: 0.000002  loss: 1.8219  time: 0.4590  data: 0.0000  max mem: 31659
Train: data epoch: [37]  [ 950/1000]  eta: 0:00:30  lr: 0.000002  loss: 1.2736  time: 0.4555  data: 0.0000  max mem: 31659
Train: data epoch: [37]  [ 999/1000]  eta: 0:00:00  lr: 0.000002  loss: 1.0302  time: 0.5382  data: 0.0000  max mem: 31659
Train: data epoch: [37] Total time: 0:10:15 (0.6155 s / it)
Train: data epoch: [38]  [   0/1000]  eta: 11:20:01  lr: 0.000002  loss: 1.1843  time: 40.8012  data: 0.0000  max mem: 31659
Train: data epoch: [38]  [  50/1000]  eta: 0:19:50  lr: 0.000002  loss: 1.2481  time: 0.4526  data: 0.0000  max mem: 31659
Train: data epoch: [38]  [ 100/1000]  eta: 0:12:51  lr: 0.000002  loss: 1.3783  time: 0.4542  data: 0.0000  max mem: 31659
Train: data epoch: [38]  [ 150/1000]  eta: 0:12:50  lr: 0.000002  loss: 0.9163  time: 1.1531  data: 0.0000  max mem: 31659
Train: data epoch: [38]  [ 200/1000]  eta: 0:10:35  lr: 0.000002  loss: 1.2966  time: 0.4575  data: 0.0000  max mem: 31659
Train: data epoch: [38]  [ 250/1000]  eta: 0:10:00  lr: 0.000002  loss: 2.1891  time: 0.6393  data: 0.0000  max mem: 31659
Train: data epoch: [38]  [ 300/1000]  eta: 0:10:06  lr: 0.000002  loss: 0.3168  time: 0.4718  data: 0.0000  max mem: 31659
Train: data epoch: [38]  [ 350/1000]  eta: 0:09:26  lr: 0.000002  loss: 1.1799  time: 1.5561  data: 0.0000  max mem: 31659
Train: data epoch: [38]  [ 400/1000]  eta: 0:09:13  lr: 0.000002  loss: 1.1714  time: 2.5120  data: 0.0000  max mem: 31659
Train: data epoch: [38]  [ 450/1000]  eta: 0:07:58  lr: 0.000002  loss: 0.3326  time: 0.4552  data: 0.0000  max mem: 31659
Train: data epoch: [38]  [ 500/1000]  eta: 0:08:13  lr: 0.000002  loss: 2.0191  time: 2.3995  data: 0.0000  max mem: 31659
Train: data epoch: [38]  [ 550/1000]  eta: 0:07:41  lr: 0.000002  loss: 0.2243  time: 2.6926  data: 0.0000  max mem: 31659
Train: data epoch: [38]  [ 600/1000]  eta: 0:07:11  lr: 0.000002  loss: 1.3095  time: 2.0448  data: 0.0000  max mem: 32039
Train: data epoch: [38]  [ 650/1000]  eta: 0:06:36  lr: 0.000002  loss: 1.3375  time: 1.6709  data: 0.0000  max mem: 32039
Train: data epoch: [38]  [ 700/1000]  eta: 0:06:11  lr: 0.000002  loss: 1.2808  time: 2.8237  data: 0.0000  max mem: 32039
Train: data epoch: [38]  [ 750/1000]  eta: 0:06:56  lr: 0.000002  loss: 1.0499  time: 7.6942  data: 0.0000  max mem: 32039
Train: data epoch: [38]  [ 800/1000]  eta: 0:07:00  lr: 0.000002  loss: 1.3093  time: 6.3595  data: 0.0000  max mem: 32039
Train: data epoch: [38]  [ 850/1000]  eta: 0:05:00  lr: 0.000002  loss: 0.4909  time: 0.4527  data: 0.0000  max mem: 32039
Train: data epoch: [38]  [ 900/1000]  eta: 0:03:11  lr: 0.000002  loss: 0.3002  time: 0.4523  data: 0.0000  max mem: 32039
Train: data epoch: [38]  [ 950/1000]  eta: 0:01:32  lr: 0.000002  loss: 0.4203  time: 0.4525  data: 0.0000  max mem: 32039
Train: data epoch: [38]  [ 999/1000]  eta: 0:00:01  lr: 0.000002  loss: 1.1776  time: 0.5525  data: 0.0000  max mem: 32039
Train: data epoch: [38] Total time: 0:29:46 (1.7862 s / it)
Train: data epoch: [39]  [   0/1000]  eta: 0:07:36  lr: 0.000002  loss: 1.1812  time: 0.4564  data: 0.0000  max mem: 32039
Train: data epoch: [39]  [  50/1000]  eta: 0:07:24  lr: 0.000002  loss: 1.8997  time: 0.4522  data: 0.0000  max mem: 32039
Train: data epoch: [39]  [ 100/1000]  eta: 0:06:54  lr: 0.000002  loss: 1.9793  time: 0.4550  data: 0.0000  max mem: 32039
Train: data epoch: [39]  [ 150/1000]  eta: 0:10:02  lr: 0.000002  loss: 1.2538  time: 2.3454  data: 0.0000  max mem: 32039
Train: data epoch: [39]  [ 200/1000]  eta: 0:08:42  lr: 0.000002  loss: 0.2103  time: 0.4686  data: 0.0000  max mem: 32039
Train: data epoch: [39]  [ 250/1000]  eta: 0:07:40  lr: 0.000002  loss: 0.1760  time: 0.4540  data: 0.0000  max mem: 32039
Train: data epoch: [39]  [ 300/1000]  eta: 0:06:50  lr: 0.000002  loss: 1.3373  time: 0.4541  data: 0.0000  max mem: 32039
Train: data epoch: [39]  [ 350/1000]  eta: 0:06:09  lr: 0.000002  loss: 2.1202  time: 0.4528  data: 0.0000  max mem: 32039
Train: data epoch: [39]  [ 400/1000]  eta: 0:05:57  lr: 0.000002  loss: 1.2814  time: 1.2573  data: 0.0000  max mem: 32039
Train: data epoch: [39]  [ 450/1000]  eta: 0:11:26  lr: 0.000002  loss: 1.6216  time: 0.4717  data: 0.0000  max mem: 32039
Train: data epoch: [39]  [ 500/1000]  eta: 0:09:45  lr: 0.000002  loss: 0.6182  time: 0.5128  data: 0.0000  max mem: 32039
Train: data epoch: [39]  [ 550/1000]  eta: 0:09:37  lr: 0.000002  loss: 1.1829  time: 5.3779  data: 0.0000  max mem: 32039
Train: data epoch: [39]  [ 600/1000]  eta: 0:12:56  lr: 0.000002  loss: 1.5973  time: 13.9713  data: 0.0000  max mem: 32039
Train: data epoch: [39]  [ 650/1000]  eta: 0:13:47  lr: 0.000002  loss: 0.9327  time: 10.6524  data: 0.0000  max mem: 32039
Train: data epoch: [39]  [ 700/1000]  eta: 0:11:24  lr: 0.000002  loss: 1.1434  time: 2.4082  data: 0.0000  max mem: 32039
Train: data epoch: [39]  [ 750/1000]  eta: 0:09:00  lr: 0.000002  loss: 0.0910  time: 0.4526  data: 0.0000  max mem: 32039
Train: data epoch: [39]  [ 800/1000]  eta: 0:08:54  lr: 0.000002  loss: 0.2071  time: 0.4668  data: 0.0000  max mem: 32039
Train: data epoch: [39]  [ 850/1000]  eta: 0:06:43  lr: 0.000002  loss: 1.3353  time: 0.4510  data: 0.0000  max mem: 32039
Train: data epoch: [39]  [ 900/1000]  eta: 0:04:35  lr: 0.000002  loss: 1.2407  time: 2.8273  data: 0.0000  max mem: 32039
Train: data epoch: [39]  [ 950/1000]  eta: 0:02:15  lr: 0.000002  loss: 1.4641  time: 3.3055  data: 0.0000  max mem: 32039
Train: data epoch: [39]  [ 999/1000]  eta: 0:00:02  lr: 0.000002  loss: 0.3417  time: 0.5774  data: 0.0000  max mem: 32039
Train: data epoch: [39] Total time: 0:44:20 (2.6603 s / it)
Train: data epoch: [40]  [   0/1000]  eta: 0:08:30  lr: 0.000002  loss: 1.1563  time: 0.5108  data: 0.0000  max mem: 32039
Train: data epoch: [40]  [  50/1000]  eta: 0:22:00  lr: 0.000002  loss: 0.6614  time: 0.4509  data: 0.0000  max mem: 32039
Train: data epoch: [40]  [ 100/1000]  eta: 0:21:01  lr: 0.000002  loss: 0.2236  time: 0.4623  data: 0.0000  max mem: 32039
Train: data epoch: [40]  [ 150/1000]  eta: 0:15:24  lr: 0.000002  loss: 1.6823  time: 0.4526  data: 0.0000  max mem: 32039
Train: data epoch: [40]  [ 200/1000]  eta: 0:12:23  lr: 0.000002  loss: 0.3083  time: 0.4531  data: 0.0000  max mem: 32039
Train: data epoch: [40]  [ 250/1000]  eta: 0:12:46  lr: 0.000002  loss: 0.8695  time: 2.7848  data: 0.0000  max mem: 32039
Train: data epoch: [40]  [ 300/1000]  eta: 0:10:50  lr: 0.000002  loss: 2.1685  time: 0.4704  data: 0.0000  max mem: 32039
Train: data epoch: [40]  [ 350/1000]  eta: 0:09:21  lr: 0.000002  loss: 0.4261  time: 0.4952  data: 0.0000  max mem: 32039
Train: data epoch: [40]  [ 400/1000]  eta: 0:08:40  lr: 0.000002  loss: 1.1903  time: 1.1940  data: 0.0000  max mem: 32039
Train: data epoch: [40]  [ 450/1000]  eta: 0:07:56  lr: 0.000002  loss: 0.6864  time: 0.9399  data: 0.0000  max mem: 32039
Train: data epoch: [40]  [ 500/1000]  eta: 0:06:54  lr: 0.000002  loss: 1.2066  time: 0.5675  data: 0.0000  max mem: 32039
Train: data epoch: [40]  [ 550/1000]  eta: 0:06:04  lr: 0.000002  loss: 1.2296  time: 0.7763  data: 0.0000  max mem: 32039
Train: data epoch: [40]  [ 600/1000]  eta: 0:05:22  lr: 0.000002  loss: 2.2383  time: 0.4617  data: 0.0000  max mem: 32039
Train: data epoch: [40]  [ 650/1000]  eta: 0:04:35  lr: 0.000002  loss: 0.7800  time: 0.4534  data: 0.0000  max mem: 32039
Train: data epoch: [40]  [ 700/1000]  eta: 0:03:57  lr: 0.000002  loss: 1.1625  time: 0.4606  data: 0.0000  max mem: 32039
Train: data epoch: [40]  [ 750/1000]  eta: 0:03:16  lr: 0.000002  loss: 0.3896  time: 0.4533  data: 0.0000  max mem: 32039
Train: data epoch: [40]  [ 800/1000]  eta: 0:02:46  lr: 0.000002  loss: 0.9675  time: 0.4592  data: 0.0000  max mem: 32039
Train: data epoch: [40]  [ 850/1000]  eta: 0:02:09  lr: 0.000002  loss: 1.2943  time: 2.3953  data: 0.0000  max mem: 32039
Train: data epoch: [40]  [ 900/1000]  eta: 0:01:27  lr: 0.000002  loss: 1.9125  time: 2.0394  data: 0.0000  max mem: 32039
Train: data epoch: [40]  [ 950/1000]  eta: 0:00:44  lr: 0.000002  loss: 2.1018  time: 2.3559  data: 0.0000  max mem: 32039
Train: data epoch: [40]  [ 999/1000]  eta: 0:00:00  lr: 0.000002  loss: 1.2424  time: 2.5113  data: 0.0000  max mem: 32039
Train: data epoch: [40] Total time: 0:15:20 (0.9209 s / it)
Train: data epoch: [41]  [   0/1000]  eta: 0:12:27  lr: 0.000002  loss: 2.4058  time: 0.7472  data: 0.0000  max mem: 32039
Train: data epoch: [41]  [  50/1000]  eta: 0:22:15  lr: 0.000002  loss: 1.3654  time: 2.8631  data: 0.0000  max mem: 32039
Train: data epoch: [41]  [ 100/1000]  eta: 0:22:54  lr: 0.000002  loss: 0.3090  time: 2.1364  data: 0.0000  max mem: 32039
Train: data epoch: [41]  [ 150/1000]  eta: 0:22:59  lr: 0.000002  loss: 1.7728  time: 2.2714  data: 0.0000  max mem: 32039
Train: data epoch: [41]  [ 200/1000]  eta: 0:23:28  lr: 0.000002  loss: 1.8830  time: 3.2146  data: 0.0000  max mem: 32039
Train: data epoch: [41]  [ 250/1000]  eta: 0:26:35  lr: 0.000002  loss: 1.2417  time: 4.7935  data: 0.0000  max mem: 32039
Train: data epoch: [41]  [ 300/1000]  eta: 0:32:44  lr: 0.000002  loss: 1.1365  time: 7.0883  data: 0.0000  max mem: 32039
Train: data epoch: [41]  [ 350/1000]  eta: 0:35:15  lr: 0.000002  loss: 1.0695  time: 8.3297  data: 0.0000  max mem: 32039
Train: data epoch: [41]  [ 400/1000]  eta: 0:42:51  lr: 0.000002  loss: 1.7548  time: 12.9655  data: 0.0000  max mem: 32039
Train: data epoch: [41]  [ 450/1000]  eta: 0:37:24  lr: 0.000002  loss: 0.0692  time: 0.4512  data: 0.0000  max mem: 32039
Train: data epoch: [41]  [ 500/1000]  eta: 0:30:59  lr: 0.000002  loss: 0.3535  time: 0.4608  data: 0.0000  max mem: 32039
Train: data epoch: [41]  [ 550/1000]  eta: 0:25:40  lr: 0.000002  loss: 0.2780  time: 0.4534  data: 0.0000  max mem: 32039
Train: data epoch: [41]  [ 600/1000]  eta: 0:21:10  lr: 0.000002  loss: 2.3546  time: 0.4531  data: 0.0000  max mem: 32039
Train: data epoch: [41]  [ 650/1000]  eta: 0:17:18  lr: 0.000002  loss: 0.3772  time: 0.4555  data: 0.0000  max mem: 32039
Train: data epoch: [41]  [ 700/1000]  eta: 0:13:56  lr: 0.000002  loss: 1.2482  time: 0.4543  data: 0.0000  max mem: 32039
Train: data epoch: [41]  [ 750/1000]  eta: 0:11:28  lr: 0.000002  loss: 1.2990  time: 4.9628  data: 0.0000  max mem: 32039
Train: data epoch: [41]  [ 800/1000]  eta: 0:08:41  lr: 0.000002  loss: 0.2460  time: 0.4573  data: 0.0000  max mem: 32039
Train: data epoch: [41]  [ 850/1000]  eta: 0:06:12  lr: 0.000002  loss: 2.1033  time: 0.4535  data: 0.0000  max mem: 32039
Train: data epoch: [41]  [ 900/1000]  eta: 0:03:57  lr: 0.000002  loss: 1.4997  time: 0.4533  data: 0.0000  max mem: 32039
Train: data epoch: [41]  [ 950/1000]  eta: 0:01:53  lr: 0.000002  loss: 1.7273  time: 0.4529  data: 0.0000  max mem: 32039
Train: data epoch: [41]  [ 999/1000]  eta: 0:00:02  lr: 0.000002  loss: 2.3520  time: 0.5136  data: 0.0000  max mem: 32039
Train: data epoch: [41] Total time: 0:36:26 (2.1862 s / it)
Train: data epoch: [42]  [   0/1000]  eta: 0:07:28  lr: 0.000002  loss: 0.7237  time: 0.4485  data: 0.0000  max mem: 32039
Train: data epoch: [42]  [  50/1000]  eta: 0:31:54  lr: 0.000002  loss: 1.5327  time: 4.4372  data: 0.0000  max mem: 32039
Train: data epoch: [42]  [ 100/1000]  eta: 0:18:39  lr: 0.000002  loss: 2.3346  time: 0.4654  data: 0.0000  max mem: 32039
Train: data epoch: [42]  [ 150/1000]  eta: 0:17:44  lr: 0.000002  loss: 1.7111  time: 2.4939  data: 0.0000  max mem: 32039
Train: data epoch: [42]  [ 200/1000]  eta: 0:14:04  lr: 0.000002  loss: 1.0596  time: 0.4580  data: 0.0000  max mem: 32039
Train: data epoch: [42]  [ 250/1000]  eta: 0:11:41  lr: 0.000002  loss: 1.7905  time: 0.4530  data: 0.0000  max mem: 32039
Train: data epoch: [42]  [ 300/1000]  eta: 0:09:58  lr: 0.000002  loss: 1.4025  time: 0.4541  data: 0.0000  max mem: 32039
Train: data epoch: [42]  [ 350/1000]  eta: 0:08:38  lr: 0.000002  loss: 1.0936  time: 0.4542  data: 0.0000  max mem: 32039
Train: data epoch: [42]  [ 400/1000]  eta: 0:11:06  lr: 0.000002  loss: 0.4415  time: 0.4693  data: 0.0000  max mem: 32039
Train: data epoch: [42]  [ 450/1000]  eta: 0:09:31  lr: 0.000001  loss: 1.1564  time: 0.4530  data: 0.0000  max mem: 32039
Train: data epoch: [42]  [ 500/1000]  eta: 0:08:10  lr: 0.000001  loss: 0.5706  time: 0.4543  data: 0.0000  max mem: 32039
Train: data epoch: [42]  [ 550/1000]  eta: 0:06:59  lr: 0.000001  loss: 2.1081  time: 0.4546  data: 0.0000  max mem: 32039
Train: data epoch: [42]  [ 600/1000]  eta: 0:06:31  lr: 0.000001  loss: 1.0792  time: 0.4573  data: 0.0000  max mem: 32039
Train: data epoch: [42]  [ 650/1000]  eta: 0:05:28  lr: 0.000001  loss: 1.2010  time: 0.4536  data: 0.0000  max mem: 32039
Train: data epoch: [42]  [ 700/1000]  eta: 0:04:31  lr: 0.000001  loss: 2.0589  time: 0.4527  data: 0.0000  max mem: 32039
Train: data epoch: [42]  [ 750/1000]  eta: 0:03:38  lr: 0.000001  loss: 1.4002  time: 0.4531  data: 0.0000  max mem: 32039
Train: data epoch: [42]  [ 800/1000]  eta: 0:02:57  lr: 0.000001  loss: 0.1897  time: 0.4845  data: 0.0000  max mem: 32039
Train: data epoch: [42]  [ 850/1000]  eta: 0:02:09  lr: 0.000001  loss: 0.3660  time: 0.4535  data: 0.0000  max mem: 32039
Train: data epoch: [42]  [ 900/1000]  eta: 0:01:23  lr: 0.000001  loss: 1.2685  time: 0.4534  data: 0.0000  max mem: 32039
Train: data epoch: [42]  [ 950/1000]  eta: 0:00:43  lr: 0.000001  loss: 1.1367  time: 0.4512  data: 0.0000  max mem: 32039
Train: data epoch: [42]  [ 999/1000]  eta: 0:00:00  lr: 0.000001  loss: 2.9393  time: 0.5475  data: 0.0000  max mem: 32039
Train: data epoch: [42] Total time: 0:14:08 (0.8482 s / it)
Train: data epoch: [43]  [   0/1000]  eta: 0:09:41  lr: 0.000001  loss: 0.3541  time: 0.5810  data: 0.0000  max mem: 32039
Train: data epoch: [43]  [  50/1000]  eta: 0:14:52  lr: 0.000001  loss: 0.5157  time: 1.6874  data: 0.0000  max mem: 32039
Train: data epoch: [43]  [ 100/1000]  eta: 0:10:31  lr: 0.000001  loss: 1.2914  time: 0.4558  data: 0.0000  max mem: 32039
Train: data epoch: [43]  [ 150/1000]  eta: 0:08:46  lr: 0.000001  loss: 1.1963  time: 0.4582  data: 0.0000  max mem: 32039
Train: data epoch: [43]  [ 200/1000]  eta: 0:09:02  lr: 0.000001  loss: 1.2767  time: 0.4557  data: 0.0000  max mem: 32039
Train: data epoch: [43]  [ 250/1000]  eta: 0:07:55  lr: 0.000001  loss: 0.1441  time: 0.4542  data: 0.0000  max mem: 32039
Train: data epoch: [43]  [ 300/1000]  eta: 0:09:03  lr: 0.000001  loss: 0.1773  time: 1.9685  data: 0.0000  max mem: 32039
Train: data epoch: [43]  [ 350/1000]  eta: 0:07:54  lr: 0.000001  loss: 1.2174  time: 0.4527  data: 0.0000  max mem: 32039
Train: data epoch: [43]  [ 400/1000]  eta: 0:07:51  lr: 0.000001  loss: 1.4595  time: 0.4705  data: 0.0000  max mem: 32039
Train: data epoch: [43]  [ 450/1000]  eta: 0:09:39  lr: 0.000001  loss: 1.3283  time: 7.3409  data: 0.0000  max mem: 32039
Train: data epoch: [43]  [ 500/1000]  eta: 0:08:55  lr: 0.000001  loss: 1.1735  time: 0.4588  data: 0.0000  max mem: 32039
Train: data epoch: [43]  [ 550/1000]  eta: 0:08:12  lr: 0.000001  loss: 2.4725  time: 2.6026  data: 0.0000  max mem: 32039
Train: data epoch: [43]  [ 600/1000]  eta: 0:06:56  lr: 0.000001  loss: 1.2503  time: 0.4609  data: 0.0000  max mem: 32039
Train: data epoch: [43]  [ 650/1000]  eta: 0:06:10  lr: 0.000001  loss: 1.1693  time: 0.4536  data: 0.0000  max mem: 32039
Train: data epoch: [43]  [ 700/1000]  eta: 0:05:16  lr: 0.000001  loss: 0.9274  time: 0.4547  data: 0.0000  max mem: 32039
Train: data epoch: [43]  [ 750/1000]  eta: 0:04:13  lr: 0.000001  loss: 3.3074  time: 0.4588  data: 0.0000  max mem: 32039
Train: data epoch: [43]  [ 800/1000]  eta: 0:03:27  lr: 0.000001  loss: 0.1995  time: 0.4613  data: 0.0000  max mem: 32039
Train: data epoch: [43]  [ 850/1000]  eta: 0:02:41  lr: 0.000001  loss: 1.3052  time: 3.6227  data: 0.0000  max mem: 32039
| distributed init (rank 3, world 4): env://
| distributed init (rank 0, world 4): env://
| distributed init (rank 2, world 4): env://
| distributed init (rank 1, world 4): env://
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/coco_captions/coco_karpathy_train.json
vis_path:/home/users/nus/idmwyk/scratch/temp/dataset/coco/images/train2014
loading dataset refcoco into memory...
creating index...
index created.
DONE (t=7.40s)
loading dataset refcoco+ into memory...
creating index...
index created.
DONE (t=7.15s)
loading dataset refcocog into memory...
creating index...
index created.
DONE (t=6.31s)
loading dataset refcoco into memory...
creating index...
index created.
DONE (t=7.67s)
loading dataset refcoco+ into memory...
creating index...
index created.
DONE (t=3.96s)
loading dataset refcocog into memory...
creating index...
index created.
DONE (t=8.22s)
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/gqa/train_balanced_questions.json
vis_path:/home/users/nus/idmwyk/scratch/temp/dataset/gqa/images/train2014
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/vqav2/vqa_train.json
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/vqav2/vqa_val.json
vis_path:/home/users/nus/idmwyk/scratch/temp/dataset/coco/images/train2014
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/okvqa/okvqa_train.json
vis_path:/home/users/nus/idmwyk/scratch/temp/dataset/coco/images/train2014
Using downloaded and verified file: /home/users/nus/idmwyk/scratch/temp/dataset/aokvqa/aokvqa_v1p0_train.json
vis_path:/home/users/nus/idmwyk/scratch/temp/dataset/coco/images/train2014
len(exist_annotation):11106
trainable params: 27262976 || all params: 8057524224 || trainable%: 0.33835425426081844
Position interpolate from 16x16 to 32x32
Load Minigpt-4-LLM Checkpoint: /home/users/nus/idmwyk/scratch/temp/output/saved-ckpt/minigptv4/stage1/20240907183/checkpoint_49.pth
model arch:
 MiniGPTv4(
  (llama_model): PeftModelForCausalLM(
    (base_model): LoraModel(
      (model): LlamaForCausalLM(
        (model): LlamaModel(
          (embed_tokens): Embedding(128256, 4096)
          (layers): ModuleList(
            (0-31): 32 x LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): Linear(
                  in_features=4096, out_features=4096, bias=False
                  (lora_dropout): Dropout(p=0.05, inplace=False)
                  (lora_A): Linear(in_features=4096, out_features=64, bias=False)
                  (lora_B): Linear(in_features=64, out_features=4096, bias=False)
                )
                (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
                (v_proj): Linear(
                  in_features=4096, out_features=1024, bias=False
                  (lora_dropout): Dropout(p=0.05, inplace=False)
                  (lora_A): Linear(in_features=4096, out_features=64, bias=False)
                  (lora_B): Linear(in_features=64, out_features=1024, bias=False)
                )
                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
                (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
                (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
                (act_fn): SiLU()
              )
              (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
              (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
            )
          )
          (norm): LlamaRMSNorm((4096,), eps=1e-05)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (lm_head): CastOutputToFloat(
          (0): Linear(in_features=4096, out_features=128256, bias=False)
        )
      )
    )
  )
  (visual_encoder): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0-38): 39 x Block(
        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1408, out_features=4224, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1408, out_features=1408, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1408, out_features=6144, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=6144, out_features=1408, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (ln_vision): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)
  (local_attn): LocalAttention(
    (txt): Linear(in_features=4096, out_features=1408, bias=True)
    (img): Linear(in_features=1408, out_features=1408, bias=True)
  )
  (llama_proj): Linear(in_features=5632, out_features=4096, bias=True)
)
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.28.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.28.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.28.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.28.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.29.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.29.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.29.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.29.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.30.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.30.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.30.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.30.self_attn.v_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.31.self_attn.q_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.31.self_attn.q_proj.lora_B.weight
module.llama_model.base_model.model.model.layers.31.self_attn.v_proj.lora_A.weight
module.llama_model.base_model.model.model.layers.31.self_attn.v_proj.lora_B.weight
module.local_attn.txt.weight
module.local_attn.txt.bias
module.local_attn.img.weight
module.local_attn.img.bias
module.llama_proj.weight
module.llama_proj.bias
resume the checkpoint
batch sizes [[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]]
Train: data epoch: [43]  [   0/1000]  eta: 11:12:49  lr: 0.000001  loss: 0.3496  time: 40.3697  data: 0.0000  max mem: 29407
Train: data epoch: [43]  [  50/1000]  eta: 0:57:17  lr: 0.000001  loss: 1.1882  time: 1.5674  data: 0.0000  max mem: 30425
Train: data epoch: [43]  [ 100/1000]  eta: 0:33:22  lr: 0.000001  loss: 1.4917  time: 0.4521  data: 0.0000  max mem: 30509
Train: data epoch: [43]  [ 150/1000]  eta: 0:24:57  lr: 0.000001  loss: 1.5700  time: 0.4818  data: 0.0000  max mem: 30605
Train: data epoch: [43]  [ 200/1000]  eta: 0:19:11  lr: 0.000001  loss: 0.6587  time: 0.4647  data: 0.0000  max mem: 30605
Train: data epoch: [43]  [ 250/1000]  eta: 0:15:32  lr: 0.000001  loss: 1.2714  time: 0.4541  data: 0.0000  max mem: 30646
Train: data epoch: [43]  [ 300/1000]  eta: 0:13:00  lr: 0.000001  loss: 2.6699  time: 0.4911  data: 0.0000  max mem: 30933
Train: data epoch: [43]  [ 350/1000]  eta: 0:11:05  lr: 0.000001  loss: 0.5701  time: 0.4814  data: 0.0000  max mem: 30976
Train: data epoch: [43]  [ 400/1000]  eta: 0:09:33  lr: 0.000001  loss: 1.4179  time: 0.4550  data: 0.0000  max mem: 30976
Train: data epoch: [43]  [ 450/1000]  eta: 0:08:15  lr: 0.000001  loss: 0.1188  time: 0.4524  data: 0.0000  max mem: 30976
Train: data epoch: [43]  [ 500/1000]  eta: 0:07:13  lr: 0.000001  loss: 0.2283  time: 0.4552  data: 0.0000  max mem: 30976
Train: data epoch: [43]  [ 550/1000]  eta: 0:06:15  lr: 0.000001  loss: 0.3111  time: 0.5734  data: 0.0000  max mem: 30976
Train: data epoch: [43]  [ 600/1000]  eta: 0:05:21  lr: 0.000001  loss: 2.9182  time: 0.5009  data: 0.0000  max mem: 30976
Train: data epoch: [43]  [ 650/1000]  eta: 0:04:32  lr: 0.000001  loss: 1.2243  time: 0.4525  data: 0.0000  max mem: 30976
Train: data epoch: [43]  [ 700/1000]  eta: 0:03:46  lr: 0.000001  loss: 0.0831  time: 0.4524  data: 0.0000  max mem: 30976
Train: data epoch: [43]  [ 750/1000]  eta: 0:03:03  lr: 0.000001  loss: 1.1244  time: 0.4553  data: 0.0000  max mem: 30976
Train: data epoch: [43]  [ 800/1000]  eta: 0:02:23  lr: 0.000001  loss: 1.2631  time: 0.5088  data: 0.0000  max mem: 30976
Train: data epoch: [43]  [ 850/1000]  eta: 0:01:45  lr: 0.000001  loss: 1.7362  time: 0.4528  data: 0.0000  max mem: 31381
Train: data epoch: [43]  [ 900/1000]  eta: 0:01:09  lr: 0.000001  loss: 0.8225  time: 0.6320  data: 0.0000  max mem: 31381
Train: data epoch: [43]  [ 950/1000]  eta: 0:00:34  lr: 0.000001  loss: 0.5461  time: 0.4575  data: 0.0000  max mem: 31381
Train: data epoch: [43]  [ 999/1000]  eta: 0:00:00  lr: 0.000001  loss: 1.6734  time: 0.6350  data: 0.0000  max mem: 31381
Train: data epoch: [43] Total time: 0:11:14 (0.6745 s / it)
Train: data epoch: [44]  [   0/1000]  eta: 0:07:41  lr: 0.000001  loss: 2.3385  time: 0.4617  data: 0.0000  max mem: 31381
Train: data epoch: [44]  [  50/1000]  eta: 0:07:31  lr: 0.000001  loss: 0.5566  time: 0.4533  data: 0.0000  max mem: 31381
Train: data epoch: [44]  [ 100/1000]  eta: 0:07:03  lr: 0.000001  loss: 1.2943  time: 0.4828  data: 0.0000  max mem: 31381
Train: data epoch: [44]  [ 150/1000]  eta: 0:06:42  lr: 0.000001  loss: 1.2204  time: 0.4537  data: 0.0000  max mem: 31381
Train: data epoch: [44]  [ 200/1000]  eta: 0:06:15  lr: 0.000001  loss: 0.7738  time: 0.4535  data: 0.0000  max mem: 31381
Train: data epoch: [44]  [ 250/1000]  eta: 0:05:54  lr: 0.000001  loss: 1.1754  time: 0.4639  data: 0.0000  max mem: 31381
Train: data epoch: [44]  [ 300/1000]  eta: 0:05:28  lr: 0.000001  loss: 0.2266  time: 0.4540  data: 0.0000  max mem: 31381
Train: data epoch: [44]  [ 350/1000]  eta: 0:05:06  lr: 0.000001  loss: 1.3202  time: 0.5132  data: 0.0000  max mem: 31381
Train: data epoch: [44]  [ 400/1000]  eta: 0:04:42  lr: 0.000001  loss: 1.2872  time: 0.5002  data: 0.0000  max mem: 31381
Train: data epoch: [44]  [ 450/1000]  eta: 0:04:27  lr: 0.000001  loss: 2.4298  time: 0.4540  data: 0.0000  max mem: 31381
Train: data epoch: [44]  [ 500/1000]  eta: 0:04:01  lr: 0.000001  loss: 0.6947  time: 0.4546  data: 0.0000  max mem: 31381
Train: data epoch: [44]  [ 550/1000]  eta: 0:03:36  lr: 0.000001  loss: 0.2342  time: 0.4552  data: 0.0000  max mem: 31381
Train: data epoch: [44]  [ 600/1000]  eta: 0:03:11  lr: 0.000001  loss: 0.8728  time: 0.4546  data: 0.0000  max mem: 31381
Train: data epoch: [44]  [ 650/1000]  eta: 0:02:56  lr: 0.000001  loss: 2.9624  time: 1.1819  data: 0.0000  max mem: 31381
Train: data epoch: [44]  [ 700/1000]  eta: 0:02:29  lr: 0.000001  loss: 2.2750  time: 0.4639  data: 0.0000  max mem: 31381
Train: data epoch: [44]  [ 750/1000]  eta: 0:02:04  lr: 0.000001  loss: 1.3025  time: 0.4537  data: 0.0000  max mem: 31381
Train: data epoch: [44]  [ 800/1000]  eta: 0:01:38  lr: 0.000001  loss: 1.6628  time: 0.4525  data: 0.0000  max mem: 31381
Train: data epoch: [44]  [ 850/1000]  eta: 0:01:13  lr: 0.000001  loss: 1.1902  time: 0.4534  data: 0.0000  max mem: 31381
Train: data epoch: [44]  [ 900/1000]  eta: 0:00:48  lr: 0.000001  loss: 1.2762  time: 0.4538  data: 0.0000  max mem: 31381
Train: data epoch: [44]  [ 950/1000]  eta: 0:00:25  lr: 0.000001  loss: 1.1970  time: 0.4535  data: 0.0000  max mem: 31381
Train: data epoch: [44]  [ 999/1000]  eta: 0:00:00  lr: 0.000001  loss: 1.2030  time: 0.5212  data: 0.0000  max mem: 31381
Train: data epoch: [44] Total time: 0:08:19 (0.4999 s / it)
Train: data epoch: [45]  [   0/1000]  eta: 0:08:10  lr: 0.000001  loss: 1.2322  time: 0.4903  data: 0.0000  max mem: 31381
Train: data epoch: [45]  [  50/1000]  eta: 0:07:25  lr: 0.000001  loss: 1.3951  time: 0.4905  data: 0.0000  max mem: 31381
Train: data epoch: [45]  [ 100/1000]  eta: 0:07:27  lr: 0.000001  loss: 2.5121  time: 0.6010  data: 0.0000  max mem: 31381
Train: data epoch: [45]  [ 150/1000]  eta: 0:06:50  lr: 0.000001  loss: 1.1096  time: 0.4548  data: 0.0000  max mem: 31381
Train: data epoch: [45]  [ 200/1000]  eta: 0:06:21  lr: 0.000001  loss: 0.5350  time: 0.4668  data: 0.0000  max mem: 31381
Train: data epoch: [45]  [ 250/1000]  eta: 0:05:55  lr: 0.000001  loss: 1.3502  time: 0.4679  data: 0.0000  max mem: 31381
Train: data epoch: [45]  [ 300/1000]  eta: 0:05:36  lr: 0.000001  loss: 0.1339  time: 0.4527  data: 0.0000  max mem: 31381
Train: data epoch: [45]  [ 350/1000]  eta: 0:05:16  lr: 0.000001  loss: 1.3128  time: 0.4532  data: 0.0000  max mem: 31381
Train: data epoch: [45]  [ 400/1000]  eta: 0:05:28  lr: 0.000001  loss: 1.5688  time: 0.4568  data: 0.0000  max mem: 31381
Train: data epoch: [45]  [ 450/1000]  eta: 0:04:55  lr: 0.000001  loss: 1.2670  time: 0.4646  data: 0.0000  max mem: 31381
Train: data epoch: [45]  [ 500/1000]  eta: 0:04:24  lr: 0.000001  loss: 0.1348  time: 0.4535  data: 0.0000  max mem: 31381
Train: data epoch: [45]  [ 550/1000]  eta: 0:03:55  lr: 0.000001  loss: 2.5597  time: 0.4566  data: 0.0000  max mem: 31381
Train: data epoch: [45]  [ 600/1000]  eta: 0:03:37  lr: 0.000001  loss: 1.2881  time: 1.2879  data: 0.0000  max mem: 31381
Train: data epoch: [45]  [ 650/1000]  eta: 0:03:09  lr: 0.000001  loss: 2.5970  time: 0.4536  data: 0.0000  max mem: 31381
Train: data epoch: [45]  [ 700/1000]  eta: 0:02:40  lr: 0.000001  loss: 2.3524  time: 0.4595  data: 0.0000  max mem: 31381
Train: data epoch: [45]  [ 750/1000]  eta: 0:02:12  lr: 0.000001  loss: 1.1866  time: 0.4539  data: 0.0000  max mem: 31381
Train: data epoch: [45]  [ 800/1000]  eta: 0:01:49  lr: 0.000001  loss: 1.2499  time: 1.3087  data: 0.0000  max mem: 31381
Train: data epoch: [45]  [ 850/1000]  eta: 0:01:21  lr: 0.000001  loss: 1.8306  time: 0.4533  data: 0.0000  max mem: 31381
Train: data epoch: [45]  [ 900/1000]  eta: 0:00:53  lr: 0.000001  loss: 1.3589  time: 0.4532  data: 0.0000  max mem: 31381
Train: data epoch: [45]  [ 950/1000]  eta: 0:00:27  lr: 0.000001  loss: 0.9770  time: 0.4556  data: 0.0000  max mem: 31522
Train: data epoch: [45]  [ 999/1000]  eta: 0:00:00  lr: 0.000001  loss: 1.1782  time: 0.5346  data: 0.0000  max mem: 31522
Train: data epoch: [45] Total time: 0:09:06 (0.5461 s / it)
Train: data epoch: [46]  [   0/1000]  eta: 0:09:26  lr: 0.000001  loss: 1.0316  time: 0.5661  data: 0.0000  max mem: 31522
Train: data epoch: [46]  [  50/1000]  eta: 0:14:03  lr: 0.000001  loss: 1.2330  time: 0.4528  data: 0.0000  max mem: 31522
Train: data epoch: [46]  [ 100/1000]  eta: 0:10:10  lr: 0.000001  loss: 1.1369  time: 0.4593  data: 0.0000  max mem: 31522
Train: data epoch: [46]  [ 150/1000]  eta: 0:10:36  lr: 0.000001  loss: 0.5809  time: 1.5486  data: 0.0000  max mem: 31522
Train: data epoch: [46]  [ 200/1000]  eta: 0:09:00  lr: 0.000001  loss: 0.3072  time: 0.4572  data: 0.0000  max mem: 31522
Train: data epoch: [46]  [ 250/1000]  eta: 0:09:29  lr: 0.000001  loss: 0.1110  time: 0.4539  data: 0.0000  max mem: 31522
Train: data epoch: [46]  [ 300/1000]  eta: 0:09:13  lr: 0.000001  loss: 1.1277  time: 1.6569  data: 0.0000  max mem: 31522
Train: data epoch: [46]  [ 350/1000]  eta: 0:08:02  lr: 0.000001  loss: 1.1161  time: 0.4550  data: 0.0000  max mem: 31522
Train: data epoch: [46]  [ 400/1000]  eta: 0:07:42  lr: 0.000001  loss: 1.2217  time: 0.4586  data: 0.0000  max mem: 31522
Train: data epoch: [46]  [ 450/1000]  eta: 0:07:33  lr: 0.000001  loss: 0.1741  time: 1.5385  data: 0.0000  max mem: 31522
Train: data epoch: [46]  [ 500/1000]  eta: 0:06:51  lr: 0.000001  loss: 0.6080  time: 0.4560  data: 0.0000  max mem: 31522
Train: data epoch: [46]  [ 550/1000]  eta: 0:06:39  lr: 0.000001  loss: 1.8521  time: 1.2706  data: 0.0000  max mem: 31522
Train: data epoch: [46]  [ 600/1000]  eta: 0:07:29  lr: 0.000001  loss: 0.5726  time: 4.9519  data: 0.0000  max mem: 31522
Train: data epoch: [46]  [ 650/1000]  eta: 0:09:02  lr: 0.000001  loss: 1.2636  time: 0.4558  data: 0.0000  max mem: 31522
Train: data epoch: [46]  [ 700/1000]  eta: 0:07:21  lr: 0.000001  loss: 2.4685  time: 0.4674  data: 0.0000  max mem: 31522
Train: data epoch: [46]  [ 750/1000]  eta: 0:05:50  lr: 0.000001  loss: 0.1642  time: 0.4532  data: 0.0000  max mem: 31522
Train: data epoch: [46]  [ 800/1000]  eta: 0:04:28  lr: 0.000001  loss: 2.4202  time: 0.4531  data: 0.0000  max mem: 31522
Train: data epoch: [46]  [ 850/1000]  eta: 0:03:13  lr: 0.000001  loss: 1.2458  time: 0.4564  data: 0.0000  max mem: 31522
Train: data epoch: [46]  [ 900/1000]  eta: 0:02:04  lr: 0.000001  loss: 1.2045  time: 0.4542  data: 0.0000  max mem: 31522
Train: data epoch: [46]  [ 950/1000]  eta: 0:01:00  lr: 0.000001  loss: 1.1924  time: 0.4536  data: 0.0000  max mem: 31522
Train: data epoch: [46]  [ 999/1000]  eta: 0:00:01  lr: 0.000001  loss: 1.4120  time: 0.5297  data: 0.0000  max mem: 31522
Train: data epoch: [46] Total time: 0:19:38 (1.1785 s / it)
Train: data epoch: [47]  [   0/1000]  eta: 0:07:28  lr: 0.000001  loss: 2.7658  time: 0.4488  data: 0.0000  max mem: 31522
Train: data epoch: [47]  [  50/1000]  eta: 0:07:11  lr: 0.000001  loss: 1.1528  time: 0.4545  data: 0.0000  max mem: 31522
Train: data epoch: [47]  [ 100/1000]  eta: 0:06:48  lr: 0.000001  loss: 1.2045  time: 0.4541  data: 0.0000  max mem: 31522
Train: data epoch: [47]  [ 150/1000]  eta: 0:06:26  lr: 0.000001  loss: 0.8608  time: 0.4547  data: 0.0000  max mem: 31522
Train: data epoch: [47]  [ 200/1000]  eta: 0:06:03  lr: 0.000001  loss: 1.2530  time: 0.4538  data: 0.0000  max mem: 31522
Train: data epoch: [47]  [ 250/1000]  eta: 0:05:40  lr: 0.000001  loss: 2.6308  time: 0.4549  data: 0.0000  max mem: 31522
Train: data epoch: [47]  [ 300/1000]  eta: 0:06:09  lr: 0.000001  loss: 0.1221  time: 1.0919  data: 0.0000  max mem: 31522
Train: data epoch: [47]  [ 350/1000]  eta: 0:05:36  lr: 0.000001  loss: 1.7406  time: 0.4536  data: 0.0000  max mem: 31522
Train: data epoch: [47]  [ 400/1000]  eta: 0:05:06  lr: 0.000001  loss: 1.2743  time: 0.4534  data: 0.0000  max mem: 31522
Train: data epoch: [47]  [ 450/1000]  eta: 0:04:37  lr: 0.000001  loss: 0.2235  time: 0.4616  data: 0.0000  max mem: 31522
Train: data epoch: [47]  [ 500/1000]  eta: 0:04:09  lr: 0.000001  loss: 1.0090  time: 0.4628  data: 0.0000  max mem: 31522
Train: data epoch: [47]  [ 550/1000]  eta: 0:03:43  lr: 0.000001  loss: 1.0531  time: 0.4544  data: 0.0000  max mem: 31522
Train: data epoch: [47]  [ 600/1000]  eta: 0:03:16  lr: 0.000001  loss: 1.3468  time: 0.4531  data: 0.0000  max mem: 31522
Train: data epoch: [47]  [ 650/1000]  eta: 0:02:51  lr: 0.000001  loss: 1.0885  time: 0.4555  data: 0.0000  max mem: 31522
Train: data epoch: [47]  [ 700/1000]  eta: 0:02:26  lr: 0.000001  loss: 2.2246  time: 0.4565  data: 0.0000  max mem: 31522
Train: data epoch: [47]  [ 750/1000]  eta: 0:02:01  lr: 0.000001  loss: 1.3933  time: 0.4601  data: 0.0000  max mem: 31522
Train: data epoch: [47]  [ 800/1000]  eta: 0:01:37  lr: 0.000001  loss: 1.9936  time: 0.5105  data: 0.0000  max mem: 31522
Train: data epoch: [47]  [ 850/1000]  eta: 0:01:12  lr: 0.000001  loss: 1.2433  time: 0.4596  data: 0.0000  max mem: 31522
Train: data epoch: [47]  [ 900/1000]  eta: 0:00:48  lr: 0.000001  loss: 1.1907  time: 0.4805  data: 0.0000  max mem: 31522
Train: data epoch: [47]  [ 950/1000]  eta: 0:00:24  lr: 0.000001  loss: 1.9205  time: 0.4541  data: 0.0000  max mem: 31522
Train: data epoch: [47]  [ 999/1000]  eta: 0:00:00  lr: 0.000001  loss: 1.1722  time: 0.7462  data: 0.0000  max mem: 31522
Train: data epoch: [47] Total time: 0:08:13 (0.4936 s / it)
Train: data epoch: [48]  [   0/1000]  eta: 0:08:17  lr: 0.000001  loss: 2.3950  time: 0.4980  data: 0.0000  max mem: 31522
Train: data epoch: [48]  [  50/1000]  eta: 0:07:28  lr: 0.000001  loss: 0.1581  time: 0.4537  data: 0.0000  max mem: 31522
Train: data epoch: [48]  [ 100/1000]  eta: 0:06:56  lr: 0.000001  loss: 0.2501  time: 0.4539  data: 0.0000  max mem: 31522
Train: data epoch: [48]  [ 150/1000]  eta: 0:06:32  lr: 0.000001  loss: 0.3087  time: 0.4543  data: 0.0000  max mem: 31522
Train: data epoch: [48]  [ 200/1000]  eta: 0:06:09  lr: 0.000001  loss: 0.4782  time: 0.4540  data: 0.0000  max mem: 31522
Train: data epoch: [48]  [ 250/1000]  eta: 0:05:49  lr: 0.000001  loss: 1.2701  time: 0.4543  data: 0.0000  max mem: 31522
Train: data epoch: [48]  [ 300/1000]  eta: 0:05:31  lr: 0.000001  loss: 1.9513  time: 0.5803  data: 0.0000  max mem: 31567
Train: data epoch: [48]  [ 350/1000]  eta: 0:05:06  lr: 0.000001  loss: 1.2054  time: 0.4546  data: 0.0000  max mem: 31567
Train: data epoch: [48]  [ 400/1000]  eta: 0:04:43  lr: 0.000001  loss: 0.1690  time: 0.5340  data: 0.0000  max mem: 31567
Train: data epoch: [48]  [ 450/1000]  eta: 0:04:21  lr: 0.000001  loss: 1.0314  time: 0.4558  data: 0.0000  max mem: 31567
Train: data epoch: [48]  [ 500/1000]  eta: 0:03:58  lr: 0.000001  loss: 2.0293  time: 0.4537  data: 0.0000  max mem: 31567
Train: data epoch: [48]  [ 550/1000]  eta: 0:03:44  lr: 0.000001  loss: 1.3082  time: 0.5418  data: 0.0000  max mem: 31567
Train: data epoch: [48]  [ 600/1000]  eta: 0:03:18  lr: 0.000001  loss: 1.8699  time: 0.4558  data: 0.0000  max mem: 31567
Train: data epoch: [48]  [ 650/1000]  eta: 0:02:52  lr: 0.000001  loss: 2.3215  time: 0.4546  data: 0.0000  max mem: 31567
Train: data epoch: [48]  [ 700/1000]  eta: 0:02:26  lr: 0.000001  loss: 0.6529  time: 0.4556  data: 0.0000  max mem: 31567
Train: data epoch: [48]  [ 750/1000]  eta: 0:02:01  lr: 0.000001  loss: 3.0662  time: 0.4552  data: 0.0000  max mem: 31567
Train: data epoch: [48]  [ 800/1000]  eta: 0:01:38  lr: 0.000001  loss: 0.3403  time: 0.7981  data: 0.0000  max mem: 31659
Train: data epoch: [48]  [ 850/1000]  eta: 0:01:14  lr: 0.000001  loss: 1.0298  time: 0.4538  data: 0.0000  max mem: 31659
Train: data epoch: [48]  [ 900/1000]  eta: 0:00:50  lr: 0.000001  loss: 1.8165  time: 0.4551  data: 0.0000  max mem: 31659
Train: data epoch: [48]  [ 950/1000]  eta: 0:00:24  lr: 0.000001  loss: 1.2683  time: 0.4730  data: 0.0000  max mem: 31659
Train: data epoch: [48]  [ 999/1000]  eta: 0:00:00  lr: 0.000001  loss: 1.0201  time: 0.7498  data: 0.0000  max mem: 31659
Train: data epoch: [48] Total time: 0:08:23 (0.5040 s / it)
Train: data epoch: [49]  [   0/1000]  eta: 0:08:02  lr: 0.000001  loss: 1.1879  time: 0.4821  data: 0.0000  max mem: 31659
Train: data epoch: [49]  [  50/1000]  eta: 0:07:11  lr: 0.000001  loss: 1.2165  time: 0.4535  data: 0.0000  max mem: 31659
Train: data epoch: [49]  [ 100/1000]  eta: 0:08:45  lr: 0.000001  loss: 1.3788  time: 0.4549  data: 0.0000  max mem: 31659
Train: data epoch: [49]  [ 150/1000]  eta: 0:07:39  lr: 0.000001  loss: 0.9067  time: 0.4536  data: 0.0000  max mem: 31659
Train: data epoch: [49]  [ 200/1000]  eta: 0:06:55  lr: 0.000001  loss: 1.2937  time: 0.4539  data: 0.0000  max mem: 31659
Train: data epoch: [49]  [ 250/1000]  eta: 0:06:53  lr: 0.000001  loss: 2.1443  time: 0.4519  data: 0.0000  max mem: 31659
Train: data epoch: [49]  [ 300/1000]  eta: 0:06:15  lr: 0.000001  loss: 0.3166  time: 0.4579  data: 0.0000  max mem: 31659
Train: data epoch: [49]  [ 350/1000]  eta: 0:05:41  lr: 0.000001  loss: 1.1800  time: 0.4540  data: 0.0000  max mem: 31659
Train: data epoch: [49]  [ 400/1000]  eta: 0:05:47  lr: 0.000001  loss: 1.1586  time: 0.4561  data: 0.0000  max mem: 31659
Train: data epoch: [49]  [ 450/1000]  eta: 0:05:10  lr: 0.000001  loss: 0.3367  time: 0.4572  data: 0.0000  max mem: 31659
Train: data epoch: [49]  [ 500/1000]  eta: 0:04:36  lr: 0.000001  loss: 2.0021  time: 0.4540  data: 0.0000  max mem: 31659
Train: data epoch: [49]  [ 550/1000]  eta: 0:04:28  lr: 0.000001  loss: 0.2268  time: 1.3900  data: 0.0000  max mem: 31659
Train: data epoch: [49]  [ 600/1000]  eta: 0:03:54  lr: 0.000001  loss: 1.2690  time: 0.4744  data: 0.0000  max mem: 32037
Train: data epoch: [49]  [ 650/1000]  eta: 0:03:21  lr: 0.000001  loss: 1.3501  time: 0.4537  data: 0.0000  max mem: 32037
Train: data epoch: [49]  [ 700/1000]  eta: 0:03:00  lr: 0.000001  loss: 1.2773  time: 0.5637  data: 0.0000  max mem: 32037
Train: data epoch: [49]  [ 750/1000]  eta: 0:02:27  lr: 0.000001  loss: 1.0359  time: 0.4536  data: 0.0000  max mem: 32037
Train: data epoch: [49]  [ 800/1000]  eta: 0:01:58  lr: 0.000001  loss: 1.3188  time: 0.8295  data: 0.0000  max mem: 32037
Train: data epoch: [49]  [ 850/1000]  eta: 0:01:29  lr: 0.000001  loss: 0.4671  time: 0.4545  data: 0.0000  max mem: 32037
Train: data epoch: [49]  [ 900/1000]  eta: 0:01:02  lr: 0.000001  loss: 0.3029  time: 1.9229  data: 0.0000  max mem: 32037
Train: data epoch: [49]  [ 950/1000]  eta: 0:00:30  lr: 0.000001  loss: 0.4084  time: 0.4526  data: 0.0000  max mem: 32037
Train: data epoch: [49]  [ 999/1000]  eta: 0:00:00  lr: 0.000001  loss: 1.1760  time: 0.6349  data: 0.0000  max mem: 32037
Train: data epoch: [49] Total time: 0:10:30 (0.6307 s / it)
